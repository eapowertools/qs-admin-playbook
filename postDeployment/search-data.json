{
	"0": {
		"title": "Review 24 Hour Summary (Ops Monitor)",
		"content": "Review 24 Hour Summary (Ops Monitor) . Cadence Daily . Sites production .   Initial Recurring . Estimated Time | 2 min | 2 min | . Benefits: . Increase stability | Increase system awareness | . . Goal . The goal for this spot-check is use the Operations Monitor to review the last 24 hours of activity for the Qlik site. This will allow the administrator the ability to spot anomalies, keep abreast of trends, and be generally aware of the health of a Qlik Sense Enterprise deployment. . Table of Contents . 24 Hour Summary (Ops Monitor) | . . 24 Hour Summary (Ops Monitor) . Open the Operations Monitor application and navigate to the 24-Hour Summary sheet: . . On this sheet, there are 6 distinct KPIs and 1 table which are of note. For the KPIs the administrator can click on the relevant KPI to drill into a detailed sheet should the need arise. . (1) : Max Sense Engine CPU + (2) Max Sense Engine RAM GB : These two metrics signal the maximum compute use by any of the nodes in a Qlik Sense Enterprise cluster. This can be combined with (7) and (8) to spot any spikes. Given the Qlik Engine’s ability to scale across available compute, one-off spikes aren’t necessarily a signal that action is needed. It’s extended saturation events which signal that the available compute is insufficient for the required demands. See Review Architecture/Scale Plan for more guidance on the considerations when scaling Qlik Sense Enterprise. The detailed information in (7) and (8) allow for easy filtering to the time period where the anomalous compute occurred. | (3) Reload Failures and (4) Max Reload Duration: This is a complement to the Spot Check: Tasks activity. (3) would be well covered by the Spot Check of Tasks whereas (4) would not be. Long-running tasks should be avoided where possible. The causes could range from complicated ETL demands to data source resource constraints (e.g. a slow database). Investigation into the causes of the long-running should be done if they are unexpected. | (5) Max Concurrent Users and (6) Max Concurrent Apps : These two metrics signal the maximum consumption demands for the site, either defined by unique apps or unique users. Anomalies here may signal the need for more compute (1) and (2) above. Spikes here may also be expected given the applications developed and the user population. For example, an app which is targeted to a sales audience is expected to be more popular during quarter end events where quotas are evaluated. The same is true for applications with tax relevant data which are consumed near the close of the business’s fiscal year. | . Tags . #daily . #spot_check . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_spot_check/24_hour_summary.html",
		"relUrl": "/docs/system_spot_check/24_hour_summary.html"
	},
	"1": {
		"title": "About",
		"content": "About . The Qlik Sense Admin Playbook is designed and intended to be a repository of best practices for a Qlik Administrator to reference. It is maintained by the Americas Enterprise Architecture team at Qlik. The Playbook covers many activities and exercises that help to maintain a high-performing and easily manageable site. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-triangle fa-sm&quot;&gt;&lt;/i&gt; Word of Caution&lt;/p&gt; . &lt;p&gt;The goal of this repository is to outline best practices and reference example work-flows or tooling which can be used. It is not intended that the actions be completed verbatim, as they will need to be interpreted/customized as per the needs of each individual organization. Do not go through any exercise in the Playbook blindly–ensure that each has been thorougly reviewed and tested before they are attempted, as some of the exercises physically remove assets and alter site configuration. &lt;br /&gt;&lt;br /&gt; &lt;strong&gt;The Admin Playbook is not supported by Qlik&lt;/strong&gt;, but rather is maintained by Qlik employees. . &lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-triangle fa-sm&quot;&gt;&lt;/i&gt; Scope&lt;/p&gt; . &lt;p&gt;Since the goal of this repository is to present best practices, it will not be effective as a &lt;i&gt;troubleshooting&lt;/i&gt; guide. Practically, if a particular Qlik site is experiencing stability issues then this repository is unlikely to be effective in stabilizing the environment. That being said, faithful adherence to the guidance in this playbook will reduce the probability that a given environment will experience stability issues by reducing assets and qualifying problematic assets. . &lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-hands-helping fa-sm&quot;&gt;&lt;/i&gt; Site Support&lt;/p&gt; . &lt;p&gt;If there are requests for additional enhancements of the site, including additional documentation or coverage, or if something isn’t working as expected, please submit an issue on GitHub &lt;a href=&quot;https://github.com/eapowertools/issues&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; . Playbook Lifecycle . The cadence of the activities shown on the Playbook itself are common, yet they will not fit every organization, or might fit an organization for a period, and then shift. It is encouraged to work through the Playbook at the recommended starting cadence and adapt the Playbook of each organization’s deployment. Some activities may be more or less useful / relevant for an organization, so review the outcomes and make appropriate adjustments. There is no “one size fits all” solution, as every organization has different needs and faces challenges that evolve over time. . Customization . Give that the Playbook doesn’t currently support customization via the web interface, a Qlik Application has been created that can load in a csv of the Playbook’s itinerary, and can be customized within Qlik–linking live back to the site. . Find the Playbook qvf and associated xlsx here. . Icons Used Throughout the Site . Tooling . This icon is used to denote that the section either requires or is a tool that has been created that is not directly supported by Qlik. Many of them are simply qvf files with minimal configuration, but others involve custom installers, etc. Documentation is provided for all tools referenced, and all repositories, at the time that this was written, are currently active and supported by their owners–many of whom work for Qlik. . &lt;p&gt;&lt;i class=&quot;fas fa-tools fa-xs&quot;&gt;&lt;/i&gt; Tooling Support&lt;/p&gt; . &lt;p&gt;Many of the actions and exercises within these best practices involve third-party tools, and those tools are not directly supported by Qlik. For support for these tools, add an issue to the tool’s GitHub page (e.g. the &lt;a href=&quot;https://github.com/eapowertools/qs-telemetry-dashboard/issues&quot;&gt;issues page for the Telemetry Dashboard&lt;/a&gt;). &lt;/p&gt; . Script . This icon denotes that the section offers a solution that could be achieved with script, marked by the asterisk, or a section that requires script. The vast majority of the sections offer solutions that can be automated with scripts, but in most cases, it is not required. The script icon frequently is found on sections where solutions involving the Qlik CLI for Windows are offered. Other times, the icon may denote the use of the command line, Engine API, or otherwise. .",
		"url": "https://adminplaybook.qlik-poc.com/docs/about",
		"relUrl": "/docs/about"
	},
	"2": {
		"title": "Analyze App Adoption",
		"content": "Analyze App Adoption . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 25 Min | 10 min | . Benefits: . Understand adoption | Better understand user-base | Identify top and bottom applications by adoption | . . Goal . At it simplest, the goal of this page is to identify the top five and bottom five applications by two metrics: . Total sessions by application | Total distinct users by application | . It is also important to identify any visible trends of usage – is usage trending up or down, or are there consistent spikes? In addition, it is helpful to characterize these applications, e.g. highly used yet only by a few inidividuals, widely used but infrequently accessed, etc. To visualize these areas, two additional charts will be built. . Table of Contents . Suggested Prerequisites | Operations Monitor Confirm Operations Monitor is Operational | . | User &amp; Session Metrics | Application/Session Activity Breakdown | Application Usage Trending | . . Suggested Prerequisites . Remove/Quarantine Unused Apps | . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . User &amp; Session Metrics . Navigate to the Monitoring apps stream and open up the Operations Monitor application. . . Select the Session Details sheet. . . Next, assuming this process is taking place quarterly, select the latest three full months. . . Following, sort the Sessions column of the App Session Summary table descending to view the applications with the greatest number of sessions. . . Now, to see the applications with the greatest number of distinct users, sort the Users column descending. . . In this case, three of the applications overlap, but notice that one of them was heavily used only by a single user. It is important to recognize and weigh the application’s importance by considering both metrics. Is it because the application is only relevant to one person, or is it only known to that person, or is that person not aware of a newer application? These are just some questions one might want to consider asking. . Repeat the same process above for the bottom five applications, by sorting both the Sessions and Users column descending one by one and recording and then comparing the results. . Application/Session Activity Breakdown . The metrics above are valuable for discerning which applications are used the most, but now consider the following questions: . What is the breakdown of each user’s usage within each app? | Is the session usage relatively evenly distributed, or is it condensed to only a few users? | What is the percentange of an application’s usage against other applications? For instance, what percentage do the top five applications take up of the entire environment? | . A new chart can be created to easily visualize session usage and users to answer all of the above questions. . Duplicate the Session Details sheet, and clear some real estate on the dashboard to make room for a new chart object. . . Drag and drop the Mekko chart (available as of the November 2019 release) and select Add dimension. Then, select the Session App Name field. . . Next, add the second dimension of UserId. . . Now, select Add measure, and add Sessions. . . In the properties panel, expand the Session App Name panel and set the Limitation to Fixed number. Next, set the Top to 6. The 6 allows for the top five applications to show, along with the Others faux dimension. Ensure that Show others is toggled on. . . Following, select the UserId dimension, and do the same as above except now set the Top to 11. . . Navigate down to the sorting section, and expand Session App Name. Untoggle the default sorting (Auto), toggle on Sort by expression, select Descending, and then enter Sum([Session Count]) as the expression value. . . Repeat the same process for the UserId sorting. . . Ensure that the sort order is: . Session App Name | UserId | Sessions | Next, move down to the Colors and legend section under Appearance. Toggle off Show legend to give the chart some exra space. . . Lastly, view the completed chart. One can quickly spot: . Who the predominant consumers of the top five applications are (if the distribution is less even) | The distribution of sessions by user (even or condensed to several) | The percentage of distribution of sessions relative to others | The percentage of all sessions of the top five applications against all others | . . By clicking in on the top five applications specifically (after noting the higher-level metric regarding overall usage), it can make the detail a bit easier to consume. . . Application Usage Trending . After identifying the top five applications, it is important to see in which direction their usage is trending, if any. To do so, a new chart can be created. . First, create some room on the duplicated Session Details sheet. In this case, the details table has been removed, as it is not relevant to this analysis. . Select the Combo chart, and insert it. Then, select the Date as the dimension. . . Select Add measure, and insert the Sessions measure. . . Under Measures, select Sessions, and change the type to Line. . . While remaining in the editing pane, under Height, select Add. Next, click the fx button. . . Insert the expression count({&lt;[Session Count]={1}&gt;} DISTINCT UserId). Name the measure Users, and ensure it is of type Line. . . View the the completed chart. It is easy to spot when there are many distinct users with few sessions each, or when there are only a few users with many sessions. Ensure that each of the top five applications are selected one by one (as well as the bottom five) so that the trends can be viewed individually. . . Tags . #quarterly . #asset_management . #apps . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/analyze_app_adoption.html",
		"relUrl": "/docs/asset_management/apps/analyze_app_adoption.html"
	},
	"3": {
		"title": "Analyze App Metadata Analyzer",
		"content": "Analyze App Metadata Analyzer . Cadence Weekly . Sites developmentproduction .   Initial Recurring . Estimated Time | 45 Min | 15 min | . Benefits: . Anomaly detection | Visibility/standardization of data models (e.g. gating mechanism to Production) | Optimization of data models | Load balancing visibility/validation | . . Goal . Leveraging the App Metadata Analyzer enables the following: . Insight into the composition of all data models across an entire Qlik site. | Visibility into what apps have synthetic keys, data islands, and circular references. | Visibility of base RAM footprints–which allows for sizing/load balancing considerations. | Indicators of what fields could be dropped/optimized by clearly displaying their size/proportion of the application. | Ability to apply thresholds at the application level such as: total records, total number of fields, total number of tables, disk size, etc – allowing one to employ gating-like mechanisms to ensure data modeling standards and best practices across the site. | . Table of Contents . Demo Video | Prerequisite Confirm the App Metadata Analyzer is Operational | . | Set Thresholds | Alerting Configure Alerting Tags (Optional) | Suggested Workflow &amp; Setup Guide | . | Establish Goals | Optimization Identify applications with large base RAM footprints. | Identify Fields for Optimization | Identify Tables for Optimization | Identify the Presence of Synthetic Keys &amp; Data Islands | . | Review Load Balancing | Documentation | . . Demo Video . . . Prerequisite . If the App Metadata Analyzer has not yet been imported into the site, navigate to the Where to get it section of the App Metadata Analyzer article for instructions on where to find the application. . Confirm the App Metadata Analyzer is Operational . Navigate to the Monitoring apps stream (or wherever the App Metadata Analyzer application has been published to) and select the Details button (info icon) on the App Metadata Analyzer application. Confirm that the application’s data is up-to-date. . . If the App Metadata Analyzer is not up-to-date, please refer to the App Metadata Analyzer article for configuration details and troubleshooting steps. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;As the &lt;strong&gt;App Metadata Analyzer&lt;/strong&gt; is not imported into the QMC by default, it is suggested that it is put on a nightly reload task. The application can of course be reloaded more frequently if desired, but nightly is the suggested cadence.&lt;/p&gt; . . Set Thresholds . Navigate to the Data load editor and select the configuration tab. On this tab, optional variables can be set to adjust the threshold settings. Ensure that these settings are set to values that the organization does not want to exceed. For example, if it is desired to not have a table with more than 100M records, then the vTableRecordCountThreshold can be set to 100000000. . These threshold variables will set dimensional fields so that applications that breach these thresholds can be flagged and easily selected during analysis. . . Alerting . In version 2.2.1 and forward of the App Metadata Analyzer, a new Alerting sheet has been added, along with two new variables in the load script. The purpose of this sheet and added capability is to make integration with Qlik Alerting as simple as possible. This new capability and view allows for a Qlik administrator to easily see what applications have breached what thresholds (as well as how many) and be quickly alerted on them. It also allows for the administrator to disable alerts for specific applications and mark others as under review, which might have a different cadence. . Configure Alerting Tags (Optional) . Navigate to the Data load editor and select the configuration tab. On this tab, optional variables can be set to the names of tags in the QMC. . . vu_ignore_alert_tag_name: This variable holds the tag name that a Qlik administrator would tag an application with that they do not want to receive alerts on. . vu_review_alert_tag_name: This variable holds the tag name that a Qlik administrator would tag an application with that is “currently under review”, e.g. the Qlik administrator has identified a problematic app and has gotten confirmation that a developer is working on resolving the issues. This allows the administrator to not be repeatedly alerted about these tagged applications. . These variables are defaulted to string values that if not found in the QRS will not impact the application. Simply put, if the tags are not used, all applications will be available for alerting (as there won’t be any dimensional filters to filter them out). . Suggested Workflow &amp; Setup Guide . While the Qlik administrator does not have to have Qlik Alerting to leverage this new sheet and capability manually, i.e. the administrator would simply check the app periodically instead of being alerted on applications, a Qlik Alerting guide with the App Metadata Analyzer has been created with suggested workflows and examples. . That guide can be found on the App Metadata Analyzer (Windows) Qlik Community Page, the file specifically here: Qlik Alerting with the App Metadata Analyzer for QSEoW. . Establish Goals . This application can be used for many different purposes, depending on the environment it is running in (Dev, Test, Prod, etc). Decide what the overall goals should be with the application for the administrator. . Suggested Goals per Tier . Dev Use as a gating mechanism for higher tiers. Applications must conform to data modeling standards and exist below the desired thresholds unless a justification is provided for an exception. | Use to gain insight/clarity into what developers are doing/enforce thresholds. | Identify developers that might be candidates for educational data modeling courses. I.e. those that have many apps with synthetic keys, very many fields/many tables, etc. This can be relevant to organizations that have many developers. | . | Test Use for optimization. Refer to the Optimization section below. | . | Prod Use for optimization. Refer to the Optimization section below. | Identify candidates for load balancing. Refer to the load balancing section of the Review/Update Capacity Plan article. | . | . Optimization . The purpose of this subsection is to optimize application’s data models, prioritizing the applications with large base RAM footprints first (the amount of RAM an application takes on the server without any users). . Identify applications with large base RAM footprints. . Navigate to the Dashboard sheet. . . Refer to the App Memory Footprint (MB) table. In this example, there is an application that takes up ~63 GB in RAM. . . Prioritize applications that are both large and widely used. Refer to Analyze App Adoption for how to view the user activity on an application, and review each large application from this exercise there. . Identify Fields for Optimization . For each large app identified above, look for fields that take a large amount of RAM. Refer to the Field Memory Footprint (MB) table. This table illustrates the Symbol Tables (for a good read on Symbol Tables and Data Tables, refer to this article on Qlik Community). If the values are large in this table, it typically implies that the field’s values are large and non-unique. Take a comment field for example – long text values with a very high cardinality. It is imporant to ensure fields like this are optimized/necessary for analysis, as they can add weight to applications quickly. . . Can these fields be optimized or potentially removed if unused? For instance, are any of the fields timestamps that could be floored or split apart into multiple fields to reduce cardinality? . To see if the fields are unused, it is suggested to use Rob Wunderlich’s App Analyzer. This tool is used to lift a single app into RAM and analyze it, then provide a detailed output in the form of a Qlik application. It is a great companion tool to the App Metadata Analyzer, as the App Metadata Analyzer allows one to spot potential applications that could use optimization, and then the App Analyzer can drill into the low-level details of that application. It has the ability to optimize the UI of the application as well, which this exercise does not cover. . Identify Tables for Optimization . For each large app identified above, look for tables that take a large amount of RAM. Have a look at the Table Memory Footprint (MB) table, also on the Dashboard sheet. This table illustrates the Data Tables (article on Qlik Community). The more records/columns in a table, the higher the table memory footprint. . . How many fields exist in these tables? If there are many fields (hundreds for example) in a table, it is likely that the developer is using a SELECT * FROM approach, and likely have many fields that aren’t used for analysis in the application. This is another prime opportunity to leverage Rob Wunderlich’s App Analyzer to remove many fields. . It is also worth considering the total record count of the tables. Are they at the appropriate level of grain? Is it possible that the table or portions of the table could be aggregated, or could alternative approaches like app segmentation, app chaining, or on-demand app generation be leveraged? . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;When dealing with very large data volumes, there are many strategies when designing the architecture of your applications. Here are three strategies: &lt;br /&gt;&lt;br /&gt; &lt;a href=&quot;https://community.qlik.com/t5/Qlik-Design-Blog/Big-Data-with-Qlik-One-method-does-not-fit-all/ba-p/1474484&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;&lt;/a&gt;: Segmenting QVDs and QVF’s by timeframes or another dimensionality like region. You might have a QVF that views the most recent two years, then another that views years that are further back, and one large QVF that contains all data that is only used when necessary by a small subset of users. Another approach might be to have multiple apps focused on different regions, so that user’s do not open an app with data that they are not interested in or have the right to see (data that is not able to be seen via section access still affects memory). With this method, the lighter weight (most recent data) application will be used by most users, saving memory as that is all they will typically need. &lt;br /&gt;&lt;br /&gt; &lt;a href=&quot;https://help.qlik.com/en-US/sense/Subsystems/Hub/Content/DataSource/Manage-big-data.htm&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;ODAG&lt;/strong&gt;&lt;/a&gt;: ODAG stands for on-demand app generation, and is a method where you have two applications: 1. a shopping cart (aggregated data), 2. an empty template app to display detail. The workflow is such that a user must first make selections in the shopping cart app (this criteria is completely customizable), and once a threshold has been met, a custom LOAD script is then created which then populates the template app with whatever detail was requested. &lt;br /&gt;&lt;br /&gt; &lt;a href=&quot;https://community.qlik.com/t5/Qlik-Design-Blog/Big-Data-with-Qlik-One-method-does-not-fit-all/ba-p/1474484&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Document Chaining&lt;/strong&gt;&lt;/a&gt;: Document chaining is where you have an aggregated application which typically is sufficient to the user, but when the user does need the detail, selections can be passed from the aggregated app to the detail app so that they can view a lower level of granularity. This keeps a low user footprint on the detail app, thereby reducing the memory of everyone loading unnecessary detail. While this is directly available in QlikView, it is supported via the APIs and thereby extensions in Qlik Sense.&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;br /&gt; Much more information around optimizing applications and data models can be found on the &lt;a href=&quot;https://diagnostictoolkit.qlik-poc.com&quot; target=&quot;_blank&quot;&gt;Diagnostic Toolkit&lt;/a&gt; website.&lt;/p&gt; . Together, both of these metrics (Table RAM and Field RAM) add up to the base RAM footprint. . . Identify the Presence of Synthetic Keys &amp; Data Islands . Navigate to the Threshold Analysis sheet. . . On each sheet on the bottom left, there are two tables: . . If there are synthetic keys, the majority of the time, it is a sign of a problem in the data model, and they should be rectified. There are of course scenarios when synthetic keys are harmless, and in fact the most optimal option, but that is typically not the case. For more information, refer to this Qlik Design Blog: Synthetic Keys article. . If there are any synthetic keys, select Has Synthetic Keys, and then view the applications, tables, and fields that contain them. . . If there are data islands, these also should be avoided where possible and should be attempted to be rectified in the model. For more information, refer to The Impact of Data Islands on Cache and CPU, an article by Rob Wunderlich. . Repeat the process for data islands, by selecting Has Island Table, and viewing the associated apps and island tables. . Review Load Balancing . If the Qlik Sense site has greater than two end-user facing nodes, it is worth considering “pinning” (load balancing) large applications to dedicated engines (2+ for resiliency). . Refer to the load balancing section of the Review/Update Capacity Plan article, where this process is explained. . For more detail on load balancing of assets and instructions on how to do so, refer to the Segregation of Larger Qlik Apps section of the Review Pinning/Load Balancing article. . Documentation . App Metadata Analyzer . Tags . #weekly . #asset_management . #apps . #metadata . #app_metadata_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/analyze_app_metadata_analyzer.html",
		"relUrl": "/docs/asset_management/apps/analyze_app_metadata_analyzer.html"
	},
	"4": {
		"title": "Analyze/Curate Extensions",
		"content": "Analyze/Curate Extensions . Cadence Quarterly . Sites all .   Initial Recurring . Estimated Time | 45 min | 10 min | . Benefits: . Reduce/eliminate unsupported extensions | Increase stability | Decrease upgrade maintenance/time | . . Goal . The goal of this activity is to be aware of new extensions which have been imported into the environment in addition to the use of the Extension Usage Dashboard to focus optimization efforts / reduce the carrying cost to a Qlik deployment. . Table of Contents . Monitor for New Extensions | Extension Usage Dashboard Migratable Extensions | Unused Extensions | Extension Consolidation | . | . . Monitor for New Extensions . In the QMC, select Extensions. . . In the upper right hand side of the screen, select the Column selector and then select the Created column. To make the resulting table a bit more manageable, feel free to deselect the Tags column. . . Now sort by the Created column to see recently added extensions. . . Given this overview of new extensions is rather minimal, a pre-built tool (Qlik Sense Extension Usage Dashboard) has been developed by the Enterprise Architecture team to help administrators provide more insight into what applications are using extensions and where. This will be covered next. . Extension Usage Dashboard . The Qlik Sense Extension Usage Dashboard, covered in this guide here, allows administrators to drill into granular metadata on extensions in their environment. The goal of this exercise is to take a look at high level KPIs which an administrator should monitor and take action on. . Migratable Extensions . On the KPIs sheet in the upper right hand corner, the app will count the number of instances where a Qlik application is using an extension which has since become part of a Qlik Sense bundle. . . Take the below as an example. . . In this example an app named Uses Deprecated Extensions uses an extension named barsPlus on the sheet named Bar Chart Sheet. This extension can be replaced by the extension named qlik-barplus-chart and this app now will receive full support from Qlik should issues arise. . Administrators should monitor this metric and communicate with the application’s owner(s) to have their use of the deprecated extension changed to using the supported bundle from Qlik. . Unused Extensions . On the KPIs sheet in the bottom left KPI, the secondary measure is the count of extensions which exist on the Qlik server and which are not used in a Qlik app. . . For a list of which extensions are not used, the administrator can do so in the Extension Overview sheet. . . As extensions are listed on this section of the Extension Usage Dashboard, the administrator should contact the owner of the extension whether the extension is needed. To take an example, for the extension foo which is unused, the administrator would filter on the name (foo) in the QMC to see who the owner of the extension is. . . At this point, it is apparent that the extension was created on October 24th 2019 by the user named xxx. A good guideline here is if an extension has been in a Qlik site for over a three months without being used then the owner needs to justify why the extension needs to exist. Without a cogent explanation for the use case for the extension, there is arguably little need for it to exist in its current state. The owner (or administrator) can easily export the extension for potential use later then delete the extension. . Extension Consolidation . The last activity that will be covered is a bit more difficult to operationalize. For many Qlik deployments, as the number of used extension grows there is often an overlap of capabilities / visualizations / etc. It is not uncommon to have multiple types of bar chart extensions, for example. While this is not intrinsically a problem for deployments, the administrator is encouraged to use usage information from the Extension Usage Dashboard to focus energies on eliminating extensions which are not widely used and offer no or little unique capability from other extensions or bundles from Qlik. Over time this will reduce the efforts needed to test extensions after upgrades of the Qlik deployment. . Tags . #quarterly . #asset_management . #extensions . #extension_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/extensions/analyze_curate_extensions.html",
		"relUrl": "/docs/asset_management/extensions/analyze_curate_extensions.html"
	},
	"5": {
		"title": "Analyze Data Connections",
		"content": "Analyze Data Connections . Cadence Monthly . Sites developmentproduction .   Initial Recurring . Estimated Time | 20 min (+ long initial reload) | 10 min | . Benefits: . Map data connections to owners | Track data connection usage | Enable auditing | Allow for elegant data connection deprecation | . . Goal . There are numerous goals to managing and curating data connections in a Qlik site – especially in a development tier, where potentially many, many power users are creating data connections. In essence, the goal is to run a leaner, more performant, and more easily and holistically governed Qlik Sense site. . In this section, there will be a How-to and an Action section for each of the commonly asked questions about data connections. . . Table of Contents . Prerequisite Data Connection Analyzer | . | Which connections are no longer used? | What connections have never been used? | Where are data connections being used? | Do we have duplicate data connections? | What connections have been deleted that used to be used? | What are the most widely used data connections? | Who is using what data connections? | Via what mechanism are data connections being used? | . . Prerequisite . Data Connection Analyzer . This pages leverages the Data Connection Analyzer application. For documentation, please refer to Data Connection Analyzer. To understand why this tool is needed for this process, please refer to the Data Connection Usage section of the Remove Unused Data Connections article. . . Which connections are no longer used? . A data connection is found in a script log and the data connection exists in the Qlik Sense site, however, the applications which refer to this data connection in their scripts no longer exist. . It is common for data connections to become deprecated or unused over time. A data source may change from one database to another. Or perhaps the need for the data source no longer exists. By leveraging this app, one can identify which connections have been used inside of Qlik apps but those Qlik apps have been deleted / no new Qlik apps use this data connection. . How-to . Navigate to the Unused Connection Analysis sheet. . . If there are any connections that fall into this category, they will appear in the Connections that have been used, but the Apps have since been Deleted: X table. In the example here, there are 6. . . Action . If the data connection Last Used (Reloaded) date is over 90 days old (or whatever is the desired age of a connection to be unused organizationlly), contact the owner of the data connection to see if it can be quarantined and then ultimately removed. Please refer to the Suggested Actions section of the Remove Unused Data Connections article. . . What connections have never been used? . A data connection exists in the Qlik Sense site, but no reference exists to it in any script log. . It is common that users will create data connections to test connectivity, but then never actually use them. By leveraging this app, one can identify connections that have never been used and have existed for x amount of time (say 90+ days), so that action can be taken to remove them. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;The Data Connection Analyzer app leverages a variable named &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vNumLogDays&lt;/code&gt; in the script to specify how many days worth of logs the app should parse. If this variable is modified from its default value of &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;9999&lt;/code&gt; then it is possible that the data connection was used but outside of the specified &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vNumLogDays&lt;/code&gt; and thus be considered unused by the Data Connection Analyzer. If the goal is to get a more accurate read here, ensure that &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;vNumLogDays&lt;/code&gt; is set to capture all logs, with a value of &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;99999&lt;/code&gt;. &lt;br /&gt; If old Archived Logs are being archived (refer to: &lt;a href=&quot;/docs/backup_and_archiving/archive_old_archived_logs.html&quot;&gt;Archive Old Archived Logs&lt;/a&gt;) then that needs to be taken into account as well, as those script logs will not be analyzed. Typically, this is perfectly acceptable, as those script logs are usually quite old, and if a connection hasn’t been used since then, it is safe to say it is unused and can be removed.&lt;/p&gt; . How-to . Navigate to the Unused Connection Analysis sheet. . . If there are any connections that fall into this category, they will appear in the Connections that have never been used in an App Reload table. In the example here, there are 27. . . Action . If the data connection Last Used (Reloaded) date is over 90 days old (or whatever is the desired age of a connection to be unused organizationlly), contact the owner of the data connection to see if it can be quarantined and then ultimately removed. Please refer to the Suggested Actions section of the Remove Unused Data Connections article. . . Where are data connections being used? . Let’s say a data source is being transformed and will be moved from one database to another location. The first question one might ask is, “What applications are using that connection, so we can re-route it to the new db and make adjustments to the load scripts?”. This historically has not been easy to answer. This application allows one to select that connection and visualize apps that are using it. . How-to . Navigate to the Dashboard sheet. . . Select True next to the Connections Used KPI to select all used data connections. . . Select a single data connection to visualize all of the applications that are dependent on it. . . Optionally, select a single application to view all of the data connection dependencies for a specific application. . . Action . If a data connection is being deprecated, or re-routed, etc–ensure that the process above of identifying all of the applications that are dependent on that data connection is followed, so that the new connection details can be addressed in each of the application’s load scripts. . . Do we have duplicate data connections? . By analyzing the connection strings, one can tackle duplicate connections to the same source data. This eases administration overhead and will ensure that there is reusability/consistency across the platform. . How-to . Navigate to the Duplicate Analysis sheet. . . In this example, the type of Folder connection has been selected, and it is visible that there are 2 folder connection duplicates. From the below we can tell that: . The C: QlikShare QVD connection Has four duplicates across two different users. | Is leveraged by two applications. | . | The C: QlikShare SharedApps connection Has two duplicates across a single user. | Isn’t leveraged by any application. | . | . It is very likely that these folder connections point to different subfolders within. . . Now, in the following example, the type of QvOdbcConnectorPackage.exe has been selected, and it is visible that there is a single ODBC connection that has a single duplicate. It is also visible that the connection has a different username–which is common across ODBC connections to ensure database security for individual users where passthrough auth is not an option. . . Action . Connection duplicates should be resolved where possible. Contact the owners of the applications, and audit which applications are using them. Decide which connections should become the primary with the developers, and ensure that they are all moved to that new connection and tested. . . What connections have been deleted that used to be used? . By parsing the script logs, one can visualize old data connection names/paths that can help to serve as an audit trail. . How-to . Navigate to the Dashboard sheet. . . Select the value of True next to the Connections Only Found in Script KPI. The Connections by Type object will now show all of the connections that were found in script log files, but were not found in the QRS. . . Action . There is no direct action necessary here, as this exercise is only particularly useful in the context of auditing. Leverage as required. . . What are the most widely used data connections? . Depending on how this application is deployed, whether it be strictly administrative or potentially visible to developers, this metric is important both administratively and socially throughout the organization. . How-to . Navigate to the Usage sheet. . . Locate the Connection # Times Used (Script Logs) table. This table visualizes the amount of script logs that each connection is found in. . . Select a connection, and visualize the what applications utilized it and when–as well as by what execution type (Task, Manual, API). . . In the upper right, one can then select the Executed By filter to visualize what users have leveraged the data connection. . INTERNAL sa_scheduler maps to Task executions | INTERNAL sa_api maps to API (ODAG or otherwise) executions | End-user accounts map to Hub executions | . . Action . There is no direct action necessary here. Leverage as required. . . Who is using what data connections? . While “User A” might own “Data Connection A”, “User B” might also have read access to that data connection. This can of course be visualized through the audit capability of the QMC, however this application will physically reveal who is executing any reloads of those data connections, giving greater visibility and allowing a deeper level of auditing and governance. . How-to . The process to answer this question is addressed in the What are the most widely used data connections? section above, and can be achieved by navigating to the Usage sheet and filtering on the Executed By filter. . Action . There is no direct action necessary here. Leverage as required. . . Via what mechanism are data connections being used? . The application visualizes what connections are being run as tasks, manually, or in ODAG (or other API) requests. This is crucial in understanding user behavior. . How-to . The process to answer this question is addressed in the What are the most widely used data connections? section above, and can be achieved by navigating to the Usage sheet and filtering on the ExecutionType filter within the Count of Data Connections by ExecutionType table. . . Action . There is no direct action necessary here. Leverage as required. . Tags . #monthly . #asset_management . #data_connections . #data_connection_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/data_connections/analyze_data_connections.html",
		"relUrl": "/docs/asset_management/data_connections/analyze_data_connections.html"
	},
	"6": {
		"title": "Analyze Security Rules",
		"content": "Analyze Security Rules . Cadence Monthly . Sites developmentproduction .   Initial Recurring . Estimated Time | 30 min | 10 min | . Benefits: . Decrease security rule evaluation time (increase performance) | Ensure readable/scalable rules | . . Goal . The goal of this section is to analyze security rules for quality–ultimately increasing readability, performance, and scalability. This page will explore two options of doing so–one natively through the QMC (manual), and the other via an application created named the qs-security-rules-analyzer. . For auditing, please refer to Audit User Access. . Table of Contents . Readability | Scale | Performance | Security Rule Flagging | QMC - Security Rules | Security Rule Analyzer | . . When writing security rules, it is essential to think about how they will scale and how easy it is for them to be read and understood by others. There are of course times when rules must become quite complex and might be rather difficult to read, but generally, these should be niche scenarios for very granular requirements. . Readability . When writing a security rule, try to break up the formatting into multiple lines, to make the rule more easily digested by a human being. For example, here is a perfectly valid rule: . (resource.resourcetype = &quot;App&quot; and resource.stream.HasPrivilege(&quot;read&quot;) and resource.@AppLevelMgmt.empty()) or ((resource.resourcetype = &quot;App.Object&quot; and resource.published = &quot;true&quot; and resource.objectType != &quot;app_appscript&quot;) and resource.app.stream.HasPrivilege(&quot;read&quot;)) . Now, here is that same rule, formatted differently: . ( resource.resourcetype = &quot;App&quot; and resource.stream.HasPrivilege(&quot;read&quot;) and resource.@AppLevelMgmt.empty() ) or ( ( resource.resourcetype = &quot;App.Object&quot; and resource.published = &quot;true&quot; and resource.objectType != &quot;app_appscript&quot; ) and resource.app.stream.HasPrivilege(&quot;read&quot;) ) . The second is surely far easier to read. . Lastly, include a good Description of the security rule. It is common to simply write out the pseudocode. . Scale . When writing a rule, one must ensure that it can scale. While ((user.name=&quot;Rodion Romanovich Raskolnikov&quot;) or (user.name=&quot;Sofya Semyonovna Marmeladov&quot;)) works for a couple of users, as more and more users come into the system, that 1:1 rule style quickly becomes very difficult to maintain. Also, now imagine that Mr. Raskolnikov now changes roles, and should no longer see the resource that the security rule applies to. One would have to first know that he changed roles, and second, go remove him from that security rule. This simply is not maintainable. Instead, groups should be used, be that user.group or user.environment.group or user.@SomeCustomGroup. This 1:many style rule is highly scalable, and is generally hands off, especially when using the former two options, as they are dynamic from the source, vs hardcoded as a custom property. . To summarize, instead of writing this: . ((user.name=&quot;Rodion Romanovich Raskolnikov&quot;) or (user.name=&quot;Sofya Semyonovna Marmeladov&quot;)) . Write something like this, where the resource has a hardcoded custom property value assigned to it, and that matches up against a dynamic value from the group property, say, from Active Directory. This rule is then completely hands off, and scales to many users dynamically. . ((resource.@Department=user.group)) . Performance . Another area to be very mindful of is the performance of the evaluation of security rules. Of course, a rule with many and/or conditions is going to take longer to evaluate. One thing to be particularly keen on here is the number of values in the custom property field that the security rule is going to be evaluated on. If there are thousands of potential values in the custom property field, this will significantly increase evaluation times. . The following guidelines can be used to aid optimization efforts if required: . Be as specific as possible–more filtered results will perform better (the lower the grain, the better). | . Resource Filter Explanation Efficieny . * | Access to all things in Qlik Sense | Least efficient | . App* | Access to all Apps and App.Objects | More efficient than above | . App_* | Access to all Apps | More efficient than above | . App_d1309075-86e8-4784-a9fd-2658ab47018e | Access the app with the ID d1309075-86e8-4784-a9fd-2658ab47018e | More efficient than above | . Use only the required Context . Security rules have context options of Hub, QMC, or Both. Be as specific as possible. | . | Avoid traversing several object reference boundaries . An example of this would be user.@customproperty=resource.app.stream.@customproperty. In this example the user’s custom property is compared to the resource (the thing) which belongs to an app which belongs in a stream which has a custom property. | . | Minimize the number of custom property values. . Custom properties with 100s of values are expensive to process | . | Order of execution matters. . For example resource.app.stream.owner.@a = &quot;b&quot; or user.name = &quot;user1&quot;. In this example, all owners of stream need to be evaluated for a custom property and only then is the user’s name evaluated. Put the more exclusionary clauses to the rule first. Example: (user.group=&quot;rare&quot; and user.group=&quot;common&quot;). This minimizes the number of users who need to evaluate the 2nd clause. | . | Avoid use of HasPrivilege . Example: App.Stream.HasPrivilege(&quot;read&quot;). This function requires additional rule engine to evaluate the read permission check on the App’s Stream. | . | . Security Rule Flagging . To ensure security rule quality, the following areas are things to look out for either manually via the QMC, or programmatically with the provided application. . Do not create . 1:1 style rules, e.g. . Rules that contain .name . | Rules that contain .id . | . | Rules that contain * . | Rules that contain many and or operators . | Rules that reference a custom property that has many possible values (hundreds or thousands) . | QMC - Security Rules . For manually reviewing security rules, start by navigating to the QMC, and then select Security Rules. . . Click on the Column selector, and add the Conditions column. . . Select the filter on the Conditions column, and then search for any of the bad practices, such as .name, .id, *, etc. . . As an additional step, filter on the Disabled column to No to only view active security rules. . . This is of course a very manual process, and might prove to be rather difficult to consume. For an automated process that will sum up all of the flagged bad practices and allow for deeper analysis, please explore the application below. . Security Rule Analyzer . The qs-security-rule-analyzer application is an application supported by the Americas Enterprise Architecture team from Qlik. It is a very straight forward application that makes to calls to the QRS (repository database) that fetches metadata around custom properties and all security rule information. The application itself takes advantage of the existing monitor_apps_REST_app data connection, so there is no installer and it is plug and play, spare a couple of variable settings and ensuring that the user executing the reload has RootAdmin rights and access to the data connection. Complete setup instructions can be found in the script. . Download it here: qs-security-rule-analyzer . . Tags . #monthly . #asset_management . #security_rules . #security_rule_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/security_rules/analyze_security_rules.html",
		"relUrl": "/docs/asset_management/security_rules/analyze_security_rules.html"
	},
	"7": {
		"title": "Analyze Tasks",
		"content": "Analyze Tasks . Cadence Weekly . Sites production .   Initial Recurring . Estimated Time | 20 min | 10 min | . Benefits: . Decrease batch time | Decrease redundancy | Optimize concurrency | Reduce task failures | . . Goal . Checking for new tasks regularly helps not only to curate what is necessary (cadence, duplicates, etc), but also allows for you to review your batch window, task concurrencies, and any implications against the Qlik schedulers. The above may influence architectural patterns and dictate the need for reload task pinning. . Table of Contents . Batch Time Analysis | Decrease Redundancy | Optimize Concurrency | Reduce Task Failures | . . Batch Time Analysis . As outlined on Optimize Batch Window, use the Reloads Monitor app to review the peak reload hour(s). Significant deviation from previous weeks may signal . that additional resources are needed for the Qlik Sense Enterprise nodes which are performing reloads | Apps have hourly reload cadences which are not needed Example: Out of the box, the License and Operations Monitor reload task have hourly reloads which are generally not needed for most deployments. | . | . Decrease Redundancy . To determine whether apps have duplicative reload tasks, open the Operations Monitor app and Duplicate the Task Details Sheet. . . Give the new sheet a descriptive name. For example Task Details (Duplicates). . . Remove both tables as we will build a custom table for our needs. . . Drag in a table visualization object and add the dimension Task App Name. . . Add a measure. . . Select the measure Task Name and chose the aggregation of Count(). . . Change the label of the measure to a meaningful label like # of Tasks. . . Move the # of Tasks measure to be sorted first. . . Edit the dimension and remove null values. . . The completed table should look similar to this and will signal what apps have duplicative reload tasks. . . Taking the example of the app named 5k Random Data, navigate to the QMC &gt; Apps, go into the App’s record and navigate to the Tasks section. This will show the tasks associated with a given app. . . There may be use cases for apps to have multiple tasks (e.g. run every day at 6AM and noon) but review the duplicates and follow up with the app owner as to whether the currently configured duplicates have business value or are legacy tasks which can be removed. . Optimize Concurrency . As outlined on Optimize Batch Window, ensure that the Scheduler service(s) have an appropriate number of Max Concurrent Reloads for the server(s) that the services are running on. . Reduce Task Failures . For this activity, navigate to the Task Details sheet of the Operations Monitor. . . In the Reload Summary Statistics table, sort by Failure Rate. . . Action . For apps which fail at an extremely high rate. Disable the tasks and alert the app owner. In this example, reload task Reload of Failure of an App fails 100% of the time. There is no need to have this task enabled when it always fails to successfully reload. | For apps which fail at a modest rate, explore further in the Task Details sheet as to whether there are patterns (e.g. time of the day) which are correlated. Common patterns which customers face include: Resource constraint due to multiple reloads | Resource constraint due to a task reloading along side a computationally heavy task | Data source unavailability (e.g. due to maintenance) | . | . To aid in this analysis, in the Reload Details table, select the Reload Status of Failed. . . In the Reload Details table, granular details of the tasks that have failed will allow the Qlik administrator to see correlations. . . Tags . #weekly . #asset_management . #tasks .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/tasks/analyze_tasks.html",
		"relUrl": "/docs/asset_management/tasks/analyze_tasks.html"
	},
	"8": {
		"title": "App Metadata Analyzer",
		"content": "App Metadata Analyzer . developmentproduction . Estimated Configuration Time | 5 min | . Table of Contents . About | Demo &amp; Analysis | Where to get it | Screenshots | Documentation | . . About . The App Metadata Analyzer is now a supported application that is shipped with Qlik Sense as of the September 2019 release. . The App Metadata Analyzer iterates over every application metadata endpoint along with several other QRS calls (Nodes, Apps, Proxies, LB audit), ultimately providing a comprehensive dashboard to analyze your application metadata server-wide. . This allows you to have a holistic view of the makeup of all of your Qlik applications, enabling you to have awareness at a granular level of the types of applications in your organization. This application is 100% native to Qlik without any installer, and is easy to configure within the Qlik Sense Enterprise environment as the app takes advantage of the existing ‘monitor_apps_REST_app’ connection to drive all of the REST calls. . As of the Qlik Sense June 2018 release, a new application level metadata endpoint is available. Data is populated for this endpoint per app post-reload in a June 2018+ environment. You can view this application metadata within your own June 2018+ environment at: . http(s)://&lt;server&gt;/api/v1/apps/&lt;GUID&gt;/data/metadata . where &lt;server&gt; is your Qlik Sense Enterprise server and &lt;GUID&gt; is the application ID. Note that the application does not need to be lifted into RAM for the metadata to be accessed. . Data from this endpoint is derived as part of the app reload process, and therefore does not include any object or expression related metadata. . The data from the endpoint includes: . server metadata including number of server cores, total server RAM | reload time | app RAM base footprint | field metadata including cardinality, tags, total count, RAM size | table metadata including fields, rows, key fields, RAM size | . { &quot;reload_meta&quot;: { &quot;cpu_time_spent_ms&quot;: 12696, &quot;hardware&quot;: { &quot;logical_cores&quot;: 4, &quot;total_memory&quot;: 13018009600 } }, &quot;static_byte_size&quot;: 252583030, &quot;fields&quot;: [ { &quot;name&quot;: &quot;$Field&quot;, &quot;src_tables&quot;: [], &quot;is_system&quot;: true, &quot;is_hidden&quot;: true, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 239, &quot;total_count&quot;: 244, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: false, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$ascii&quot;, &quot;$text&quot;, &quot;$hidden&quot;, &quot;$system&quot;, &quot;$key&quot; ], &quot;byte_size&quot;: 6208 }, { &quot;name&quot;: &quot;$Table&quot;, &quot;src_tables&quot;: [], &quot;is_system&quot;: true, &quot;is_hidden&quot;: true, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 7, &quot;total_count&quot;: 244, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: false, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$ascii&quot;, &quot;$text&quot;, &quot;$hidden&quot;, &quot;$system&quot;, &quot;$key&quot; ], &quot;byte_size&quot;: 110 }, { &quot;name&quot;: &quot;$Rows&quot;, &quot;src_tables&quot;: [], &quot;is_system&quot;: true, &quot;is_hidden&quot;: true, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 5, &quot;total_count&quot;: 7, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$numeric&quot;, &quot;$integer&quot;, &quot;$hidden&quot;, &quot;$system&quot; ], &quot;byte_size&quot;: 70 }, { &quot;name&quot;: &quot;$Fields&quot;, &quot;src_tables&quot;: [], &quot;is_system&quot;: true, &quot;is_hidden&quot;: true, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 5, &quot;total_count&quot;: 7, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$numeric&quot;, &quot;$integer&quot;, &quot;$hidden&quot;, &quot;$system&quot; ], &quot;byte_size&quot;: 59 }, { &quot;name&quot;: &quot;$FieldNo&quot;, &quot;src_tables&quot;: [], &quot;is_system&quot;: true, &quot;is_hidden&quot;: true, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 114, &quot;total_count&quot;: 244, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$numeric&quot;, &quot;$integer&quot;, &quot;$hidden&quot;, &quot;$system&quot; ], &quot;byte_size&quot;: 1374 }, { &quot;name&quot;: &quot;$Info&quot;, &quot;src_tables&quot;: [], &quot;is_system&quot;: true, &quot;is_hidden&quot;: true, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 1, &quot;total_count&quot;: 239, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: false, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$ascii&quot;, &quot;$text&quot;, &quot;$hidden&quot;, &quot;$system&quot; ], &quot;byte_size&quot;: 6 }, { &quot;name&quot;: &quot;Game URL&quot;, &quot;src_tables&quot;: [ &quot;Plays&quot; ], &quot;is_system&quot;: false, &quot;is_hidden&quot;: false, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 2298, &quot;total_count&quot;: 343045, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: false, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$ascii&quot;, &quot;$text&quot; ], &quot;byte_size&quot;: 174648 }, { &quot;name&quot;: &quot;GameID&quot;, &quot;src_tables&quot;: [ &quot;Plays&quot;, &quot;Link&quot;, &quot;FinalScores&quot;, &quot;GamePassStats&quot;, &quot;PlayerPassStats&quot; ], &quot;is_system&quot;: false, &quot;is_hidden&quot;: false, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: true, &quot;cardinal&quot;: 2304, &quot;total_count&quot;: 0, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$numeric&quot;, &quot;$integer&quot;, &quot;$key&quot; ], &quot;byte_size&quot;: 46080 }, { &quot;name&quot;: &quot;PlayerPassStats.yacEPA_Drop&quot;, &quot;src_tables&quot;: [ &quot;PlayerPassStats&quot; ], &quot;is_system&quot;: false, &quot;is_hidden&quot;: false, &quot;is_semantic&quot;: false, &quot;distinct_only&quot;: false, &quot;cardinal&quot;: 5270, &quot;total_count&quot;: 5492, &quot;is_locked&quot;: false, &quot;always_one_selected&quot;: false, &quot;is_numeric&quot;: true, &quot;comment&quot;: &quot;&quot;, &quot;tags&quot;: [ &quot;$numeric&quot; ], &quot;byte_size&quot;: 171202 } ], &quot;has_section_access&quot;: false } . . Demo &amp; Analysis . For a demo video and information on the benefits of the application, see: Analyze App Metadata Analyzer. . . Where to get it . The application can be found in %ProgramData% Qlik Sense Repository DefaultApps on the Qlik Sense Enterprise Server and is titled App Metadata Analyzer.qvf. The latest copy of the app can also be pulled from Qlik Community - App Metadata Analyzer (Windows), and is encouraged as it will always be the most recent, whereas the version shipped with the releases could end up being a few months behind. For example, if a bug is fixed in July timeframe, it might not be shipped with the product until the September release. Also, considering that clients are commonly 2 or 3 releases behind, this is best way to keep the app up to date. . Either copy of the app can be implemented on a Qlik Sense Enterprise site which is June 2018 or newer. The Community page will always have the most recent version, so if for example a site is on June 2019, and a bug was fixed in Sept 2019, the application can be downloaded from the Community page and installed on the June 2019 version. . . Screenshots . . . . . . . Documentation . App Metadata Analyzer - Qlik Help . Tags . #tooling . #apps .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/app_metadata_analyzer.html",
		"relUrl": "/docs/tooling/app_metadata_analyzer.html"
	},
	"9": {
		"title": "Applications",
		"content": "Capacity Plan: Applications . Goal . The goal of this exercise is to identify any applications that could be optimized or load balanced to dedicated engines. . There are a number of areas that should be focused on, including the following: . Candidates for application pinning (load balancing) | Candidates for data model optimization | . Table of Contents . Operations Monitor Confirm Operations Monitor is Operational | . | App Metadata Analyzer | Application Usage Gather Top Applications by Usage | . | App Metadata Analyzer | ODAG / NPrinting / InsightBot | Example Takeaway | . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . App Metadata Analyzer . Please refer to the App Metadata Analyzer page for an overview and relevant documentation links. . . Application Usage . Gather Top Applications by Usage . Select the Session Overview sheet. . . Select the last three months (assuming this exercise is executed quarterly) by selecting the Month field in the User and App Count Trend chart. . . Record the top applications by usage in the Top 50 Apps chart. . App Metadata Analyzer . For the next exercise, the App Metadata Analyzer is required. Confirm that it is setup, and then navigate to the Dashboard sheet. . . Find the intersection of the highly used applications from the Operations Monitor with applications with high base RAM footprints. In the below example, an application that is consistently leveraged has been selected that has a base RAM footprint of ~64 GB RAM. This application has ~600 M records. . . Two steps need to occur here: . If there are mutliple end-user facing engines in the deployment, where is this application currently available? As it is quite large and heavily used, it might not make sense to have it available on all nodes. For example, if there are more than two end-user nodes, it would be worth considering “pinning” this application to a minimum of two nodes for resiliency–allowing the other nodes to not be overloaded. . | Is there an optimization event? It can be noted in the Field Memory Footprint (MB) table that Field17 consumes ~14 GB RAM. Is that field necessary, can it be optimized? For instance, is it a timestamp that can be floored, or a field that can otherwise be broken apart to reduce cardinality? Other areas of interest include: total number of fields in an application, total number of records in an application and/or table, presence of synthetic keys, presence of data islands, etc. Please refer to the Analyze App Metadata Analyzer for examples and documentation. . | Step 1 can quickly be validated by navigating to the App Availability sheet, while that application remains selected. . . Here, it can easily be seen that out of the three available engine nodes, the application is available on all of them. . . ODAG / NPrinting / InsightBot . Is ODAG in play or going to be in play with any applications? . It is best to simply ask the business if unaware, and one can check if ODAG is able to be run by navigating to the QMC and selecting On-demand apps service, and then viewing whether Enable on-demand app service is toggled on or off. . . . Is Qlik NPrinting or Qlik InsightBot on the horizon or in play? . It is important here to recognize how many applications are being used against both. It is encouraged to ask the business. . Example Takeaway . Candidates for “App Pinning” Candidates for Data Model Optimization ODAG Apps Qlik NPrinting Apps Qlik InsightBot Apps . 2 | 3 | 1 | 0 | 0 | . Tags . #capacity_plan . #apps . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_update_capacity_plan/applications.html",
		"relUrl": "/docs/system_planning/review_update_capacity_plan/applications.html"
	},
	"10": {
		"title": "Apps",
		"content": "Apps .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps.html",
		"relUrl": "/docs/asset_management/apps.html"
	},
	"11": {
		"title": "Architecture 101",
		"content": "Architecture 101 (Components, Terminology) . Goal . The goal of this page is to understand the basic terminology and components/services of a site. . Table of Contents . Architectural Components | Terminology | . . Architectural Components . There are a variety of components used by Qlik Sense Enterprise. This table will outline them as comprehension of the components is necessary for deciding on an architecture. . Name Description . Qlik Sense Hub | Drag and drop development, analysis, and self-service environment. | . Qlik Sense Management Console | Centralized management of all aspects of a Qlik Sense deployment. | . Qlik Sense Proxy (QPS) | Entry point into Qlik Sense for users and administrators. Manages Authentication (last mile), manages sessions / license provisioning, able to load balance across engines. | . Qlik Sense Engine (QES) | In-memory, associative data indexing engine. Used for reloads and app consumption. | . Qlik Sense Scheduler (QSS) | Scheduling engine for application reloads from data sources. | . Qlik Sense Repository (QRS) | Centralized storage of deployment information. | . Qlik Sense Repository Database | A PostgreSQL database which persists metadata relating to the Qlik site. | . Qlik Sense Applications (.QVF) | Centralized storage of Applications before loading into memory, as part of a centralized SMB file share. | . For additional documentation regarding the Qlik services, please refer to Services. . . Terminology . With Qlik Sense Enterprise architecture, a site is used to refer to a potentially distributed cluster of nodes. All members of this site share the same QMC and use the same license key. . A node refers to a specific server and is often referred to by its purpose, i.e. Engine node. . . Tags . #architecture . #scale .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_architecture_scale_plan/architecture_101.html",
		"relUrl": "/docs/system_planning/review_architecture_scale_plan/architecture_101.html"
	},
	"12": {
		"title": "Archive Old Archived Logs",
		"content": "Archive Old Archived Logs . Cadence Quarterly . Sites all .   Initial Recurring . Estimated Time | 30 min | 15 min | . Benefits: . Reduce storage costs | Simplify troubleshooting | . . Goal . The goal for this activity is to purge or archive old logs from a Qlik Sense Enterprise deployment. This allows the administrator to reduce the storage costs of their Qlik Sense deployment and increase troubleshooting effectiveness by reducing number of logs which need to be searched / parsed / etc. . When completing this activity, the administrator needs to confirm whether the log files for their Qlik deployment must be retained for compliance purposes. The answer to this will determine whether the cleanup process focuses on archiving or purging old logs. This can include local laws, contractual obligations (e.g. an ELA with Qlik), or business standards like PCI, etc. . Insofar as the administrator is permitted to archive or purge log files, the administrator then needs to determine how far back they intend to use those log files for Qlik apps which monitor their Qlik Sense Enterprise site. For example, if the administrator needs to report on license usage yearly in as part of an internal busines review, then they will need to retain their logs for at least 1 year. . Table of Contents . Default consumption period for Monitoring Apps | Practical retention periods | Script for Archiving / Purging | . Default consumption period for Monitoring Apps . In order to come to a decision on what retention period is appropriate, an initial barier is the bundled monitoring apps which come with every Qlik Sense Enterprise deployment. If the administrator needs to use those tools to answer specific adoption / utilization questions, then the requisite log files which are used as sources for the monitoring apps need to be present. . Tool Volume of Data Primary data source . Operations Monitor | 3 Months | log files on disk | . License Monitor | 12 Months | log files on disk | . Log Monitor | 7 Days | log files on disk | . Reloads Monitor | 12 Months | log files on disk | . Sessions Monitor | 12 Months | log files on disk | . Sense Performance Analyzer | 7 Days | Centralized Logging Database | . App Metadata Analyzer | N/A | APIs | . Practical retention periods . Practically for most organizations, retention periods will vary between tiers of their Qlik deployment (if applicable) with production tiers retaining logs longer than non-production tiers. In general the minumum retention period which makes sense would be: . Production: 18 months | Development: 6 months | Sandbox: 1-2 months | . Again this is a baseline, will need to be adjusted for the legal, contractual, and compliance standards the organization has. . Script for Archiving / Purging . cacheinitializer_deploy.log . # Specify the age threshold which is desired # e.g. 60 would move all logs older than 60 days $days = &quot;60&quot; # Usage https://technet.microsoft.com/en-us/library/cc733145(v=ws.11).aspx # /e applies to files and subdirectories in the path # /mov moves the files $option1 = &quot;/mov&quot; $option2 = &quot;/e&quot; # Path of Qlik Sense Logs, typically the Archived Logs $source = &quot;C: QlikShare ArchivedLogs&quot; # Path of where the log files should be moved $dest = &quot;C: OldLogs&quot; # Remove logs y/n $removelogs = &quot;n&quot; #Checking to see if the $dest path exists, else create it if(!(Test-Path -Path $dest )){ New-Item -ItemType directory -Path $dest } # Passing the current directory for log creation # Start core robocopy call $scriptDir = Split-Path -Path $MyInvocation.MyCommand.Definition -Parent &amp; robocopy $source $dest $option1 $option2 /MINAGE:$days /LOG:$scriptDir robolog.log /MT # Deletes files if $removelogs = y If ($removelogs -eq &#39;y&#39;) {Remove-Item $dest -Force -Recurse} Else {&quot;Files moved&quot;} . Tags . #quarterly . #backup_and_archiving . #logs . #archive .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/backup_and_archiving/archive_old_archived_logs.html",
		"relUrl": "/docs/backup_and_archiving/archive_old_archived_logs.html"
	},
	"13": {
		"title": "Asset Management",
		"content": "Asset Management .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management",
		"relUrl": "/docs/asset_management"
	},
	"14": {
		"title": "Audit",
		"content": "Audit .",
		"url": "https://adminplaybook.qlik-poc.com/docs/audit",
		"relUrl": "/docs/audit"
	},
	"15": {
		"title": "Audit User Access",
		"content": "Audit User Access * . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 30 min | 20 min | . Benefits: . Ensure Security | . . Goal . This activity will center around auditing what assets (e.g. apps, streams, data connections) that users of the Qlik site have access to. This audit can be done on an ad-hoc basis using the QMC or in a more systemic way using script(s) + a Qlik app. Auditing provides a validation for the configured security rules. This section is listed as ideal to do quarterly so that the Qlik administrator can be generally aware of who has access to what in their environment but practically an audit should be performed before and after a change in the security rules in the environment. . Table of Contents . QMC - Audit Tips | . | qs-security-audit | . . QMC - Audit . To audit access using the QMC, navigate to the Audit section of the QMC. . . The available options are displayed across the top pane of the Audit section. . . Target Resource: The Qlik thing you are auditing. Examples include: Streams, Apps, etc. | (Optional) Filtering of 1: In this section you can filter to a subset of the type of thing selected for 1. | Users: In this section you can filter on the user(s) which you are auditing. | Environment: Select whether you are auditing Hub or QMC access, or both. | The resulting audit will highlight whether the user(s) have access to the Qlik thing(s). . . From the documentation, we can interpret the colors used in the audit: . Green: The Security Rule which provides access is valid and enabled | Yellow: The Security rule which (would) provide access is valid but disabled | Red The Security rule which (would) provide access is invalid | . In this example, Andrew has Read and Publish access to the Monitoring Apps stream. By selecting the cell for the action that we are interested in, we can see what rule(s) are providing access. . . . Tips . Be sure to set an appropriate context. Security rules can be applied to the QMC, Hub, or Both. The default value for this is Both which will result in showing access if the user has access to the asset either in the QMC and/or Hub. If this is the intended audit, then the default choice is acceptable. But for most use cases, the administrator will want to be specific about the context. | . . (1) : In the Upper right hand corner the administrator can select Privileges to audit which will allow the administrator to select which security rule actions they are auditing. | (2) : The list of available Privileges changes as the administrator changes the Target Resource since the types of action(s) which can be applied to a Qlik entity inside of a security rule varies. For example, the Export Data action is applied at the App level. That action neither exists nor makes sense at the Stream level since Streams do not contain data. | . . The administrator can audit Security, Load Balancing, or License rules using the Audit interface. | . . If using Session Attributes (example), the administrator can simulate the effect should the attribute be present. Since session attributes are not stored by Qlik Sense Enterprise, the administrator will need to type / paste in the values here. | . . qs-security-audit . Coming Soon . Tags . #quarterly . #audit . #users .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/audit/audit_user_access.html",
		"relUrl": "/docs/audit/audit_user_access.html"
	},
	"16": {
		"title": "Backup & Archiving",
		"content": "Backup &amp; Archiving .",
		"url": "https://adminplaybook.qlik-poc.com/docs/backup_and_archiving",
		"relUrl": "/docs/backup_and_archiving"
	},
	"17": {
		"title": "Cache Warming",
		"content": "Cache Warming . production . Estimated Configuration Time | 1 hr | . Table of Contents . About Frequency of cache warming | . | Options CacheInitializer Usage | Limitations | . | Butler-Cache Warming Usage | Limitations | . | Qlik Sense Scalability Tools Usage | Limitations | . | . | . About . Cache Warming in Qlik Sense Enterprise refers to programmatically opening Qlik applications so that the base application is preloaded into RAM before users attempt to open the application. When users open and interact with a Qlik application, there are three types of caching which occur. This can be visualized using this example: . . (taken from Qlik Engine Technical Brief) . In this example, the types are symbolized by colors: . Grey : Base App RAM cache | Dark Green : User session cache | Light Green : Cached result set cache | . The Base App cache is the result of the Qlik Engine opening the Qlik app from disk and uncompressing it into RAM. This process is what is occurring when a user first opens an app (or if the app opens on a new engine) and must wait for the app to render. The user session cache is the meta-data of a user’s session with a given app. This includes things like the current selections and all previous selections. This cache is generally quite low in terms of a percentage since it’s pure metadata; think metadata like country=canada rather than any resulting calculations from that selection. The Cached result set cache is the cache of the calculations which resulted from all user’s selections. This would be something like Sum(Sales) or Count(DISTINCT ProductId) when the user had country=canada selected. . For many deployments of Qlik Sense Enterprise, an administrator may want to reduce the time spent waiting for an app to initially load. To achieve this, a cache warming process needs to be run. . Frequency of cache warming . The Qlik Engine is configured to retain the Base App RAM for a period of time. By default this time period is 28,800 seconds (or 8 hours) but can be configured in the QMC &gt; Engines &gt; Edit &gt; Apps &gt; App cache time (seconds): . . This means that the administrator needs to plan to execute a cache warming frequency that is aware of this configuration. Practically the two options are: . Execute a cache warming process before the first users are expected to access the application | Execute a cache warming process at an interval which smaller than the App cache time (seconds) configuration | While these two options can be combined, the straight-forward guidance is that option (1) is ideal for deployments which are have regional user access (e.g. standard US or Europe timezones only) and/or have computationally expensive batch reloads running during off-hours which would conflict with the cache warming process. Option (2) is ideal for deployments which are expected to serve users across multiple time zones. . Options . In order to cache warm a Qlik app, the administrator needs to run a process which can programmatically open a Qlik app and optionally make selections. The options covered in this guide which can achieve this result are: . Tool Technical Approach Complexity Flexibility . CacheInitializer | .NET SDK | Low | Medium/High | . Butler-Cache Warming | JavaScript | Low/Medium | Medium | . Qlik Sense Scalability Tools | .NET SDK | Medium | High | . CacheInitializer . The CacheInitializer is a project initially developed by a Product Manager at Qlik, but has since been taken on by the America’s Presales Enterprise Architecture team. It is a project built in C# using the .NET SDK to: . open apps | (optionally) pre-cache the visualizations on the app’s sheets | (optionally) pass selections | It is a very lightweight tool which is ideal for an initial introduction to cache warming or for smaller environments where the limitations of this tool are not problematic. . Usage . To use the CacheInitializer tool, the administrator will download the tool from the project’s releases page (or optionally download the source files and compile themselves in Visual Studio). The release will include the primary executable and supporting files for the tool’s execution. Available parameters are: . -s, --server Required. URL to the server. -a, --appname App to load (using app name) -i, --appid App to load (using app ID) -p, --proxy Virtual Proxy to use -o, --objects (Default: False) cycle through all sheets and objects -f, --field field to make selections in e.g Region -v, --values values to select e.g &quot;France&quot;,&quot;Germany&quot;,&quot;Spain&quot; --help Display this help screen. . The required parameters are server and app (specified by Name or ID). The optional parameters allow the administrator to specify a virtual proxy (-p or --proxy) if Windows authentication is not the prefixless virtual proxy, open up the objects on each sheet in the app (-o or --objects), pass through selections (-v or --values) on given fields (-f or --field). . The tool will use Windows authentication to open the app as the user who is executing the process. This means that if this tool is used, the administrator should execute it in the context of a dedicated user who both has a license assigned to them and the required permissions to open the app using security rules (and section access if used). . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;While the parameters have both short and long forms, this guide will use the long forms for consumability purposes.&lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This guide will use &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--appname&lt;/code&gt; for consumability purposes. It is recommended to use &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;--appid&lt;/code&gt; to ensure precision of the app being opened.&lt;/p&gt; . The simplest scenario involves opening a Qlik app on a specified server so that the Base App cache is built: CacheInitializer.exe --server https://qliksense.company.com --appname &quot;Operations Monitor&quot; the tool will then provide the resulting style of output: . 02:27:54 - Operations Monitor: Opening app 02:27:55 - Operations Monitor: App open 02:27:55 - Operations Monitor: App cache completed 02:27:55 - Cache initialization complete. Total time: 00:00:07.3192225 . A more complex scenario involves opening a Qlik app on a specified server and opening all visualizations within the Qlik App. This will build out the Base App cache as well as seed the Cached result sets cache. Example: . CacheInitializer.exe --server https://qliksense.company.com --appname &quot;Operations Monitor&quot; --objects the tool will then provide the resulting style of output: . 02:30:49 - Operations Monitor: Opening app 02:30:50 - Operations Monitor: App open 02:30:50 - Operations Monitor: Clearing Selections 02:30:50 - Operations Monitor: Getting sheets 02:30:50 - Operations Monitor: Number of sheets - 17, getting children 02:30:50 - Operations Monitor: Number of objects - 144, caching all objects 02:30:51 - Operations Monitor: Objects cached 02:30:51 - Operations Monitor: App cache completed 02:30:51 - Cache initialization complete. Total time: 00:00:06.5812126 . The next example will combine the previous example along with specifying a specific virtual proxy (named windows) and passing selections for the Cached result sets cache. Example: . CacheInitializer.exe --server https://qliksense.company.com --proxy windows --appname &quot;Operations Monitor&quot; --objects --field &quot;App Name&quot; --values &quot;Operations Monitor&quot;,&quot;License Monitor&quot; the tool will then provide the resulting style of output: . 02:33:55 - Operations Monitor: Opening app 02:33:56 - Operations Monitor: App open 02:33:56 - Operations Monitor: Clearing Selections 02:33:56 - Operations Monitor: Applying Selection: App Name = Operations Monitor 02:33:56 - Operations Monitor: Getting sheets 02:33:56 - Operations Monitor: Number of sheets - 17, getting children 02:33:56 - Operations Monitor: Number of objects - 144, caching all objects 02:33:57 - Operations Monitor: Objects cached 02:33:57 - Operations Monitor: Clearing Selections 02:33:57 - Operations Monitor: Applying Selection: App Name = License Monitor 02:33:57 - Operations Monitor: Getting sheets 02:33:57 - Operations Monitor: Number of sheets - 17, getting children 02:33:57 - Operations Monitor: Number of objects - 144, caching all objects 02:33:57 - Operations Monitor: Objects cached 02:33:57 - Operations Monitor: App cache completed 02:33:57 - Cache initialization complete. Total time: 00:00:09.1996060 . In this last example, a specific application is opened across multiple Qlik Engine nodes (using dedicated virtual proxies for each Engine) both with no selections and with defined selections: . # Function for logging function Get-TimeStamp { return &quot;[{0:MM/dd/yyyy} {0:HH:mm:ss}]&quot; -f (Get-Date) } &lt;# Add Logging #&gt; Set-Location $PSScriptRoot $logdir = $PSScriptRoot + &#39; logs&#39; $logfile = $logdir + &quot; &quot; + &quot;CacheInitializer-Deploy-&quot; + (Get-Date -Format yyyyMMdd) + &quot;.log&quot; if (!(Test-Path $logdir)){ New-Item -path $logdir -type directory | Out-Null } # Set the working directory Set-Location &quot;C: temp CacheInitializer&quot; # Set the name of the Qlik Sense server whose Qlik Proxy Service is hosting the virtual proxies $server = &#39;https://qliksense.company.com&#39; &lt;# CacheInitializer Options: -s, --server Required. URL to the server. -a, --appname App to load (using app name) -i, --appid App to load (using app ID) -p, --proxy Virtual Proxy to use -o, --objects (Default: False) cycle through all sheets and objects -f, --field field to make selections in e.g Region -v, --values values to select e.g &quot;France&quot;,&quot;Germany&quot;,&quot;Spain&quot; --help Display this help screen. #&gt; # Define the virtual proxy prefixes $vps = @( &quot;02windows&quot; , &quot;03windows&quot; , &quot;04windows&quot; ) # Define the app $appid = &#39;40c69bba-1825-4109-985c-399af8e96e63&#39; Write-Output &quot;$(Get-TimeStamp): Begin Cache Warm for $($appid)&quot; | Out-File -FilePath $logfile -Append # Loop over each virtual proxy and execute the CacheInitializer foreach ($vp in $vps) { Write-Output &quot;$(Get-TimeStamp): Connecting to $($server) on $($vp)&quot; | Out-File -FilePath $logfile -Append $results = &#39;&#39; $results = . CacheInitializer.exe --server $($server) --proxy $vp --appid $appid --objects if ($results.count -gt 1) { Write-Output &quot;Successfully cached $($appid) on $($vp)&quot; | Out-File -FilePath $logfile -Append $results | Out-File -FilePath $logfile -Append } else { Write-Output &quot;Cache Failure on $($vp) for $($appid) on $($vp)&quot; | Out-File -FilePath $logfile -Append } # Example of passing selection states for a given field and accompanying value(s) Write-Output &quot;$(Get-TimeStamp): Connecting to $($server) on $($vp)&quot; | Out-File -FilePath $logfile -Append $results = &#39;&#39; $results = . CacheInitializer.exe --server $($server) --proxy $vp --appid $appid --objects --field &quot;Country&quot; --values &quot;United States&quot;,&quot;Germany&quot; if ($results.count -gt 1) { Write-Output &quot;Successfully cached $($appid) on $($vp) with selections&quot; | Out-File -FilePath $logfile -Append $results | Out-File -FilePath $logfile -Append } else { Write-Output &quot;Cache Failure on $($vp) for $($appid) on $($vp) with selections&quot; | Out-File -FilePath $logfile -Append } } . Resulting log file: . cacheinitializer_deploy.log . [03/07/2020 14:51:20]: Begin Cache Warm for 40c69bba-1825-4109-985c-399af8e96e63 [03/07/2020 14:51:20]: Connecting to https://qliksense.company.com on 02windows Successfully cached 40c69bba-1825-4109-985c-399af8e96e63 on 02windows 02:51:23 - Scalability Sample - PT Sales Analytics-50M: Opening app 02:51:24 - Scalability Sample - PT Sales Analytics-50M: App open 02:51:24 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:24 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:24 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:25 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:25 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:25 - Scalability Sample - PT Sales Analytics-50M: App cache completed 02:51:25 - Cache initialization complete. Total time: 00:00:04.6379880 [03/07/2020 14:51:25]: Connecting to https://qliksense.company.com on 02windows Successfully cached 40c69bba-1825-4109-985c-399af8e96e63 on 02windows with selections 02:51:28 - Scalability Sample - PT Sales Analytics-50M: Opening app 02:51:29 - Scalability Sample - PT Sales Analytics-50M: App open 02:51:29 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:29 - Scalability Sample - PT Sales Analytics-50M: Applying Selection: Country = United States 02:51:29 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:29 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Applying Selection: Country = Germany 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:30 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:30 - Scalability Sample - PT Sales Analytics-50M: App cache completed 02:51:30 - Cache initialization complete. Total time: 00:00:04.9299998 [03/07/2020 14:51:30]: Connecting to https://qliksense.company.com on 03windows Successfully cached 40c69bba-1825-4109-985c-399af8e96e63 on 03windows 02:51:34 - Scalability Sample - PT Sales Analytics-50M: Opening app 02:51:34 - Scalability Sample - PT Sales Analytics-50M: App open 02:51:34 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:34 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:35 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:35 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:35 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:35 - Scalability Sample - PT Sales Analytics-50M: App cache completed 02:51:35 - Cache initialization complete. Total time: 00:00:04.5729952 [03/07/2020 14:51:35]: Connecting to https://qliksense.company.com on 03windows Successfully cached 40c69bba-1825-4109-985c-399af8e96e63 on 03windows with selections 02:51:39 - Scalability Sample - PT Sales Analytics-50M: Opening app 02:51:39 - Scalability Sample - PT Sales Analytics-50M: App open 02:51:39 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:39 - Scalability Sample - PT Sales Analytics-50M: Applying Selection: Country = United States 02:51:40 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:40 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:40 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Applying Selection: Country = Germany 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:41 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:41 - Scalability Sample - PT Sales Analytics-50M: App cache completed 02:51:41 - Cache initialization complete. Total time: 00:00:05.3330192 [03/07/2020 14:51:41]: Connecting to https://qliksense.company.com on 04windows Successfully cached 40c69bba-1825-4109-985c-399af8e96e63 on 04windows 02:51:44 - Scalability Sample - PT Sales Analytics-50M: Opening app 02:51:45 - Scalability Sample - PT Sales Analytics-50M: App open 02:51:45 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:45 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:46 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:46 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:46 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:46 - Scalability Sample - PT Sales Analytics-50M: App cache completed 02:51:46 - Cache initialization complete. Total time: 00:00:04.7609894 [03/07/2020 14:51:46]: Connecting to https://qliksense.company.com on 04windows Successfully cached 40c69bba-1825-4109-985c-399af8e96e63 on 04windows with selections 02:51:50 - Scalability Sample - PT Sales Analytics-50M: Opening app 02:51:50 - Scalability Sample - PT Sales Analytics-50M: App open 02:51:50 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:50 - Scalability Sample - PT Sales Analytics-50M: Applying Selection: Country = United States 02:51:50 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:51 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:51 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:51 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:51 - Scalability Sample - PT Sales Analytics-50M: Clearing Selections 02:51:51 - Scalability Sample - PT Sales Analytics-50M: Applying Selection: Country = Germany 02:51:51 - Scalability Sample - PT Sales Analytics-50M: Getting sheets 02:51:52 - Scalability Sample - PT Sales Analytics-50M: Number of sheets - 4, getting children 02:51:52 - Scalability Sample - PT Sales Analytics-50M: Number of objects - 28, caching all objects 02:51:52 - Scalability Sample - PT Sales Analytics-50M: Objects cached 02:51:52 - Scalability Sample - PT Sales Analytics-50M: App cache completed 02:51:52 - Cache initialization complete. Total time: 00:00:05.0779960 . Limitations . Concurrency : Since this tool opens applications over the Qlik Proxy Service, the user who executes the process will be limited to 5 executions in a 5 minute period. This is due to a throttle on the number of parallel sessions which a license can use. For most cache warming scenarios, this isn’t a major issue, although this limitation may be encountered when doing initial testing of the process. | Multinode : If an administrator needs to open a given app across multiple Qlik Engine nodes, then they will need to have separate virtual proxies using Windows Authentication which are each attached to a single Qlik Engine. This ensures that the application will be opened across each Qlik Engine node. Building off the Concurrency limitation above, this means that the CacheInitializer tool will only be able to cache warm a given app across 5 separate Qlik Engine nodes during a 5 minute period. If attempting to build a cache warming process for multiple apps across multiple engines, then this 5 distinct session limitation needs to be accounted for. | . Butler-Cache Warming . An alternative project using NodeJS to interact with Qlik’s Engine API(s) is Butler-Cache Warming which was developed by Göran Sander. Göran is a consultant and Qlik Luminary who previously has worked with a very large Qlik customer to help deploy Qlik at scale. The tool provides the ability to: . open apps | cycle through sheets to build out the Cached result sets cache | define a schedule to open applications | The tool communicates with the Qlik Engine over the Qlik Engine API port (4747) and uses an internal Qlik account (sa_repository) to open applications programmatically. Due to this, the administrator will not need to account for license assignment nor for security rule permissions. A license is not needed to access an app using sa_repository (or any internal account) since no interactive user can use these accounts. Likewise, the default security rule ServiceAccount ensures that the internal accounts have the requisite authorization to access any Qlik app. If section access is used, then modification of the section access table to include INTERNAL sa_repository will be required with the default configuration present in Butler-Cache Warming. . Usage . Since configuration of this tool is covered on the tool’s GitHub page, this guide will only review the high level configuration needed. . Open up a command prompt: set NODE_ENV=production | Edit butler-cw-master config apps.yaml Enter an appropriate server or servers | Enter an appropriate app GUID(s) | Enter an appropriate schedule (every X units, e.g. every 5 minutes, every 1 hour) | . | Edit butler-cw-master config production.yaml Set an appropriate logDirectory | . | If running on one of the members of the Qlik Sense cluster, then proceed to execution If not, grab the client.pem and client_key.pem from C: ProgramData Qlik Sense Repository Exported Certificates .Local Certificates on one of the members NodeJS needs to be installed on the server | . | Adjust the clientCertPath &amp; clientCertKeyPath paths in production.yaml to the path where those certificates live | . | Execute in a command prompt: If on one of the members of the Qlik Sense Cluster, this works: cd C: temp butler-cw-master Where this is the path of the project | . | C: Program Files Qlik Sense ServiceDispatcher Node &quot;node.exe index.js | . | If on another server: cd C: temp butler-cw-master Where this is the path of the project | . | node index.js | . | . | . An example yaml file is: . apps: - server: qlikserver02 appId: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 appDescription: Cache warm big app on Node02 appStepThroughSheets: true freq: every 5 hours - server: qlikserver03 appId: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 appDescription: Cache warm big app on Node03 appStepThroughSheets: true freq: every 5 hours - server: qlikserver04 appId: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 appDescription: Cache warm big app on Node04 appStepThroughSheets: true freq: every 5 hours - server: qlikserver02 appId: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d appDescription: Cache warm app with section access on Node02 appStepThroughSheets: true freq: every 5 minutes - server: qlikserver03 appId: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d appDescription: Cache warm app with section access on Node03 appStepThroughSheets: true freq: every 5 minutes - server: qlikserver04 appId: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d appDescription: Cache warm app with section access on Node04 appStepThroughSheets: true freq: every 5 minutes . And the resulting example output is: . C: temp butler-cw-master&gt;&quot;C: Program Files Qlik Sense ServiceDispatcher Node &quot;node.exe index.js 2020-02-24T00:27:14.205Z info: Starting Qlik Sense cache warmer. 2020-02-24T00:27:15.601Z info: App loaded: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d 2020-02-24T00:27:15.631Z info: App bb5b386b-9bf5-4aaf-b946-3cceb7eb409d: Cached 1 visualizations on 1 sheets. 2020-02-24T00:27:15.700Z info: App loaded: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 2020-02-24T00:27:15.756Z info: App loaded: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d 2020-02-24T00:27:15.772Z info: App bb5b386b-9bf5-4aaf-b946-3cceb7eb409d: Cached 1 visualizations on 1 sheets. 2020-02-24T00:27:16.015Z info: App 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0: Cached 3 visualizations on 1 sheets. 2020-02-24T00:27:16.036Z info: App loaded: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 2020-02-24T00:27:16.179Z info: App loaded: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d 2020-02-24T00:27:16.195Z info: App bb5b386b-9bf5-4aaf-b946-3cceb7eb409d: Cached 1 visualizations on 1 sheets. 2020-02-24T00:27:16.393Z info: App 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0: Cached 3 visualizations on 1 sheets. 2020-02-24T00:29:21.126Z info: App loaded: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 2020-02-24T00:30:00.160Z info: App loaded: 4ebe6de0-ab6e-4c35-b6dd-70cc1c27ebe0 2020-02-24T00:30:00.385Z info: App loaded: bb5b386b-9bf5-4aaf-b946-3cceb7eb409d 2020-02-24T00:30:00.408Z info: App bb5b386b-9bf5-4aaf-b946-3cceb7eb409d: Cached 1 visualizations on 1 sheets. . Limitations . Connectivity : Since this tool communicates with the Qlik Engine Services directly, the Qlik Engine API port 4747 will need to be open between the server executing Butler-CW and the Qlik Sense node(s). The primary benefit of this approach is that the session limitation present for CacheInitializer and Qlik Sense Scalability Tools will not affect Butler-CW. | Selections : This tool does not have the ability to pass selection states to the Qlik app. This means that the Cached result set cache will not be warmed with selections. | . Qlik Sense Scalability Tools . The last project that will be reviewied is the Qlik Sense Scalability Tools. This is a project created by Qlik’s Performance and Scalability team within R&amp;D. It is an open source release of one of the key pieces of tooling that Qlik’s R&amp;D team uses to test scalability and performance of Qlik Sense Enterprise. It is a compiled project using the .NET SDK to: . open apps | (optionally) pre-cache the visualizations on the app’s sheets | (optionally) pass selections | While the Scalability Tools package is primarily used for scale testing Qlik applications, it does allow for command line execution, which means that it can be used for cache warming activities. When using this approach, the administrator needs to ensure that the user/users being simulated both has/have a license assigned to them and the required permissions to open the app using security rules (and section access if used). . Usage . After downloading the package, the administrator will configure initial connectivity (Youtube video on initial setup) and build a scenario (Youtube video on building a test) which describes the actions that the simulated user will perform in the app. In the scenario, you can optionally pass one to many selection values in the context of the entire application or on a per-sheet basis. . Once the scenario is built and initial success is found using the process, the administrator can execute the scenario via command line like so: . C: scalabilitytools SDKExerciser Sep2018 SDKExerciserConsole.exe config=&quot;C: scalabilitytools Scenario app1-cachewarm.json&quot; configname=&quot;1-app1-cachewarm&quot; server=qliksenseserver02 app=&quot;325c3daa-1aeb-434c-9c1d-c7ce14ee201a&quot; ssl=True headername=X-Qlik-User virtualproxyprefix=header iterations=1 usernameprefix=CacheWarmUser concurrentusers=1 rampupdelay=2.00 executiontime=3600 instancenumber=1 appmode=Open newuserforeachiteration=True afteriterationwait=NoWait afteriterationwaittime=0 logDir=&quot;C: scalabilitytools Results cachewarm&quot; . This execution defines the scenario to run (config), the Qlik Proxy to connect to (server), the header based virtual proxy to authenticate against (virtualproxyprefix + it’s corresponding headername), the app to open (app), and the user’s prefix (usernameprefix). . Limitations . Concurrency : Since this tool opens applications over the Qlik Proxy Service, the user who executes the process will be limited to 5 executions in a 5 minute period. This is due to a throttle on the number of parallel sessions which a license can use. For most cache warming scenarios, this isn’t a major issue, although this limitation may be encountered when doing initial testing of the process. | Multinode : If an administrator needs to open a given app across multiple Qlik Engine nodes, then they will need to have separate virtual proxies using Header Authentication which are each attached to a single Qlik Engine. This ensures that the application will be opened across each Qlik Engine node. Building off the Concurrency limitation above, this means that the Qlik Sense Scalability Tool will only be able to cache warm a given app across 5 separate Qlik Engine nodes during a 5 minute period for each scenario. If attempting to build a cache warming process for multiple apps across multiple engines, then the administrator can alter the usernameprefix parameter to emulate different users to avoid the Concurrency limitation above, at the cost of additional license consumption. | Authentication : Since this tool uses Header authentication to emulate users, it is recommended to only attach the header virtual proxy(ies) to back-end nodes which are not user facing. Header authentication is intrinsically insecure and thus can constitute a security concern for most organizations without separating this authentication to back-end nodes. | . Tags . #tooling .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/cache_warming.html",
		"relUrl": "/docs/tooling/cache_warming.html"
	},
	"18": {
		"title": "Check for New Apps",
		"content": "Check for New Apps * . Cadence Weekly . Sites developmentproduction .   Initial Recurring . Estimated Time | 5 Min | 5 min | . Benefits: . Awareness of new applications | . . Goal . While the idea of simply checking for new applications seems relatively trivial and not particularly actionable, it is a good practice as it only takes a couple of minutes and can increase reaction times to the presence of large applications. This page illustrates three methods of visualizing/gathering that high-level application data on newly created applications, so that the administrator can be aware/potentially report on it. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This page will outline three methods for accomplishing this activity (two using the QMC and another using a Qlik CLI for Windows script). The QMC approach is generally appropriate for most environments. The Qlik CLI for Windows approach is more appropriate for environments where automation is required.&lt;/p&gt; . Table of Contents . QMC - Apps | Operations Monitor Confirm Operations Monitor is Operational | . | Hub - Operations Monitor | Get List of New Apps (Qlik CLI for Windows) Script | . | . . QMC - Apps . In the QMC, select Apps: . . In the upper right hand side of the screen, select the Column selector, and then select the File size (MB) and Created columns. To make the resulting table a bit more manageable, optionally deselect additional columns like Version and Tags. . . Now select the filter icon for the Created column, and then select the filter of Last seven days, or the desired range. . . Lastly, one can review the resulting table and view any new apps, noting their file sizes. If any are particularly large, it might be worthwhile to follow-up with the owner of the application, and possibly do further analysis in with the App Metadata Analyzer. . . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Hub - Operations Monitor . Open up the Hub and navigate to the Monitoring apps stream. Select the Operations Monitor application. . . From the App overview page, select the Apps sheet. . . Select Duplicate, as a column will be added that isn’t currently in a table. . . In Edit mode, select the App Details table, and add the App Created Date field. . . It is now possible to sort by that column to view new apps. In addition, feel free to add the App File Size field as well to filter by large applications only. . . . Get List of New Apps (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . The script will bring back any application that is greater than or equal to x days old and greater than or equal to z bytes. The script will then store the output into a desired location in either csv or json format. . Script . # Function to collect applications that were created in the last x days over z size in bytes ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;machineName&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date $daysBack = 7 # set the byte size threshold for application disk size (if only large apps are desired) $byteSize = 0 # directory for the output file $filePath = &#39;C: tmp &#39; # desired filename of the output file $fileName = &#39;new_apps&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # set the date to the current time minus $daysback $date = (Get-Date -date $(Get-Date).AddDays(-$daysBack) -UFormat &#39;+%Y-%m-%dT%H:%M:%S.000Z&#39;).ToString() # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts | Out-Null # check the output format # GET all apps that are created &gt;= $date and &gt;= $byteSize # output results to $outfile If ($outputFormat.ToLower() -eq &#39;csv&#39;) { Get-QlikApp -filter &quot;createdDate ge &#39;$date&#39; and fileSize ge $byteSize&quot; -full | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { Get-QlikApp -filter &quot;createdDate ge &#39;$date&#39; and fileSize ge $byteSize&quot; -full | ConvertTo-Json | Set-Content $outFile } . Tags . #weekly . #asset_management . #apps . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/check_new_apps.html",
		"relUrl": "/docs/asset_management/apps/check_new_apps.html"
	},
	"19": {
		"title": "Check for New Data Connections",
		"content": "Check for New Data Connections * . Cadence Weekly . Sites production .   Initial Recurring . Estimated Time | 5 Min | 5 min | . Benefits: . Decrease redundancy | Increase awareness | . . Goal . Checking for new data connections on a regular basis is one of the ways to help curb the amount of connections that exist in the environment. It allows for the ability to spot duplicates ahead of time, track what sources are being used, and catch any potential out-of-process additions. For a much deeper analysis of data connections, please refer to the Data Connection Analyzer. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This page will outline two methods for accomplishing this activity (using the QMC and using a Qlik CLI for Windows script). The QMC approach is generally appropriate for most environments. The Qlik CLI for Windows approach is more appropriate for environments where automation is required.&lt;/p&gt; . Table of Contents . QMC - Data Connections | Get List of New Data Connections (Qlik CLI for Windows) Script | . | . . QMC - Data Connections . In the QMC, select Data Connections: . . In the upper right hand side of the screen, select the Column selector, and then select the Connection String, Type, and Created columns. . . Now select the filter icon for the Created column, and then select the filter of Last seven days, or the desired range. . . Lastly, review the resulting table and view any new data connections. . . . Get List of New Data Connections (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . The script will bring back any data connection with a Created Date that is greater than or equal to x days old. The script will then store the output into a desired location in either csv or json format. . Script . # Function to collect data connections that were created in the last x days ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date $daysBack = 7 # directory for the output file $filePath = &#39;C: tmp &#39; # desired filename of the output file $fileName = &#39;new_data_connections&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # set the date to the current time minus $daysback $date = (Get-Date -date $(Get-Date).AddDays(-$daysBack) -UFormat &#39;+%Y-%m-%dT%H:%M:%S.000Z&#39;).ToString() # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # check the output format # get all data connections that are created &gt;= $date # output results to $outfile If ($outputFormat.ToLower() -eq &#39;csv&#39;) { Get-QlikDataConnection -filter &quot;createdDate ge &#39;$date&#39;&quot; -full | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { Get-QlikDataConnection -filter &quot;createdDate ge &#39;$date&#39;&quot; -full | ConvertTo-Json | Set-Content $outFile } . Tags . #weekly . #asset_management . #data_connections .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/data_connections/check_new_data_connections.html",
		"relUrl": "/docs/asset_management/data_connections/check_new_data_connections.html"
	},
	"20": {
		"title": "Check for New Streams",
		"content": "Check for New Streams * . Cadence Weekly . Sites developmentproduction .   Initial Recurring . Estimated Time | 2 min | 2 min | . Benefits: . Increase awareness | Increase reaction times | . . Goal . Checking for new streams and ensuring that stream governance is tightly controlled is an important aspect of Qlik management. If streams are being created regularly, it is a potential sign that the way assets are organized might not be optimal, or potentially that users/LOBs’ are trying to go around a certain process. Ideally, very few individuals should have the right to create streams, so it is an important thing to keep an eye on to ensure nothing is out of the ordinary. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This page will outline two methods for accomplishing this activity (using the QMC and using a Qlik CLI for Windows script). The QMC approach is generally appropriate for most environments. The Qlik CLI for Windows approach is more appropriate for environments where automation is required.&lt;/p&gt; . Table of Contents . QMC - Streams | Get List of New Streams (Qlik CLI for Windows) Script | . | . . QMC - Streams . In the QMC, select Streams: . . In the upper right hand side of the screen, select the Column selector, and then select the Owner and Created columns. . . Now select the filter icon for the Created column, and then select the filter of Last seven days, or the desired range. . . Lastly, review the resulting table and view any new streams. . . . Get List of New Streams (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . The script will bring back any streams with a Created Date that is greater than or equal to x days old. The script will then store the output into a desired location in either csv or json format. . Script . # Script to collect streams that were created in the last x days ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date $daysBack = 7 # directory for the output file $filePath = &#39;C: tmp &#39; # desired filename of the output file $fileName = &#39;new_streams&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # set the date to the current time minus $daysback $date = (Get-Date -date $(Get-Date).AddDays(-$daysBack) -UFormat &#39;+%Y-%m-%dT%H:%M:%S.000Z&#39;).ToString() # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # check the output format # get all streams that are created &gt;= $date # output results to $outfile If ($outputFormat.ToLower() -eq &#39;csv&#39;) { Get-QlikStream -filter &quot;createdDate ge &#39;$date&#39;&quot; -full | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { Get-QlikStream -filter &quot;createdDate ge &#39;$date&#39;&quot; -full | ConvertTo-Json | Set-Content $outFile } . Tags . #weekly . #asset_management . #streams .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/streams/check_new_streams.html",
		"relUrl": "/docs/asset_management/streams/check_new_streams.html"
	},
	"21": {
		"title": "Check for New/Modified Security Rules",
		"content": "Check for New/Modified Security Rules * . Cadence Weekly . Sites developmentproduction .   Initial Recurring . Estimated Time | 5 min | 5 min | . Benefits: . Increase awareness | Increase reaction times | . . Goal . Ensuring that security rules are tightly managed and governed is arguably the most important aspect of managing a Qlik site from an administrator’s perspective. This area is critical to ensuring the right people have access to the appropriate resources, and have the appropriate privileges to act on those resources. It is important to see if new rules are being created, and it is also very important to check rules that have recently been modified. This section will cover both. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This page will outline two methods for accomplishing this activity (using the QMC and using a Qlik CLI for Windows script). The QMC approach is generally appropriate for most environments. The Qlik CLI for Windows approach is more appropriate for environments where automation is required.&lt;/p&gt; . Table of Contents . QMC - Security Rules | Get List of New/Modified Security Rules (Qlik CLI for Windows) Script | . | Backup Security Rules | . . QMC - Security Rules . In the QMC, select Security Rules: . . In the upper right hand side of the screen, select the Column selector, and then select the Created, Last Modified, and Modified by columns. . . Now select the filter icon for the Created column, and then select the filter of Last seven days, or the desired range. . . Lastly, review the resulting table and view any new security rules. Repeat this process for the Last modified column, reviewing what security rules were modified and by whom. . . Get List of New/Modified Security Rules (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . The script will bring back any security rules with a Created Date or Modified Date that is greater than or equal to x days old. The script will then store the output into a desired location in either csv or json format. . Script . # Function to collect security rules that were created or modified in the last x days ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date $daysBack = 7 # directory for the output file $filePath = &#39;C: tmp &#39; # desired filename of the output file $fileName = &#39;new_security_rules&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # set the date to the current time minus $daysback $date = (Get-Date -date $(Get-Date).AddDays(-$daysBack) -UFormat &#39;+%Y-%m-%dT%H:%M:%S.000Z&#39;).ToString() # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # check the output format # get all security rules that were modified or created &gt;= $date # output results to $outfile If ($outputFormat.ToLower() -eq &#39;csv&#39;) { Get-QlikRule -filter &quot;(createdDate ge &#39;$date&#39; or modifiedDate ge &#39;$date&#39;) and category eq &#39;Security&#39;&quot; -full | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { Get-QlikRule -filter &quot;(createdDate ge &#39;$date&#39; or modifiedDate ge &#39;$date&#39;) and category eq &#39;Security&#39;&quot; -full | ConvertTo-Json | Set-Content $outFile } . Backup Security Rules . Given the Qlik CLI for Windows script above, that script could actually be modified to pull security rules (by removing the filter) on a scheduled basis and store them out to separate files at a desired cadence, so that if an administrator wanted to “roll back” changes, they could. Refer to an example here: Qlik Support - Exporting and Importing Security Rules. . Tags . #weekly . #asset_management . #security_rules .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/security_rules/check_security_rules.html",
		"relUrl": "/docs/asset_management/security_rules/check_security_rules.html"
	},
	"22": {
		"title": "Check for New/Modified Custom Properties",
		"content": "Check for New/Modified Custom Properties * . Cadence Weekly . Sites developmentproduction .   Initial Recurring . Estimated Time | 5 min | 5 min | . Benefits: . Increase awareness | Increase reaction times | . . Goal . Regularly checking for new/modified custom properties allows the administrator to track what/how users are controlling security and management across the Qlik environment. . Some of the outcomes: . Discover new custom properties so that their usage can be identified (e.g. through contacting the creator). | Identify modified custom properties so that their expanded usage can be identified and tracked. | Regulate the amount of custom property value options, as there can be performance impacts if there are many. | Identify whether individual custom properties are being leveraged for security rules or for management/automation purposes. | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This page will outline two methods for accomplishing this activity (using the QMC and using a Qlik CLI for Windows script). The QMC approach is generally appropriate for most environments. The Qlik CLI for Windows approach is more appropriate for environments where automation is required.&lt;/p&gt; . Table of Contents . QMC - Custom Properties | Get List of New/Modified Custom Properties (Qlik CLI for Windows) Script | . | . . QMC - Custom Properties . In the QMC, select Custom Properties: . . In the upper right hand side of the screen, select the Column selector, and then select the Created, Last modified, and Modified by columns. . . Now select the filter icon for the Created column, and then select the filter of Last seven days, or the desired range. . . Take the time to review the results and potentially reach out to their creators if they are brand new custom properties. Ensure that the same steps above are followed to filter on Last modified as well. . Next, it is encouraged to select the custom property and view its values and its applicable resources. . . Lastly, it is simple to check if the custom property is being used in any security rules. Navigate to the Security Rules section in the QMC. . . In the upper right hand side of the screen, select the Column selector, and then select the Conditions column. . . Now select the filter icon for the Conditions column, and then enter the name of the custom property, prepended by the @ symbol, which denotes the use of a custom property in a security rule, e.g. @Department. . . If any security rules are using this rule, they will be visible, so the rule can then be explored to confirm whether it is enabled or disabled. . . . Get List of New/Modified Custom Properties (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . The script will bring back any custom properties that have been created or modified within the last x days. The script will then store the output into a desired location in either csv or json format. . Script . # Function to collect custom properties that were created or modified in the last x days ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date $daysBack = 7 # directory for the output file $filePath = &#39;C: tmp &#39; # desired filename of the output file $fileName = &#39;new_custom_properties&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # set the date to the current time minus $daysback $date = (Get-Date -date $(Get-Date).AddDays(-$daysBack) -UFormat &#39;+%Y-%m-%dT%H:%M:%S.000Z&#39;).ToString() # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # check the output format # get all custom properties that are created/modified &gt;= $date # output results to $outfile If ($outputFormat.ToLower() -eq &#39;csv&#39;) { Get-QlikCustomProperty -filter &quot;createdDate ge &#39;$date&#39; or modifiedDate ge &#39;$date&#39;&quot; -full | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { Get-QlikCustomProperty -filter &quot;createdDate ge &#39;$date&#39; or modifiedDate ge &#39;$date&#39;&quot; -full | ConvertTo-Json | Set-Content $outFile } . Tags . #weekly . #custom_properties .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/custom_properties/custom_properties.html",
		"relUrl": "/docs/asset_management/custom_properties/custom_properties.html"
	},
	"23": {
		"title": "Custom Properties",
		"content": "Custom Properties .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/custom_properties.html",
		"relUrl": "/docs/asset_management/custom_properties.html"
	},
	"24": {
		"title": "Data Connection Analyzer",
		"content": "Data Connection Analyzer . developmentproduction . Estimated Configuration Time | 15 min | . Table of Contents . About | Analysis | Common Questions &amp; Associated Benefits of the App | Screenshots | Where to get it | Documentation | . . About . The Qlik Sense Data Connection Analyzer is a Qlik application that parses script log files and queries the QRS API, allowing analysis of data connection usage, patterns, age, and cleanliness across a Qlik Sense site. In short, this app will help to run a leaner, more performant, and more easily and holistically governed Qlik Sense site. . The application is supported by Qlik’s Americas Enterprise Architecture team, and uses only native connections and capabilities of the Qlik platform–making it plug-and-play . . Analysis . For a walkthrough of all how to answer all of the questions in the section below, please refer to Analyze Data Connections. . . Common Questions &amp; Associated Benefits of the App . Which connections are no longer used? (A data connection is found in a script log and exists in the Qlik Sense site, however no associated applications currently exist that at one point had used it). . Removing unused connections will increase performance across a site, as less connections will have to be evaluated in security rule evaluations. If a site has hundreds or thousands of connections, this calculation time can build up. Removing unused connections makes general administration easier, as there is less to manage. | . What connections have never been used? (A data connection exists in the Qlik Sense site, but no reference exists to it in any script log.) . It is common that users will create data connections to test connectivity, but then never actually use them. By leveraging this app, one can identify connections that have never been used and have existed for x amount of time (say 90+ days), so that action can be taken to remove them. The benefits from both simpler management and performance are listed in the bullet above. | . What connections have been deleted that used to be used? (A data connection that is found in script logs, but no longer exists in the Qlik Sense site and no app is using it.) . By parsing the script logs, one can visualize old data connection names/paths that can help to serve as an audit trail. | . What are the most widely used data connections? . Depending on how this application is deployed, whether it be strictly administrative or potentially visible to developers, this metric is important both administratively and socially throughout the organization. | . Do we have duplicate data connections? . By analyzing the connection strings, one can tackle duplicate connections to the same source data. This eases administration overhead and will ensure that there is reusability/consistency across the platform. | . Who is using what data connections? . While “User A” might own “Data Connection A”, “User B” might also have read access to that data connection. This can of course be visualized through the audit capability of the QMC, however this application will physically reveal who is executing any reloads of those data connections, giving greater visibility and allowing a deeper level of auditing and governance. | . Where are data connections being used? . Let’s say a data source is being transformed and will be moved from one database to another location. The first question one might ask is, “What applications are using that connection, so we can re-route it to the new db and make adjustments to the load scripts?”. This historically has not been easy to answer. This application allows one to select that connection and visualize apps that are using it. | . Via what mechanism are data connections being used? . The application visualizes what connections are being run as tasks, manually, or in ODAG (or other API) requests. This is crucial in understanding user behavior. | . . Screenshots . . . . . . . . Where to get it . The application can be found on GitHub under the EA Powertools repository here: Data Connection Analyzer. . . Documentation . Complete documentation can be found here: Data Connection Analyzer - Documentation. . Tags . #tooling . #data_connections .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/data_connection_analyzer.html",
		"relUrl": "/docs/tooling/data_connection_analyzer.html"
	},
	"25": {
		"title": "Data Connections",
		"content": "Data Connections .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/data_connections.html",
		"relUrl": "/docs/asset_management/data_connections.html"
	},
	"26": {
		"title": "Example Production Architectures",
		"content": "Example Production Architectures . Goal . The goal of this page is to provide example generic production architectures to be used for reference. . Table of Contents . Example Production Architectures Single Node Qlik site | Two Server Qlik Site | Three Server Qlik Site | Four Server Qlik Site | Five Server Qlik Site | . | . . Example Production Architectures . Single Node Qlik site . . Features: . Single Qlik Sense node | . This is an incredibly common starting point for Qlik sites. It allows for rapid deployment at the cost of a lack of separation of roles (back end vs. front end) and resiliency. . Two Server Qlik Site . . Features: . Segregation of front and back end activities | . This also is an incredibly common starting point for Qlik sites. Likewise is the next logical step for organizations who have deployed Qlik but desire a more performant site. This architecture allows for a separation of back and front end activities and thus is ideal starter architecture for sites which (1) service multiple departments and/or (2) require intra-day reloads. By separating the back-end reload activities, user experience on app consumption is more reliable. . Three Server Qlik Site . . Features: . User Resiliency | Horizontal scaling | . This architecture shows horizontal scaling on consume nodes which allows for resiliency and more compute to be delivered to app consumption. . Four Server Qlik Site . . Features: . Horizontal Scaling | User resiliency | Reload resiliency | Site-wide high availability | . This architecture shows horizontal scaling on both the front and back end nodes of Qlik. By segregating PostgreSQL and the persistence layer for Qlik, site-wide high availability is possible. This is a common Enterprise architecture for multi-department Qlik sites. . Five Server Qlik Site . . Features: . Horizontal Scaling | User resiliency | Reload resiliency | Site-wide high availability | . This architecture further horizontally scales the front end compared to the Four Server Qlik site to meet additional compute needs for app consumption. Most organizations require more compute for consumption of apps compared to app reloads. . For further documentation and examples, please refer to Qlik Sense Enterprise on Windows: multi-node deployment. . Tags . #architecture . #scale .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_architecture_scale_plan/example_production_architectures.html",
		"relUrl": "/docs/system_planning/review_architecture_scale_plan/example_production_architectures.html"
	},
	"27": {
		"title": "Extension Usage Dashboard",
		"content": "Extension Usage Dashboard . developmentproduction . Estimated Configuration Time | 15 min | . Table of Contents . About | Questions answered by the app | Documentation | . . About . The Qlik Sense Extension Usage Dashboard is a Qlik application that parses application meta-data to uncover which applications use extension objects. The sources for this data are the meta-data fetch from the Qlik Sense Telemetry project and QRS API calls. By combining these, we can see which apps use extensions, where those extensions are used, who are the users who use extensions, which extensions are not used, as well as which extension usages could be replaced by a bundled visualization from Qlik. . . Questions answered by the app . Which applications in my Qlik Sense Enterprise site use extensions? | For apps that use extensions, on what sheets are they? | Which extensions are unused? | Which apps use extensions which can be migrated to bundled visualization objects? | With the Extension Usage Dashboard, BI admins can lower their carrying costs of extensions, focus efforts during upgrades, and move their Qlik apps to using bundles from Qlik which come with full support by Qlik. . Documentation . Complete documentation can be found on the Extension Usage Dashboard GitHub page. . Tags . #tooling . #extensions .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/extension_usage_dashboard.html",
		"relUrl": "/docs/tooling/extension_usage_dashboard.html"
	},
	"28": {
		"title": "Extensions",
		"content": "Extensions .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/extensions.html",
		"relUrl": "/docs/asset_management/extensions.html"
	},
	"29": {
		"title": "Flag Unused Base/Community Sheets",
		"content": "Flag Unused Base/Community Sheets * . Cadence Monthly . Sites production w/ self-service .   Initial Recurring . Estimated Time | 1-2 hr | 30 min | . Benefits: . Increase performance | Reduce maintenance | Improve focus | . . Goal . There are three primary goals: . Remove (or consider modifying) “Base” sheets that are not being used. This will focus down your applications and remove clutter, while also increasing performance of the site. | Keep “Community” sheets under control. In large environments where self-service is enabled, both private (personal sheets on a published application), and community sheets can grow rapidly out of control – especially if the application’s base sheets doesn’t offer what the end-user is looking for, causing them to create their own sheets. | Finding out “why” users are not using certain base sheets and/or why they are creating and publishing many community sheets, as that content could potentially be made part of the standardized application. | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;“Private” sheets should be handled a bit differently–please refer to: &lt;a href=&quot;/docs/asset_management/apps/remove_unused_private_sheets.html&quot;&gt;Remove Unused Private Sheets&lt;/a&gt;&lt;/p&gt; . Table of Contents . Suggested Prerequisites | Audit Activity Log | Operations Monitor Confirm Operations Monitor is Operational | . | Identification of Unused Sheets | Suggested Actions | Bulk Community Sheet Removal Script to Tag Unused Community Sheets | Script to Delete Tagged Sheets Prerequisites | Steps | . | . | . . Suggested Prerequisites . Remove/Quarantine Unused Apps | . . Audit Activity Log . As of the February 2019 release, an improvement was added to the product to log sheet usage at default log levels. This enables the ability to measure sheet adoption as well as manage the amount of sheets in the applications–keeping them trimmed to only what is being leveraged. . Ensure that the Audit Activity log level is set to Basic for every engine. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This is the default setting, but it is encouraged for the Qlik administrator to confirm what is configured for their environment(s).&lt;/p&gt; . . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Identification of Unused Sheets . This usage information is then surfaced inside of the Operations Monitor on the Sheet Usage sheet. . . As an example, we’ll select the Telemetry Dashboard application on one of our rarely used internal servers, and we can see that the App Profiling base sheet hasn’t been accessed in, actually almost exactly one year. . . It is suggested that the administrator would add the App Owner field to the Sheet Usage table, as this table already contains the relevant information needed to report on usage, and the owner field is need to know who to contact. . . As far as the time range for sheets that are unused (or minimally used), it is suggested to select the &gt; 90 days value from the Latest Activity Range field – though this range is ultimately up to the organization. . . . Suggested Actions . Once the table has been built out and the filters and time ranges have been decided upon, it is suggested to then contact the owners either manually or programmatically via something like NPrinting. It is advised that it should be the owner’s responsibility to decide what to do with these unused sheets. The app owners can then be responsible for contacting the community sheet owners for their individual cleanup. Base sheets should be considered the most critical sheets to address, with community sheets following. . In addition to the above, it is entirely possible that users aren’t leveraging sheets because they potentially aren’t positioned (ordered) properly, users are unaware of them, users don’t understand how to leverage them, or users possibly aren’t interested in the data presented on them. Rather than simply remove them, is is encouraged for the app owners to understand why they are not being leveraged to better the applications and overall Qlik experience. . . Bulk Community Sheet Removal . The below script snippet requires the Qlik CLI for Windows. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;Base sheets should never be removed programmatically.&lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;When possible, one should always remove community sheets manually, leaving that responsibility to the owner of the applications. That being said, if there are potentially thousands of community sheets that need to be removed, and this is the first time the organization is starting this management process, it is possible to programmatically remove these assets. This would generally be a one-time operation, as it is suggested to do this process monthly, which should be able to be maintained incrementally.&lt;/p&gt; . The script below will tag any community sheets with the tag UnusedCommunitySheet. It expects an Excel file (XLSX) as an input, where the name of the column with the Sheet Id is specified. This allows for the Qlik Administrator to export a filtered down list from the Sheet Usage table in the Sheet Usage sheet of the Operations Monitor. . The below script assumes that the desired Tag has already been created, e.g. UnusedCommunitySheet. . Script to Tag Unused Community Sheets . #Requires -Modules ImportExcel # Assumes the ImportExcel module: `Install-Module -Name ImportExcel` # Function to tag community sheet ids from excel and tag them # Assumes tag exists, such as &#39;UnusedCommunitySheet&#39; # GUID validation code referenced from: https://pscustomobject.github.io/powershell/functions/PowerShell-Validate-Guid-copy/ ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date # fully qualified path to excel file with sheet ids $inputXlsxPath = &#39;&lt;absolute file path&gt;/&lt;filename&gt;.xlsx&#39; # column number of sheet id column in Excel file $sheetIdColumnNumber = &#39;9&#39; # the desired name of the tag to tag sheets with - it must exist in the QRS $tagName = &#39;UnusedCommunitySheet&#39; # directory for the output file $outFilePath = &#39;C: &#39; # desired filename of the output file $outFileName = &#39;tagged_community_sheets&#39; ################ ##### Main ##### ################ # set the output file path $outFile = ($outFilePath + $outFileName + &#39;.csv&#39;) # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # if the output file already exists, remove it if (Test-Path $outFile) { Remove-Item $outFile } # function to validate GUIDs function Test-IsGuid { [OutputType([bool])] param ( [Parameter(Mandatory = $true)] [string]$ObjectGuid ) [regex]$guidRegex = &#39;(?im)^[{(]?[0-9A-F]{8}[-]?(?:[0-9A-F]{4}[-]?){3}[0-9A-F]{12}[)}]?$&#39; return $ObjectGuid -match $guidRegex } # import sheet ids from excel $data = Import-Excel $inputXlsxPath -DataOnly -StartColumn $sheetIdColumnNumber -EndColumn $($sheetIdColumnNumber + 1) # validate GUIDs and only use those (handles nulls/choosing wrong column) $sheetIds = $data | foreach { $_.psobject.Properties } | where Value -is string | foreach { If(Test-IsGuid -ObjectGuid $_.Value) {$_.Value} } # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # add headers to output csv Add-Content -Path $outFile -Value $(&#39;SheetObjectName,SheetObjectSheetId,SheetObjectAppId,SheetObjectAppName&#39;) # GET desired tag JSON $tagsJson = Get-QlikTag -filter &quot;name eq &#39;$tagName&#39;&quot; -raw # get the id of the tag $tagId = $tagsJson.id # if the tag exists if($tagsJson) { # for each tag foreach ($sheetId in $sheetIds) { # GET the object, ensuring it is a community sheet $sheetObjJson = Get-QlikObject -filter &quot;published eq true and approved eq false and id eq $sheetId&quot; -full -raw # if the object exists and is a community sheet if ($sheetObjJson) { # set a flag to check if the tag is already assigned to the sheet $tagAlreadyThere = $false # get the current tags assigned to sheet, if any $currentTags = $sheetObjJson.tags $currentTags # for each tag foreach ($tag in $currentTags) { # if the target tag is already there, set the flag to &quot;true&quot; if ($tagId -eq $tag.id) { $tagAlreadyThere = $true break } else { continue } } # get the sheet name, app id, and app name $sheetObjName = $sheetObjJson.name $sheetObjAppId = $sheetObjJson.app.id $sheetObjAppName = $sheetObjJson.app.name # if the tag isn&#39;t already there, add it if (!$tagAlreadyThere) { $sheetObjJson.tags += $tagsJson # convert to JSON for the PUT $sheetObjJson = $sheetObjJson | ConvertTo-Json # PUT the sheet with the new tag Invoke-QlikPut -path /qrs/app/object/$sheetId -body $sheetObjJson } # write output Add-Content -Path $outFile -Value $($sheetObjName + &#39;,&#39; + $sheetId + &#39;,&#39; + $sheetObjAppId + &#39;,&#39; + $sheetObjAppName) } # the sheet is not a community sheet else { $sheetId + &#39; is not a community sheet. Skipping.&#39; } } } # the tag doesn&#39;t exist else { &quot;Tag: &#39;&quot; + $tagName + &quot;&#39; doesn&#39;t exist. Please create it in the QMC.&quot; } . Script to Delete Tagged Sheets . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;It is highly recommended to &lt;em&gt;backup your site and applications&lt;/em&gt; before considering taking the approach of programmatic sheet removal. This process cannot be reversed. The sheet pointers are stored in the repository database, and the sheets reside within the qvfs themselves.&lt;/p&gt; . In order to completely remove sheets from both an application and the repository database, the Qlik Engine JSON API must be used. To work with this API, the sample script leverages Enigma.js. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;If it is attempted to use the QRS API to remove sheets instead of the Engine API, only the “pointers” to those sheets will be removed from the repository database–the sheet information itself stored inside of the qvf will not be removed. This is why the Engine API must be leveraged for programmatic deletion, as it purges both.&lt;/p&gt; . Prerequisites . NodeJS | . This process uses NodeJS to interact with the Qlik Engine JSON API. To confirm that NodeJS is installed and properly configured, run the following commands in cmd.exe: . node --version | npm --version | . Steps . Download the following files from here and place them in a desired folder. remove_tagged_community_sheets.js | package.json | . | Edit the following mandatory variables in remove_tagged_community_sheets.js host | TAG_TO_SEARCH_FOR | . | Open a cmd prompt, and navigate to the folder from step 1. | Enter npm install | To execute the program, enter node remove_tagged_community_sheets.js | Refer to both log.txt and output.csv | Tags . #monthly . #asset_management . #apps . #sheets . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/flag_unused_base_community_sheets.html",
		"relUrl": "/docs/asset_management/apps/flag_unused_base_community_sheets.html"
	},
	"31": {
		"title": "License Maintenance",
		"content": "License Maintenance . Cadence Monthly . Sites all .   Initial Recurring . Estimated Time | 20 min | 10 min | . Benefits: . Ensure all licenses can be leveraged | Ensure desired users have access | . Goal . The goal of this activity is to keep up with general license maintenance, specifically focusing on the following exercises: . Remove license allocations for inactive users | Consider/review license allocations for users that are being denied access | Table of Contents . License Monitor Confirm License Monitor is Operational | . | Removing License Allocations for Inactive Users | Check for License Denials | . . License Monitor . This page leverages the License Monitor. Please refer to the License Monitor page for an overview and relevant documentation links. . Confirm License Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the License Monitor application. Confirm that the application’s data is up-to-date. . . If the License Monitor is not up-to-date, please refer to the License Monitor Documentation for configuration details and troubleshooting steps. . . Removing License Allocations for Inactive Users . To check for inactive users, go to the QMC and select the Users section. . . Select the filter icon above the Inactive column and select Yes. The list of users will be filtered to those who are inactive. . . A user is marked inactive when a configured User Directory Connector (UDC) can no longer can find the user in the 3rd party user directory (AD, LDAP etc…). Such users are candidates for license dealloaction if they have left the company or moved to a department or group that no longer has access–the latter comes into play when there is an LDAP filter on the UDC. . To remove a license allocation, go to the QMC and select License Management . . The license allocation will be present under Professional access allocations or Analyzer access allocations. Check both sections to find the inactive users. . . To deallocate a user, select the user from the appropriate allocation screen and select Deallocate at the bottom of the screen. . . If a License Rule is present to allocate licenses to users automatically, check if the license rule should be updated. . . Check for License Denials . From the Hub select the Monitoring apps stream and then select the License Monitor application. . . Select the User Detail sheet. . . At the top, select the Date filter pane and select the most recent dates. On the right, the Denied Access object will show a list of users who have been denied access. These users are candidates for being allocated a new license. . . When users are getting a denied access, it means that they have successfully authenticated to Qlik Sense and have been identified, but they have not yet been allocated a license. It may be because of a misconfiguration in the current license allocation rule, or it may be because a new group is interested in using the platform and that group is not yet configured in the allocation rule, as they were not anticipated. . If a User Directory Connector is in use, then more information about the denied user can be determined by accessing the QMC and going to the User section. . . Click the info icon next to the user to bring up a pop-up set of additional properties. . . When a user directory connector is used, typically a group (i.e. AD Group) will be presented for additional information. . Depending on what is found out about the user, the follow-up action is either to simply allocate a license to the single user, or to update the licence allocation rules from the License Management section of the QMC. . To allocate a single license, go to the QMC and select the License Management section. . . Depending on the level of capability required, select Professional Allocations or Analyzer Allocations. . . Next, select Allocate at the bottom, select the same user that was denied, and select Allocate. . . Optionally, a license rule can be created or updated to automatically allow new users acces from the License Management screen. Select Professional access rules or Analyzer access rules. . . For more information on access rules consult the Qlik Sense help: . Qlik Help - Create Professional Access Rule | Qlik Help - Create Analyzer Access Rule | . Tags . #monthly . #licensing . #license . #users . #license_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/licensing/license_maintenance.html",
		"relUrl": "/docs/licensing/license_maintenance.html"
	},
	"32": {
		"title": "License Monitor",
		"content": "License Monitor . developmentproduction . Table of Contents . About | License Monitor sheets | Screenshots | Documentation | . . About . The License Monitor it is already embedded in Qlik Sense and can be found at Monitoring apps stream. . The License Monitor loads service logs to populate charts and tables covering token allocation, usage of login and user passes, and errors and warnings. . For a more detailed description of the sheets and visualizations, visit the story About the License Monitor that is available from the app overview page, under Stories. . For the location and naming convention of the log files, see Logging. . If you have a user-based license with professional and analyzer access, you will instead see figures relevant to that license type. . License Monitor sheets . The License Monitor sheets display Qlik Sense performance on the current node, and, when properly configured for multi-node (as described in Configuring multi-node environments), the app includes information across all nodes. . Sheet Content . Overview | Displays an overview of unallocated access versus total access, the available and total analyzer capacity (in minutes), summary data about login and user access sessions over the last 7, 28, and 90 days, changes in the allocation of license tokens over the last 7 days, and license usage over time. | . User Detail | Allows the user to select a time period over which to display user access pass sessions, the number of users starting sessions, and the individual users starting sessions. | . Usage by App | Allows the user to select a time period over which to display the apps for which access passes are being used and the number of tokens consumed by each app. | . Timeline | Displays token usage over time so administrators can monitor usage and anticipate future token allocation needs. | . User Access History | Allows the user to select a time period over which to display user access pass sessions, the number of users starting sessions, and the individual users starting sessions. Only valid for token-based license. | . Login Access History | Allows the user to select a time period over which to display login pass utilization, login access users, and denials of login access. Only valid for token-based license. | . Allocation History | Displays the latest changes and changes over selected times to the allocation of license tokens to login and user access passes. | . Usage Snapshot | Overview sheet providing snapshot view of license allocation and historical usage. | . Unified Licensing History | Displays the license usage for Qlik Sense and QlikView side by side. To see the QlikView license usage, the new monitor_apps_qlikview_logs data connection must point to the folder containing the QlikView Server logs. You update the data connection in the QMC. | . Log Details | Lists servers in the cluster and provides details about license usage entered in server’s logs. | . Data in the License Monitor is updated when the app is reloaded. Data is not live. ————————- . Screenshots . . . . . . . . . . . Documentation . License Monitor - Qlik Help | Youtube Video - STT - Configuring Monitoring Apps in Qlik Sense | Youtube Video - Configuring Qlik Sense Enterprise Monitoring Apps for Multi Node | Youtube Video - Qlik Fix: Troubleshooting Qlik Sense Monitoring Apps | . Tags . #tooling .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/license_monitor.html",
		"relUrl": "/docs/tooling/license_monitor.html"
	},
	"33": {
		"title": "Licenses",
		"content": "Capacity Plan: Licenses . Goal . The goal of this exercise is to identify license usage across a production site. The License Monitor application exposes this information simply, so that it can be easily referenced. . There are a number of metrics that should be focused on, including the following for Professional, Analyzer, and Capacity license types. If the site is on tokens, the same principles will apply. . The following should be followed for the Professional and Analyzer access types: . Licenses | Licenses Allocated | Licenses Allocated Unused | Licenses Remaining | . Table of Contents . License Monitor Confirm License Monitor is Operational | . | Gather License Usage Metrics | Example Takeaway | . . License Monitor . This page leverages the License Monitor. Please refer to the License Monitor page for an overview and relevant documentation links. . Confirm License Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the License Monitor application. Confirm that the application’s data is up-to-date. . . If the License Monitor is not up-to-date, please refer to the License Monitor Documentation for configuration details and troubleshooting steps. . . Gather License Usage Metrics . Select the Usage Snapshot sheet in the License Monitor. . . The four metrics listed in the Goals section can all be found in this object . . Example Takeaway . If the usage metrics above were to be used as an example (obviously this is a rarely used testing box), a table that could be used for capacity planning could look like the following: .   Licenses Licenses Allocated Licenses Allocated Unused Licenses Remaining . Professional | 100 | 10 | 8 | 90 | . Analyzer | 100 | 0 | 0 | 100 | . Tags . #capacity_plan . #licenses . #licensing . #users . #license_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_update_capacity_plan/licenses.html",
		"relUrl": "/docs/system_planning/review_update_capacity_plan/licenses.html"
	},
	"34": {
		"title": "Licensing",
		"content": "Licensing .",
		"url": "https://adminplaybook.qlik-poc.com/docs/licensing",
		"relUrl": "/docs/licensing"
	},
	"35": {
		"title": "Load Balancing Concepts",
		"content": "Load Balancing Concepts . Goal . The goal of this page is to understand what the term “Load Balancing” can be in the context of a Qlik site. . Table of Contents . Load Balancing | . . Load Balancing . When it comes to load balancing within the scope of Qlik, there are three general areas that are applicable: . Load balancing across Qlik proxies Third-party network load balancer required | . E.g. F5 BigIP, NGINX, Netscaler, AWS ALB, Azure Application Gateway, etc | Requires support for websockets and sticky sessions - Required for Qlik Proxy resilience | . | Load balancing across Qlik Engines Native to Qlik at the proxy level | . Pure round robin - Option to plug in custom load balancer | . | Load balancing rules for applications Native capability allows for “pinning” of applications to specific engines | . | For documentation/examples around load balancing rules, please refer to Creating load balancing rules with custom properties. . Tags . #architecture . #load_balancing . #scale .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_architecture_scale_plan/load_balancing_concepts.html",
		"relUrl": "/docs/system_planning/review_architecture_scale_plan/load_balancing_concepts.html"
	},
	"36": {
		"title": "Check for New Tasks",
		"content": "Check for Tasks . Cadence Weekly . Sites production .   Initial Recurring . Estimated Time | 2 min | 2 min | . Benefits: . Increase awareness | Increase reaction times | . . Goal . Checking for new tasks regularly helps not only to curate what is necessary (cadence, duplicates, etc), but also encourages reviews of batch windows, task concurrencies, and any other implications against the Qlik schedulers. The above may influence architectural patterns and dictate the need for reload task pinning. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This page will outline three methods for accomplishing this activity (using the QMC, using Operations Monitor, and using a Qlik CLI for Windows script). The QMC approach is generally appropriate for most environments. The Operations Monitor approach can be integrated with other activities (i.e. by saving any selections in a bookmark) to decrease overall time spent performing administrative tasks. The Qlik CLI for Windows approach is more appropriate for environments where automation is required.&lt;/p&gt; . Table of Contents . QMC - Tasks | Hub - Operations Monitor | Get List of New Tasks (Qlik CLI for Windows) Script | . | . . QMC - Tasks . In the QMC, select Tasks: . . In the upper right hand side of the screen, select the Column selector, and then select the Created column. . . Now select the filter icon for the Created column, and then select the filter of Last seven days, or the desired range. . . Lastly, review the resulting table and view any new tasks. . . . Hub - Operations Monitor . Open up the Hub and navigate to Monitoring apps stream. Select the Operations Monitor application. . . From the App overview page, select the Task Details sheet. . . Select Duplicate, as we will add a column that isn’t currently in a table. . . In Edit mode, select the Reload Summary Statistics table and shrink it to allow for more real estate, as another table will be added. . Drag in a new table object, and add the dimension of Task Name. . . Next, the Task Created dimension needs to be added to that table. Note that reloads from the Hub and manual reloads will not show a created date. . . It is now possible to sort by the Task Created column to view new tasks. . . For deeper analysis into tasks, refer to Analyze Tasks . . Get List of New Tasks (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . The script will bring back any reload tasks with a Created Date that is greater than or equal to x days old. The script will then store the output into a desired location in either csv or json format. . Script . # Function to collect tasks that were created within the last x days ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date $daysBack = 7 # directory for the output file $filePath = &#39;C: tmp &#39; # desired filename of the output file $fileName = &#39;new_tasks&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # set the date to the current time minus $daysback $date = (Get-Date -date $(Get-Date).AddDays(-$daysBack) -UFormat &#39;+%Y-%m-%dT%H:%M:%S.000Z&#39;).ToString() # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # check the output format # get all apps that are created &gt;= $date and &gt;= $byteSize # output results to $outfile If ($outputFormat.ToLower() -eq &#39;csv&#39;) { Get-QlikReloadTask -filter &quot;createdDate ge &#39;$date&#39;&quot; -full | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { Get-QlikReloadTask -filter &quot;createdDate ge &#39;$date&#39;&quot; -full | ConvertTo-Json | Set-Content $outFile } . Tags . #weekly . #asset_management . #tasks . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/tasks/new_tasks.html",
		"relUrl": "/docs/asset_management/tasks/new_tasks.html"
	},
	"37": {
		"title": "Review Node Health",
		"content": "Spot-Check: Node Health . Cadence Daily . Sites production .   Initial Recurring . Estimated Time | 1 min | 1 min | . Benefits: . Increase stability | Increase awareness | . . Goal . The goal for this spot-check is to be aware of the health of the node(s) in a Qlik Sense Enterprise deployment. Unexpectedly off-line nodes should be brought online. Nodes where service(s) have unexpectedly restarted should be investigated. . Table of Contents . Services up? QMC - Nodes | Services unexpectedly restarted? QMC - Nodes | Service Failure Notifications | . . Services up? QMC - Nodes . In the QMC, select Nodes: . . Inside of the Nodes section review the available node(s) in the Qlik Sense Enterprise deployment to ensure that the expected number of services are running: . . An environment where a node is entirely down or some subset of services are not available will display in this section of the QMC: . . An administrator should attempt to start / restart the down services. . . Services unexpectedly restarted? QMC - Nodes . In the QMC, select Nodes: . . Inside of the Nodes section select the i icon to bring up an informational modal for uptime of the node’s enabled services: . . This section will detail the uptime of each enabled service. Services with unexpected uptimes (e.g. the engine and proxy services in this example) should be investigated. . Service Failure Notifications . Some customers will go the route of monitoring the actual Windows services behind Qlik to receive email alerts if any unexpectedly fail. This article does not directly advise on that topic, as there are many ways to achieve the end result, such as this example. . Other methods could be employed, such as using the Qlik CLI for Windows or otherwise to periodically poll Qlik to check various endpoints for their health, however this could result in noisy logs and an unnecessary burden on the Qlik system. . Regardless, this is an area that can be explored further if one is looking to quickly react to service failures. . Tags . #daily . #spot_check .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_spot_check/nodes.html",
		"relUrl": "/docs/system_spot_check/nodes.html"
	},
	"38": {
		"title": "NPM",
		"content": "NPM . If Qlik Sense Server is installed, which means that the Node.js standalone installer has not been downloaded, NPM will need to be downloaded and installed. If the Node.js installer has already been run, this process can be skipped. . First, the latest version of NPM must be downloaded from here. The release tagged Latest release with a green tag is the likely the desired version. Scroll down to Downloads and click on Source code (zip). . 1 - Once the source code has been downloaded, unzip the file and navigate into the npm-&lt;version&gt;/bin folder. 2 - Copy the three npm files (npm, npm.cmd, nad npm-cli.js) in to C: Program Files Qlik Sense ServiceDispatcher Node. 3 - Create a folder called node_modules in C: Program Files Qlik Sense ServiceDispatcher Node. Admin rights are required. 4 - Rename the downloaded and extracted npm-&lt;version&gt; folder to npm, then move the entire npm folder into node_modules. The end result should be: C: Program Files Qlik Sense ServiceDispatcher Node node_modules npm . The npm folder should have all the root files from the downloaded source. . Following, add the Node folder to the Windows Path. . Open the Control Panel and select System. | Select the Advanced system settings on the left. | Click the Environment Variables... button. | Under System Variables, select Path, then click Edit. | Add the following folder location C: Program Files Qlik Sense ServiceDispatcher Node . Note that every path in the environment variable should be separated by a single semicolon, so there should be one before and one after the folder location. | Restart all cmd prompts to ensure the environmental variables are the latest. | Test that npm is working by opening a new Windows cmd prompt and typing in npm and hitting enter. .",
		"url": "https://adminplaybook.qlik-poc.com/scripts/npm.html",
		"relUrl": "/scripts/npm.html"
	},
	"39": {
		"title": "404",
		"content": "NPM . If Qlik Sense Server is installed, which means that the Node.js standalone installer has not been downloaded, NPM will need to be downloaded and installed. If the Node.js installer has already been run, this process can be skipped. . First, the latest version of NPM must be downloaded from here. The release tagged Latest release with a green tag is the likely the desired version. Scroll down to Downloads and click on Source code (zip). . Once the source code has been downloaded, unzip the file and navigate into the npm-&lt;version&gt;/bin folder. | Copy the three npm files (npm, npm.cmd, nad npm-cli.js) in to C: Program Files Qlik Sense ServiceDispatcher Node. | Create a folder called node_modules in C: Program Files Qlik Sense ServiceDispatcher Node. Admin rights are required. | Rename the downloaded and extracted npm-&lt;version&gt; folder to npm, then move the entire npm folder into node_modules. The end result should be: C: Program Files Qlik Sense ServiceDispatcher Node node_modules npm . The npm folder should have all the root files from the downloaded source. | Following, add the Node folder to the Windows Path. . Open the Control Panel and select System. | Select the Advanced system settings on the left. | Click the Environment Variables... button. | Under System Variables, select Path, then click Edit. | Add the following folder location C: Program Files Qlik Sense ServiceDispatcher Node . Note that every path in the environment variable should be separated by a single semicolon, so there should be one before and one after the folder location. | Restart all cmd prompts to ensure the environmental variables are the latest. | Test that npm is working by opening a new Windows cmd prompt and typing in npm and hitting enter. .",
		"url": "https://adminplaybook.qlik-poc.com/docs/helper/npm.html",
		"relUrl": "/docs/helper/npm.html"
	},
	"40": {
		"title": "Operations Monitor",
		"content": "Operations Monitor . developmentproduction . Table of Contents . About | Operations Monitor sheets | Screenshots | Documentation | . . About . The Operations Monitor it is already embedded in Qlik Sense and can be found at Monitoring apps stream. . The Operations Monitor loads service logs to populate charts covering performance history of hardware utilization, active users, app sessions, results of reload tasks, and errors and warnings. It also tracks changes made in the QMC that affect the Operations Monitor. . For a more detailed description of the sheets and visualizations, visit the story About the Operations Monitor that is available from the app overview page, under Stories. . For the location and naming convention of the log files, see Logging. . With the Operations Monitor, you can track system performance and investigate activity that might adversely affect it. For example, by analyzing reload tasks and sessions, you can find bottlenecks that might be alleviated by rescheduling reloads or redistributing sessions. Or you can use the QMC Change Log sheet to review changes that might explain changes in system performance. . . Operations Monitor sheets . The Operations Monitor sheets display Qlik Sense performance on the current node, and, when properly configured for multi-node (as described in Configuring multi-node environments), the app includes information across all nodes. . Sheet Content . 24-Hour Summary | Displays hardware utilization, active users, active apps, and reload tasks over the last twenty-four hours. | . Performance | Allows the user to select a time period over which to display hardware utilization, concurrent users, and concurrent apps. | . Task Overview | Provides a statistical overview of the success, duration, and failure of reload tasks. | . Task Planning | Provides details about reload count, reload CPU spent, and task dependencies. | . Task Details | Provides details about the success and failure of individual app reloads, including execution details about duration and start and end times. | . Session Overview | Provides summary information about apps, app sessions, and app users over selected periods to show which users use which apps when. | . Session Details | Provides details about individual user and app sessions, including number, average duration, days since last session, start and end times, reasons for ending sessions, and the type of client on which the app was run. | . Export Overview | Provides summary information about apps, app objects, and app users to show which users export which app objects when. | . Sheet Usage | Provides summary and detailed information about users accessing sheets, and which sheets in which apps are not accessed. | . Apps | Provides details about the apps in the Qlik Sense Repository Service (QRS), including name and ID of app objects, owners, publishing, and streams. | . QMC Change Log | Displays changes made in the QMC that affect a range of factors from system performance to user access, including changes by QMC resource type, by specific QMC resources, by users who made changes, or by a type of action performed in the QMC. | . Log Details | Provides details about reloads of the Operations Monitor, including the time of reloads, results, error messages and warnings, and log entries. | . Data in the Operations Monitor is updated when the app is reloaded. Data is not live. . . Screenshots . . . . . . . . . . . . . . Documentation . Operations Monitor - Qlik Help | Youtube Video - STT - Configuring Monitoring Apps in Qlik Sense | Youtube Video - Configuring Qlik Sense Enterprise Monitoring Apps for Multi Node | Youtube Video - Qlik Fix: Troubleshooting Qlik Sense Monitoring Apps | . Tags . #tooling .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/operations_monitor.html",
		"relUrl": "/docs/tooling/operations_monitor.html"
	},
	"41": {
		"title": "Optimize Batch Window",
		"content": "Optimize Batch Window . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 40 min | 20 min | . Benefits: . Ensure readiness of applications post-batch window | . . Goal . The goal of this page is to ensure visibility of reload activity within the batch window so that if there is any room for optimization, it can be taken advantage of. The ultimate goal is to ensure that all reloads are able to fit within the batch window with overlapping with consumption hours. . . Reloads Monitor . This page leverages the Reloads Monitor. Please refer to the Reloads Monitor page for an overview and relevant documentation links. . Confirm Reloads Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Reloads Monitor application. Confirm that the application’s data is up-to-date. . . If the Reloads Monitor is not up-to-date, please refer to the Reloads Monitor Documentation for configuration details and troubleshooting steps. . . Process . Navigate to the Monitoring Apps stream and open up the Reloads Monitor. . . Navigate to the Task Planning sheet. . . Make selections to narrow the scope of the batch window to the appropriate days of the week and hours of the day. In the example below, Monday - Friday is selected between the hours of 3-6 am. Ensure that the appropriate selections are made to encompass the organization’s batch window. . . The 6 am hour is highlighted across all 5 days, and demonstrates a very high level of activity compared to the other hours in this batch window example. This could indicate the opportunity to move some of those reload tasks into the hours prior to 6am, to spread out the processing more evenly. . Similarly, the second highlighted box shows that Wednesday mornings have more activity in the 3am-6am timeslot than the other days. If there are weekly tasks that can be reallocated to Tuesdays or Thursdays (from that Wednesday slot) this might help even out the processing burden across days. . Task Optimization &amp; Distribution . Task Concurrency . When configuring tasks, ensure that the maximum number of concurrent reloads for each scheduler is set appropriately. For those tasks that do not need to be reloaded serially, this can dramatically cut down on the total batch processing time. . Navigate to the QMC, and select Schedulers. . . For each active Scheduler, double-click or select Edit. . . Select Advanced from the Properties pane, and notice the Max concurrent reloads option. This option can be set to a maximum of n-2, where ‘n’ is the number of cores on the machine (one dedicated to the OS, and the other dedicated to the Repository). In this example, there are 6 cores available on this scheduler, so that allows for a maximum of 4 concurrent reloads. . . Task Chaining . Ensure that any tasks that are dependent on each other are run chained, and not just run separately with ample room between. For example if “Task 1” takes 5 minutes to run, and “Task 2” needs to run after “Task 1”, do not just simply have “Task 1” run at 2am and “Task 2” run at 3am. “Task” 2 should be chained to execute on the successful completion of “Task 1” to decrease the total batch time. . QVD Reloads . When utilizing QVD files in a Qlik deployment, it is common to focus on the optimization of these reloads first, as they are critical to the dashboard reloads that depend on them. The goal should be to reload QVDs as early in the day as possible, to allow more room in the batch window for the dependent reloads that come after them. Sometimes this is as simple as starting the QVD reload tasks earlier on their schedule. However, sometimes these QVD reloads cannot start until a database has been fully updated for the evening’s processing. In that case, the goal should be to limit the lag time between the source database being updated and the QVD reloads that depend on that database from firing off. There are several methods of “triggering” a QVD reload to happen soon after a database has completed nightly processing. Search “trigger reload” on Qlik Community for examples, but high-level, they can be triggered via API calls, they can poll the database for a flag, etc. . Actions . If after reviewing and optimizing as per the above there are still issues with fitting everything into the batch windows, some solutions could consist of the following (one or many): . Add an additional scheduler(s). This would allof for the ability of more concurrent task processing, if the bottle-neck isn’t serial. | Add additional cores to the schedulers to increase the amount of concurrent reloads. | Optimize application reloads. Best practices can be found here for a start. If the bottleneck is serial, the amount of time an application takes to reload affects all others following. | . Tags . #quarterly . #system_planning . #batch . #reloads_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/optimize_batch_window.html",
		"relUrl": "/docs/system_planning/optimize_batch_window.html"
	},
	"42": {
		"title": "Optimize Sheet Order for Adoption",
		"content": "Optimize Sheet Order for Adoption . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 1 hr | 15 min | . Benefits: . Increase adoption | Improve focus | . . Goal . This supports both the administrator and the application owner in optimizing application the order of sheets, supported by usage data. . When an application is developed, the sheets are ordered in a logical flow . For example the Dashboard, Analysis, Reporting (DAR) methodology , which recommends flowing data from high-level down to the detail. When considering re-ordering sheets, ensure that this methodology isn’t lost if already in place. . It is integral to combine the usage data with the logical flow to make sure that the new sheet order will increase adoption rates and BI effectiveness. . This activity should not be attempted for every application, and should be prioritized to highly-used applications. Refer to Analyze App Adoption for guidance on qualifying which applications are most used. . Table of Contents . Suggested Prerequisites | Audit Activity Log | Operations Monitor Confirm Operations Monitor is Operational | . | Select Applications with a High and Low Number of Sessions | Analyze Application Sheet Adopion | Reorder Application Sheets | . . Suggested Prerequisites . Notification of Unused Base/Community Sheets | . . Audit Activity Log . As of the February 2019 release, an improvement was added to the product to log sheet usage at default log levels. This enables the ability to measure sheet adoption as well as manage the amount of sheets in the applications–keeping them trimmed to only what is being leveraged. . Ensure that the Audit Activity log level is set to Basic for every engine. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This is the default setting, but it is encouraged for the Qlik administrator to confirm what is configured for their environment(s).&lt;/p&gt; . . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Select Applications with a High and Low Number of Sessions . Before analyzing the sheet usage data, it is a good practice to select specific applications to analyze. It is recommended that the Qlik administrator starts with the top five applications with the highest number of sessions. . Open up the Operations Monitor application, inside of Monitoring Apps stream. . . Open the Session Details sheet. . . In the App Session Summary table, sort the Sessions column descending. . . Select the top five applications. . . . Analyze Application Sheet Adopion . After selecting the top five applications in the previous step, the next step is to take a look at the sheet usage data for each application. . While keeping the five applications selected, navigate to the Sheet Usage sheet. . . One can now visualize the number of sheets (Base, Community, and Private) that are in each selected application. . To simply the process, it is suggested to select only one application at a time. Feel free to bookmark the five applications if it is more convenient. In this example, Sample App has been selected. . . To reorder the table by the sheet usage, sort the Users Accessing Sheets column descending. . At this point, it is also an option to export the Users Accessing Sheets table and share it with the application owner to optimize the order, as the owner of the application likely knows the content the best. . Protip: It is possible to use Qlik NPrinting to distribute the Sheet Usage table to distribute sheet usage data to application owners. . . Protip: If there are community sheets with a higher quantity of sessions than base sheets, it is worth investigating the content of those sheets with the application owners and considering either promoting the sheet to base, or incorporating the content into the base app otherwise. . This process should be repeated for each of the selected applications. . . . Reorder Application Sheets . Before reordering the sheets, it is important to analyze the sheet’s logical flow from one to the next combined with the sheet’s usage data. . In this example, the Dashboard sheet should remain first, even though it is not the most accessed, so that it doesn’t break the application’s logical flow. The Sales Analysis and Inventory will be moved. . . To change the sheets order, the administrator or application owner should clone the application. . . As soon as the application gets duplicated, the new duplicated application will reside in the owner’s My work area. . . Open the application, and ensure that the Touch screen mode feature is turned off. . . Sheets can be moved by dragging and dropping them to a new position. In this case, the Sales Analysis by Region will be moved to the second position. . . The Inventory sheet will be shifted to the third position. . . The sheet order has now been optimized. . . Following, the application should now Publish and replace the original. . . Make sure to check the Replace the existing app box. . . Tags . #quarterly . #asset_management . #apps . #sheets . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/optimize_sheet_order_for_adoption.html",
		"relUrl": "/docs/asset_management/apps/optimize_sheet_order_for_adoption.html"
	},
	"43": {
		"title": "Plan Disaster Recovery",
		"content": "Plan Disaster Recovery . Cadence Yearly . Sites production .   Initial Recurring . Estimated Time | 40 min | 20 min | . Benefits: . Ensure readiness | . . Goal . The goal of this activity is to plan and review the organization’s disaster recovery plan. This ensures that the organization can recover from disaster situations and allows the Qlik site to align with the organizational goals of resiliency. . Table of Contents . Define the Recovery Time Objective (RTO) | Techniques to optimize for recovery Use of DNS alias for host names | Hardened Processes | . | Hot vs. Warm vs. Cold | Disaster Recovery Plan | . . Define the Recovery Time Objective (RTO) . In order to plan the processes which will be used to recover Qlik Sense Enterprise after a disaster event, the organization needs to have a target amount of time that downtime is allowed. For many organizations it would be considered acceptable for their business intelligence software to be off-line for a day or two if a disaster event occurs, e.g. their data center is destroyed by a hurricane. For others, the downtime allowance may be much, much smaller. With an understanding of this time objective, the Qlik administrator can then work to architect their deployment to align with this organizational objective. . Techniques to optimize for recovery . The following techniques can be used to expedite recovery processes for a Qlik site. . Use of DNS alias for host names . By using a DNS aliases, the organization can abstract from individual server names and IP addresses. This allows for a more dynamic recovery process. Consider two architectures . . The architecture on the left uses static server names for routing, whereas the architecture on the right uses DNS alias for both front and back-end servers. Deployments which use DNS aliases will have quicker recovery processes, all else being equal. . Hardened Processes . Organizations who have practiced recovery events will be able to recover quicker. Reference Practice Recovery Processes in this playbook for more guidance. During the review of the organization’s DR plan, feedback from the previous DR exercise should be integrated. . Hot vs. Warm vs. Cold . Within the context of Qlik Sense Enterprise: . Cold DR: An approach which allows for a new server to be provisioned for Qlik Sense Enterprise. This can include using VM or Cloud based snapshots, or manual back-up and recovery mechanisms. See Verify Backup Execution for more guidance on the manual approach. | Warm DR: A blend of Cold and Hot, generally involving servers running with software installed. For example, small VMs running in a secondary data center where Qlik Sense Enterprise installed which requires some manual or programmatic intervention to restore the content from the live production site. | Hot DR: A duplicative set of servers with Qlik Sense Enterprise installed and duplicated data. This allows for the quickest recovery but at the cost of increased hardware requirements. Hot DR is difficult with Qlik Sense Enterprise due to the relationship between the physical files on disk used by the site and the metadata stored in the Qlik Repository Database (PostgreSQL). This style of DR is generally confined to environments which are functionally read-only, although allowances for contributed content can be made. This generally is guided by Qlik’s Professional Services. | . Disaster Recovery Plan . Once the Qlik administrator has considered the objectives of the organization, they can begin to build out a plan of action for how their Qlik site will be recovered. A template for how this can be documented is as follows: . Recovery Objective: 1 business day | Production Server(s): qs-prod-1.company.com: 8 cores, 64GB RAM | qs-prod-2.company.com: 8 cores, 64GB RAM | . | Software: Qlik Sense Enterprise November 2019 | . | Backups Located at somefileshare applications qlik | . | Recovery / Installation Steps | Step 1 | Step 2 | … | . Tags . #yearly . #system_planning . #dr . #DR .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/plan_disaster_recovery.html",
		"relUrl": "/docs/system_planning/plan_disaster_recovery.html"
	},
	"44": {
		"title": "Plan/Review Upgrade Strategy",
		"content": "Plan/Review Upgrade Strategy . Cadence Yearly . Sites all .   Initial Recurring . Estimated Time | 3 hours | 20 min | . Benefits: . Enable new feature / functions | Receive bugfixes | . . Goal . The goal of this activity is to review the strategy for upgrading Qlik Sense Enterprise on Windows for the administrator’s organization. This includes reviewing / creating an upgrade strategy, reviewing / creating an upgrade plan, and monitoring the adherence to the plan. Ensuring that the Qlik deployment is up-to-date enables the organization to leverage new features which have been added to Qlik Sense Enterprise on Windows, decreases the risk from an upgrade (an upgrade of N+1 release is inherently less risky than an upgrade of N+5 releases), and ensures readiness should a critical issue be uncovered which requires an out-of-cycle upgrade. . When creating / reviewing an upgrade plan, the Qlik administrator may not be responsible for all portions (e.g. defining the validation procedure on whether apps work as designed), but the expected responsibility here is to build a framework where those responsible can contribute and sign-off where needed. . Great is the enemy of good for this activity. The Qlik administrator does not need to solve for every conceivable problem when constructing their upgrade plan, but instead integrate lessons learned from its execution into the plan going forward. . This activity is intended to scope to the review and/or creation of an upgrade strategy and prodecure, not to the execution of upgrades. For most deployments, upgrades should be performed more than once per year. . Table of Contents . Upgrade Strategy | Upgrade Plan Review Documentation | Define Milestones | Outline Procedure | . | . . Upgrade Strategy . An upgrade strategy for an organization can be as simple as a single sentence goal statement like: . Company XYZ’s goal is to upgrade our Qlik deployment twice a year. The upgrades should target the April and November releases of Qlik Sense Enterprise. . This strategy will be a negotiation between the stakeholders of the organization and the operational staff who are responsible for the implementation of the upgrade (i.e. administrators, app developers, etc). As staffing, resources, and priorities change for an organization the organization’s upgrade strategy should be altered to reflect the changed landscape. . From a Qlik perspective, we understand that every organization cannot upgrade to the latest release immediately. But we do want organizations to have a strategy and process around upgrades of their Qlik deployment. The goal statement above is extremely common for Enterprise customers. It provides a mimimal lag between the release of new features / functions and availability to the organization’s users as well as presents a regular enough cadence between upgrades to ensure that the operational staff have confidence in their ability to execute on the upgrade plan. . It is common for organizations to target an N-1 strategy when it comes to Qlik Sense versions. For example, if the latest release is February 2020, the organization will target November 2019. This ensures that there is road testing &amp; patches for the version that they choose to implement. . Upgrade Plan . After the Qlik administrator has created / verified the existence of a defined goal for the organization’s Qlik deployment, they will want to review / create an actual upgrade plan for the organization. This plan has multiple phases, from planning to execution to review of that execution. We will outline an example plan below. . Review Documentation . Prior to upgrading a Qlik site, the Qlik administrator should review available documentation on the release that they intend to upgrade to. This documentation can include: . Release Notes: Access the Qlik Support Portal to review the release notes of the release that is targeted as well as all incremental releases between the current and target version. These documents should be reviewed for (a) new features, (b) known issues, (c) upgrade notes, (d) deprecation notices, and (e) system requirement notes. A document (e.g. a Wiki page, Microsoft Word doc, etc) should be started with notes which need to be acted upon on the upgrade. For example, it is expected that Qlik Sense Enterprise will remove support for Microsoft Windows Server 2012 R2 in the future, as the end-of-life of that Operating System nears. At that point in time in the future, the organization will want to review the Operating Systems which run their Qlik deployment to ensure compatibility. | help.qlik.com: Access help.qlik.com to review the “What’s New” section for the targeted release as well as all incremental releases to document the expected new features which may be relevant to the organization’s users. For example, the Qlik Sense November 2019 release includes improved accumulation options in bar charts, line charts, combo charts, and tables. Document the new features for review by the organization’s developers for inclusion in their Qlik apps. | Qlik Support’s Knowledge Base: Access the Qlik Support Portal to search for known issues in the target release of Qlik Sense Enterprise. While it is uncommon for a major issue to be found in a release of Qlik Sense Enterprise, it can occur. The administrator should do a survey of articles provided by Qlik Support to stay abreast of any issues which might be encountered upon their organization’s upgrade. | . . Qlik Community: Access Qlik Community with special attention to Qlik Support’s Support Updates Blog to review notices which may impact the organization’s upgrade process. | . Define Milestones . After reviewing and assembling available documentation on the planned upgrade, the Qlik administrator should ensure that the upgrade plan has defined parameters. These parameters should include: . Timeline: Define the time frame during which an upgrade is possible or when the finalized upgrade in the organization’s production needs to be completed by. This time frame can be over a weekend, at fiscal year end, or some convenient time for the organization. | Upgrade Sequencing: For organizations which utlize multiple tiers of Qlik Sense Enterprise (e.g. Development, Test, Production), define the expected sequencing of upgrades as well as the expected timeline for the upgrade of each tier. For example, an organization may chose to upgrade their development tier of Qlik Sense Enterprise, execute on the upgrade plan, and target an upgrade of the next tier (i.e. Test or Production) two weeks after the upgrade of development. This defined sequencing enables the organization to place the appropriate urgency on each individual’s activities in the execution of their upgrade plan. | Resources Needed: Outline the various resources who may be needed during the upgrade process. The goal here is for the Qlik administrator to know who needs to be contacted should issues arise as well as to coordinate the sub-tasks of the upgrade procedure. By being as specific as possible for this task, the Qlik administrator will decrease the complexity of coordinating across multiple teams and, hopefully, decrease the time it takes for those teams to execute on their needed activites. These resources can include: IT Staff for providing service account passwords, reviewing configuration of load balancers / network appliances | Qlik app developers who are responsible for published apps | Developers who are responsible for maintaining integrations (e.g. embedding Qlik into some other portal) | . | Training: If additional training is needed for end users or developers to successfully use new features / functions, document the type of training which will be needed. The BI Admins / Developers of the site may be responsible for this task. | Review previous lessons learned: Review what was learned from previous upgrades. Examples of these can be: Ensure reviews of extensions used in Qlik apps since a break in an important extension was missed previously (reference the task on Analyzing / Curating Extensions in this playbook for this item). | The Network appliance team requires at least 1 day to be available | Restoring a VM snapshot can take 1 week depending on that team’s backlog | . | . Outline Procedure . Once the upgrade plan has some parameters which constrain its successful execution, the Qlik administrator should create / coordinate / review the organization’s actual procedure for upgrading Qlik Sense Enterprise. Many of the items in this procedure can use sections of this playbook both operationalize the process as well as decrease the amount of stuff that needs to be verified / reviewed. An example of this plan can be: . Audit Assets: Survey what assets exist and thus need to be verified to be functional. | Establish baselines: When aiming for a highly tuned Qlik deployment, doing baseline performance testing will ensure that the organization has precise numbers on their apps’ performance in order to make comparisons across versions of Qlik Sense Enterprise. | Establish dates: Define the exact dates which the deployment will undergo upgrades as well as the dates by which various signoffs need to occur (e.g. Developer X responsible for Apps A, B, and C needs to confirm functional success of the app). | Signoff from stakeholders on upgrade process: Ensure that the all the stakeholders for the deployment agree to the execution of the upgrade. This can range from the executive sponsers / champions (e.g. Vice President of Business Analytics) to the BI teams responsible for the Qlik apps to the IT resources who will need to be made available during the upgrade window. | Shutdown Activities / Notices: Communicate to all relevant parties that the site will be undergoing an upgrade. An email communication to the relevant parties is recommended. | Backups: Take backups of the Qlik site which is under-going an upgrade. Reference our guide on Backups if needed. | Upgrade: Execute the installer to upgrade the Qlik node(s) | Functional Testing: Have developers review their assets to verify that they work in an expected manner. Applications: Does everything work as expected? | Extensions: Do they operate as expected on the new version? | Authentication Mechanisms: Can users authenticate into the site? | API integrations: Do any integrations using Qlik’s APIs operate as expected? i.e. do our automation processes work? Do the embedded apps in Salesforce work as expected? | Reload Tasks: Do all apps reload as needed? | . | (Optional) Enterprise Testing Security Reviews: Execute a security / vulnerability scan of the software | Load Testing: Exeucte a load test of the site to ensure performance | . | Go Live: Communicate with relevant stakeholders and business users that the site is upgraded. Monitor the system for possible issues. | Post Upgrade Issues: Document any issues and the steps for resolution. This is key when upgrading multiple tiers of Qlik Sense. | Training: Enable developer and end users on new features / functions. | Review Process: Document what worked, what didn’t work. As gaps are found in the upgrade process, add procedures into the Upgrade plan close those gaps. | Tags . #yearly . #system_planning . #upgrade .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/plan_review_upgrade_strategy.html",
		"relUrl": "/docs/system_planning/plan_review_upgrade_strategy.html"
	},
	"45": {
		"title": "Practice Recovery Processes",
		"content": "Practice Recovery Processes . Cadence Yearly . Sites production .   Initial Recurring . Estimated Time | 2 hrs | 40 min | . Benefits: . Ensure readiness | . . Goal . The goal of this activity is for the Qlik administrator to practice the previously developed Disaster Recovery Plan. This allows the administrator to test and provide feedback to the plan. Successful execution of the plan allows the organization to have confidence that their Qlik site is resilient based on their Recovery Time Objective. . The steps outlined in this activity are intended to be examples of how to recover a Qlik site. The specific steps required will vary by site so customization of these steps on a per-site basis by the Qlik administrator will be required. . Table of Contents . Pre-reqs | Steps (Simple) | Steps (Advanced) | Feedback Loop | . . Pre-reqs . To be successful, the organization needs to have some mechanism of recovery. This can be as simple as snapshots of VMs or Cloud instances or as complex as a hot mirrored environment. This activty assumes that there is some recovery mechanism in place already. . Steps (Simple) . In this example, we will do a simple recovery of a Qlik site. This would typically leverage snapshots of the VM or Cloud instance. . Ensure that a snapshot exists | Shut down the VM / Cloud instance | Restore Snapshot In a real DR scenario, the Qlik administrator would likely restore this VM to a new VM cluster (on prem) or instance region (cloud) | . | Start the VM / OS | Access Qlik Sense Enterprise QMC / Hub | Verify: Key apps can be opened (reference Analyze App Adoption for guidance) | Key reload tasks execute successfully (reference Analyze Tasks for guidance) | Customized authentication mechanisms work | Customized integrations work (e.g. portals, automation tasks) | . | . Insofar as the VM retains the same server name, the backup should initialize successfully. If the hostname is changed then refer to this Qlik Support article for guidance. . Steps (Advanced) . In this example, we will do an advanced recovery from scratch. . Go through manual recovery process (reference this Qlik Support article and this help.qlik.com article) | Start the VM / OS | Access Qlik Sense Enterprise QMC / Hub | Verify: Key apps can be opened (reference Analyze App Adoption for guidance) | Key reload tasks execute successfully (reference Analyze Tasks for guidance) | Customized Authentication mechanisms work | Customized integrations work (e.g. portals, automation tasks) | . | . Feedback Loop . After completing the recovery process, it is expected that the actual list of steps which are required will (a) be longer and (b) be customized on a per-environment basis. The Qlik administrator should document any additional steps which are required and integrate this into internal documentation for their Qlik deployment. As sites become more complex (e.g. multi-nodes, integrations), the dependencies for a site will grow so repeating this recovery process at least yearly is ideal in order to ensure accuracy of the internally documented processes. . Tags . #yearly . #system_planning . #recovery .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/practice_recovery_processes.html",
		"relUrl": "/docs/system_planning/practice_recovery_processes.html"
	},
	"46": {
		"title": "Qlik CLI for Windows",
		"content": "Qlik CLI for Windows . developmentproduction . Estimated Configuration Time | 15 min | . Table of Contents . About | Where to get it | Documentation | Example Script | . . About . Qlik CLI for Windows is a PowerShell module that provides a command line interface for managing a Qlik Sense environment. The module provides a set of commands for viewing and editing configuration settings, as well as managing tasks and other features available through the APIs. . This is a widely used module in the Qlik ecosystem, by services internally to Qlik, partners, and customers worldwide. . . Where to get it . The Qlik CLI for Windows can be found here: Qlik CLI for Windows. . . Documentation . For documentation, please visit the Wiki for the repository here: Qlik CLI for Windows Wiki. . . Example Script . The below is a simple example of the Qlik CLI for Windows importing an app into a Qlik environment and publishing it to a stream. . Import-QlikApp -file . filename.qvf -name ExampleApp -upload | Publish-QlikApp -stream ExampleStream . Tags . #tooling . #cli .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/qlik_cli.html",
		"relUrl": "/docs/tooling/qlik_cli.html"
	},
	"47": {
		"title": "QVD Monitor",
		"content": "QVD Monitor . developmentproduction . Estimated Configuration Time | 15 min | . Table of Contents . About | Questions Answered by this Application | Download &amp; Documentation | . . About . The Qlik Sense QVD Monitor is a Qlik application that reads metadata from QVD files to provide analysis on their sizes, columns and growth over time. The sources for this application are the data connection folders that are supplied in the load script. A similar application has been available for QlikView for many years, but it has now been ported to Qlik Sense in this form. . The application is supported by Qlik’s Americas Enterprise Architecture team, and uses only native connections and capabilities of the Qlik platform–making it plug-and-play. . . Questions Answered by this Application . Questions answered by the app . Which QVDs are in which folder connections in my environment? . | Which columns are in the QVDs? . | How many rows are in each QVD and how large are the files? . | How much are the QVDs growing (or shrinking) over time? . | What is the overlap between QVDs, in terms of column names ?With the Qlik Sense QVD Monitor, BI Admins can spot anomalies, outliers and growth trends in their QVDs with ease. They can also identify overlaps and potentially clean up duplicative QVDs and data. . | Download &amp; Documentation . Complete documentation can be found on the Qlik Sense QVD Monitor GitHub page. . Tags . #tooling . #qvds .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/qvd_monitor.html",
		"relUrl": "/docs/tooling/qvd_monitor.html"
	},
	"48": {
		"title": "QVDs",
		"content": "QVDs .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/qvds.html",
		"relUrl": "/docs/asset_management/qvds.html"
	},
	"49": {
		"title": "Reloads Monitor",
		"content": "Reloads Monitor . developmentproduction . Estimated Configuration Time | 15 min | . Table of Contents . About | Where to get it | Importing the Reloads Monitor app to the Monitoring apps | Screenshots | Documentation | . . About . The Reloads Monitor loads and presents log data about reloads. Reload data is collected both from QMC tasks and apps open in the hub. You can see which apps are updated, and details about when, where, and how often they are updated. . . Where to get it . The application can be found under %ProgramData% Qlik Sense Repository DefaultApps and is titled Reloads_Monitor.qvf. This however might not be the most recent version of the app, as there have been bug fixes since. The most recent version of the app can be found at Qlik Community - Sense: Sessions Monitor, Reloads Monitor, Log Monitor, and can be used from April 2018 and beyond. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This might be able to work with Feb 2018, but proceed at own risk.&lt;/p&gt; . . Importing the Reloads Monitor app to the Monitoring apps . If you haven’t already imported the extra monitoring apps that come with Qlik Sense, but are not by default exposed in the QMC/Hub, it’s time to do so. Follow the directions in this link (Importing New Monitoring Apps). . Once you open the Reloads Monitor application in your Work area of the Hub, follow the instructions in the Load Script Editor (comments on first tab of script) for the right runtime setting. Then reload the application from the script editor to ensure it runs without error. If you experience errors reloading this application, follow this link (Support - Operations Monitor or License Monitor Tasks Do Not Reload - Monitoring Apps) for helpful tips on resolutions to monitoring app reload issues. . . Screenshots . . . . . . . Documentation . Reloads Monitor - Qlik Help | Youtube Video - STT - Configuring Monitoring Apps in Qlik Sense | Youtube Video - Configuring Qlik Sense Enterprise Monitoring Apps for Multi Node | Youtube Video - Qlik Fix: Troubleshooting Qlik Sense Monitoring Apps | . Tags . #tooling . #tasks . #reloads .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/reloads_monitor.html",
		"relUrl": "/docs/tooling/reloads_monitor.html"
	},
	"50": {
		"title": "Remove/Disable Unused Tasks",
		"content": "Remove/Disable Unused Tasks . Cadence Monthly . Sites production .   Initial Recurring . Estimated Time | 20 min | 5 min | . Benefits: . Increase performance | Increase focus | Decrease redundancy | . . Goal . The goal is to remove all unnecessary tasks from the site. This will make the site more manageable, readable, and more easily navigable. . Table of Contents . Suggested Actions Priority | . | . . Suggested Actions . Ensure daily task spot-checks are followed to rectify erroring tasks. The below section assumes those have been handled and accounted for. Refer to: System Spot Check: Tasks . Priority . “Manually triggered reload of…“ tasks. These tasks should always be routinely cleaned out, as they are created to run once and then typically left to live out their long and lonely lives to never be run again. These are “one-off” tasks that are created and triggered by the Reload now option under the Apps section in the QMC when selecting a single application. . . | Action Immediate removal. | . | Suggested Policy Do nothing. This is an easy way to clear out reloads that were used for testing, that might otherwise be more difficult to discern. | . | . | Disabled tasks. . . | Action Contact the Owner of the application for which the task is associated with, as well as potentially the Modified by user of the task, who is more than likely the user who disabled the task. This information can all be found in the QMC. | . | Suggested Policy Any disabled task with a Last modified date older than 90 days can be deleted without prior validation. | . | . | Active tasks with no Next execution . . | Action Contact the Owner of the application for which the task is associated with, as well as potentially the Modified by user of the task, who is more than likely the user who disabled the task. This information can all be found in the QMC. | . | Suggested Policy Any task with no Next execution with a Last modified date older than 90 days can be deleted without prior validation. | . | . | Tags . #monthly . #asset_management . #tasks .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/tasks/remove_disable_unused_tasks.html",
		"relUrl": "/docs/asset_management/tasks/remove_disable_unused_tasks.html"
	},
	"51": {
		"title": "Remove/Quarantine Unused Apps",
		"content": "Remove/Quarantine Unused Apps . Cadence Quarterly . Sites developmentproduction .   Initial Recurring . Estimated Time | 30 Min | 5 min | . Benefits: . Reduce Maintenance | Increase Performance | . . Goal . The goal of this procedure is to remove unnecessary (unused) applications from a Qlik site. This increases overall site performance, decreases clutter, and will focus users to what is pertinent. . Table of Contents . Operations Monitor Confirm Operations Monitor is Operational | . | Process Priority 1 | Priority 2 | Priority 3 | . | Actions | . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Process . Open the Operations Monitor App, and select the Apps sheet. . . In the App Details table object, sort by Last Accessed field and scroll to old dates or null dates. . . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;While it is not possible to select apps which have not been accessed since this value is null, it is possible to select all apps which have been accessed by entering &lt;code&gt;*&lt;/code&gt; in the &lt;b&gt;Last Accessed&lt;/b&gt; column and then selecting excluded in the &lt;b&gt;ID&lt;/b&gt; column.&lt;br /&gt;&lt;br /&gt; &lt;img src=&quot;images/quarantine_unused_apps-null.png&quot; /&gt; . &lt;/p&gt; . Priority 1 . Look for applications that are Published but have not been accessed. This can be quickly filtered to by selecting Unpublished from the Stream column, and then by selecting Select alternative to view all published applications. . . . . Priority 2 . Look for applications that are Published and have not been used for a long time. . . Priority 3 . Look for Unpublished applications that have not been used for a long time. Clear selections in the Stream field, and then select Unpublished from the same field. . . . . Actions . If the stream Quarantine doesn’t already exist, this is a good time to create it. Ideally, the stream should be walled off to only Content Admins or the like, where they can review the applications with their respective owners. . | Contact the application owners to let them know that their applications are being relocated to the Quarantine stream. . | Move the applications from Priorities 1, 2, and 3 to the Quarantine stream. . | Any applications that have been in the Quarantine stream for X number of days can be removed (corporate policy on how long they should be kept.) It is considered a best practice to export them without data, at a minimum–potentially exported with data if the intent is to archive them. . | Tags . #quarterly . #asset_management . #apps . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/remove_quarantine_unused_apps.html",
		"relUrl": "/docs/asset_management/apps/remove_quarantine_unused_apps.html"
	},
	"52": {
		"title": "Remove Unused Data Connections",
		"content": "Remove Unused Data Connections * . Cadence Monthly . Sites developmentproduction .   Initial Recurring . Estimated Time | 1/2 day | 30 min | . Benefits: . Decrease data connection security rule evaluation time (increase performance) | Decrease maintenance | Increase focus | Eliminate redundancy | . . Goal . Removing unused connections on a regular basis decreases clutter, improves performance, and creates a better user experience. The goal of this section is to leave only the data connections that are necessary for analysis. . Table of Contents . Data Connection Usage | Unused Data Connections | Suggested Actions Suggested Quarantine Method | Priority | Data Connection Backup | Scripts to Manage Unused Data Connections Script to Backup All Data Connections | Script to Add Custom Property value to Data Connections from an Excel Export | . | . | . . Data Connection Usage . Data connections are a bit of a difficult entity to map to associated resources, as they are not directly mapped in the QRS to the apps that are using them. Data connections are evaluated in the script, and leveraged at the time of reload. . There are two primary options for mapping data connections: . The lineage endpoint of an application Accessible via the Qlik Engine REST API lineage endpoint, available as of the June 2019 release | Accessible via the Qlik Engine API | . | Parsing of evaluated script logs, as demonstrated by the Data Connection Analyzer | . Pros and cons of the lineage option: . Pros Cons . Simple to iterate over (the RESTful option) from a Qlik load script | Returns fully evaluated folder paths, so that they cannot be properly mapped back to their respective lib:// connections–ultimately not allowing the mapping of any Folder connections | . Quick and efficient | Can be difficult to parse the result | . Includes INLINE and AUTOGENERATE loads | Does not include any history – only most recent reload | .   | Does not offer insight what user used them when | .   | Does not offer insight into how many times they were used | . Pros and cons of the Data Connection Analyzer option: . Pros Cons . Robust logic to handle all data connection types, including Folder connections | Can take many hours to run the first time, as it can parse thousands of logs | . Has the ability to capture logs from all time | Can potentially produce false positives for unused data connections (see app docs for details) | . Can be used for comprehensive audit |   | . Tracks what users used what connections when |   | . Tracks how many time data connections have been run |   | . Tracks the source of the reload: Hub, Scheduler, or ODAG (api) |   | . . Unused Data Connections . To capture unused data connections, either of the two options from the Data Connection Usage section above may be employed. If using the lineage option, the resulting connections will need to be mapped back to the existing connections in the QRS. Please note the issue with that endpoint and Folder type connections, documented above. . In this section, the Data Connection Analyzer will be leveraged. . There are two sheets that should be focused on within that application: . Used Connections That Have Not Been Used Within 90 Days This sheet illustrates connections that were at one point active, but have not been leveraged within the last 90 days. Meaning, there are currently no applications that leverage them, though these applications may still exist if they are reloaded at a cadence &gt;90 days. This variable is configurable in the load script, and should be set according to corporate policy. SET vNumDaysForUsedDataConnectionToBeConsideredUnused = 90; | . . | Unused Connection Analysis This sheet shows connections that have been used, by the only apps that ever used them have been deleted, as well as connections that have never been used. | . . | . . Suggested Actions . It is suggested that data connections be deleted manually, and that all data connections are validated for usage by the owner before deletion. The Data Connection Analyzer is largely accurate, but should not be treated as 100% so–validation must be done with the users. It is also suggested to first “quarantine” the data connections before deleting them. . Suggested Quarantine Method . An example of “Quarantining” a data connection can be done by following these steps: . Rename the data connection by prepending QUARANTINED - to its name. For example, My Data Connection (directory_owner) becomes QUARANTINED - My Data Connection (directory_owner) . | Change the owner of the data connection to sa_repository . | Create a custom property named QuarantinedDataConnection where the value of true is applied to any quarantined connection. . | Modify any existing customized security rules on data connections, leveraging the QuarantinedDataConnection custom property to negate them. For example, ((user.group=&quot;YourGroup&quot;)) becomes ((user.group=&quot;YourGroup&quot; and resource.@QuarantinedDataConnection.Empty())). . | These name change ensures that the data connection cannot be read in an application’s script by the scheduler, and the owner change confirms that the original owner of the user can no longer read the connection via the default security rule OwnerRead, and the security rule modifications ensure that the users cannot read the data connections by some other custom data connection rules if they have a value in the QuarantinedDataConnection custom property. . . Priority . Connections that have never been used, especially connections that were not created recently. Recently created connections that have never been used simply may not have been used yet–so those should not be removed. Otherwise, these connections are purely clutter, that more than likely were once used for testing connectivity. . | Connections that were used by applications where the applications have now been deleted. These are more than likely deprecated data sources. . | Used data connections that haven’t been used in “x” days. . | Data Connection Backup . It might not be the worst idea to take a snapshot of all data connections before removal, in case one needs to be recreated. Data connections are not typically difficult to configure, but it is nice to have to reference in case. . . Scripts to Manage Unused Data Connections . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;It is highly recommended to delete data connections manually, after validating with their respective owners. Please refer to the &lt;em&gt;Suggestions&lt;/em&gt; section above. The scripts below show how data connections can be backed up and programmatically flagged. The flagging (adding of a custom property value) allows for the administrator to temporarily “quarantine” the connections pre-removal.&lt;/p&gt; . Script to Backup All Data Connections . # Script to backup data connections to json # Parameters # Assumes default credentials are used for the Qlik CLI for Windows Connection $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # directory for the output file $outFilePath = &#39;C: &#39; # desired filename of the output file $outFileName = &#39;flagged_unused_connections&#39; ################ ##### Main ##### ################ # set the output file path $outFile = ($outFilePath + $outFileName + &#39;.json&#39;) # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # GET all data connection&#39;s full JSON elements and write them to a file Get-QlikDataConnection -raw -full | ConvertTo-Json | Set-Content $outFile . Script to Add Custom Property value to Data Connections from an Excel Export . It is assumed that the Data Connection ID column has been added to a table in the Data Connection Analyzer and exported to Excel. This file is then referenced in the below script. . # Script to import data connection ids from excel and add a custom property value to them, # as well as optionally change name and ownership. # If the custom property doesn&#39;t exist, it will be created. # Assumes the ImportExcel module: `Install-Module -Name ImportExcel`. # GUID validation code referenced from: # https://pscustomobject.github.io/powershell/functions/PowerShell-Validate-Guid-copy/ # Parameters # Assumes default credentials are used for the Qlik CLI for Windows Connection $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # fully qualified path to excel file with data connection ids $inputXlsxPath = &#39;&lt;fully qualified directory&gt; &lt;filename&gt;.xlsx&#39; # column number of data connection id column in Excel file $dataConnectionIdColumnNumber = &#39;12&#39; # name of the custom property to put on unused data connections--if it doesn&#39;t exist it will be created $customPropertyName = &#39;QuarantinedDataConnection&#39; # 1 for true and 0 for false -- changes the owner to sa_repository $changeOwner = 1 # 1 for true and 0 for false -- prefixes the name with &#39;QUARANTINED - &#39; $changeName = 1 # directory for the output file $outFilePath = &#39;C: &#39; # desired filename of the output file $outFileName = &#39;flagged_unused_connections&#39; ################ ##### Main ##### ################ # set the output file path $outFile = ($outFilePath + $outFileName + &#39;.csv&#39;) # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # if the output file already exists, remove it if (Test-Path $outFile) { Remove-Item $outFile } # function to validate GUIDs function Test-IsGuid { [OutputType([bool])] param ( [Parameter(Mandatory = $true)] [string]$ObjectGuid ) [regex]$guidRegex = &#39;(?im)^[{(]?[0-9A-F]{8}[-]?(?:[0-9A-F]{4}[-]?){3}[0-9A-F]{12}[)}]?$&#39; return $ObjectGuid -match $guidRegex } # import data connection ids from excel $data = Import-Excel $inputXlsxPath -DataOnly -StartColumn $dataConnectionIdColumnNumber ` -EndColumn $($dataConnectionIdColumnNumber + 1) # validate GUIDs and only use those (handles nulls/choosing wrong column) $dataConnectionIds = $data | foreach { $_.psobject.Properties } | where Value -is string ` | foreach { If(Test-IsGuid -ObjectGuid $_.Value) {$_.Value} } # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # GET the sa_repository user $sa_repository = Get-QlikUser -filter &quot;userId eq &#39;sa_repository&#39; and userDirectory eq &#39;INTERNAL&#39;&quot; # GET the custom property to use for unused data connections $dataConnectionCustomProperty = Get-QlikCustomProperty -filter &quot;name eq &#39;$customPropertyName&#39;&quot; -raw # GET the id of the custom property $dataConnectionCustomPropertyId = $dataConnectionCustomProperty.id # if the custom property doesn&#39;t exist, create it ($customPropertyName) if (!$dataConnectionCustomProperty) { $dataConnectionCustomProperty = New-QlikCustomProperty -name &quot;$customPropertyName&quot; ` -objectType &quot;DataConnection&quot; -choiceValues &quot;true&quot; -raw # GET the id of the custom property $dataConnectionCustomPropertyId = $dataConnectionCustomProperty.id } # for each data connection id foreach ($dataConnection in $dataConnectionIds) { # GET the existing data connection&#39;s full JSON $resp = Get-QlikDataConnection -id $dataConnection -raw # store the current name of the data connection $dataConnectionName = $resp.name # get the current custom properties assigned to the data connection, if any $currentCustomProperties = $resp.customProperties # set a flag to check if the custom property already is assigned to the data connection $dataConnectionPropAlreadyThere = $false # for each custom property in the data connection&#39;s current custom propertys foreach ($customProperty in $currentCustomProperties) { # if the custom property is already there, set the flag to &quot;true&quot; if ($customProperty.definition.id -eq $dataConnectionCustomPropertyId) { $dataConnectionPropAlreadyThere = $true break } } # if the custom property isn&#39;t already there, # create the JSON element for it and add it to the array if (!$dataConnectionPropAlreadyThere) { $newCustomProp = @{ value = &quot;true&quot; definition = @{ id = &quot;$dataConnectionCustomPropertyId&quot; } } $resp.customProperties += $newCustomProp } # change the name of the data connection, set by the $changeName flag if ($changeName) {$resp.name = $(&#39;QUARANTINED - &#39; ` + $resp.name.Replace(&#39;QUARANTINED - &#39;,&#39;&#39;))} # change the owner of the data connection, set by the $changeOwner flag if ($changeOwner) {$resp.owner = $sa_repository} # convert the response to JSON $resp = $resp | ConvertTo-Json -depth 10 # PUT the new data connection Invoke-QlikPut -path /qrs/dataconnection/$dataConnection -body $resp # logging &#39;PUT: &#39; + $dataConnectionName + &#39;,&#39; + $dataConnection Add-Content -Path $outFile -Value $($dataConnectionName + &#39;,&#39; + $dataConnection) } . Tags . #monthly . #asset_management . #data_connections . #data_connection_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/data_connections/remove_unused_data_connections.html",
		"relUrl": "/docs/asset_management/data_connections/remove_unused_data_connections.html"
	},
	"53": {
		"title": "Remove Unused Private Sheets",
		"content": "Remove Unused Private Sheets * . Cadence Quarterly . Sites production w/ self-service .   Initial Recurring . Estimated Time | 1-2 hr | 30 min | . Benefits: . Increase performance | Reduce maintenance | . . Goal . In environments where self-service is enabled, i.e. users have the ability to create their own private sheets on published applications, with a high number of users, the number of private sheets can ultimately grow very large. The goal of this section is to illustrate how to put policies/practices in place to consistently keep the number of private sheets under control–allowing for a tidy/more performant site. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;“Base” and “Community” sheets should be handled a bit differently–please refer to &lt;a href=&quot;/docs/asset_management/apps/flag_unused_base_community_sheets.html&quot;&gt;Flag Unused Base/Community Sheets&lt;/a&gt;.&lt;/p&gt; . Table of Contents . Suggested Prerequisites | Audit Activity Log | Operations Monitor Confirm Operations Monitor is Operational | . | Identification of Unused Private Sheets | Suggested Actions Retention Policy | Process | . | Bulk Private Sheet Removal Video Walk-Through | Script to Tag Unused Private Sheets | Script to Delete Tagged Sheets Prerequisites | Steps | . | . | . . Suggested Prerequisites . Remove/Quarantine Unused Apps | . . Audit Activity Log . As of the February 2019 release, an improvement was added to the product to log sheet usage at default log levels. This enables the ability to measure sheet adoption as well as manage the amount of sheets in the applications–keeping them trimmed to only what is being leveraged. . Ensure that the Audit Activity log level is set to Basic for every engine. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This is the default setting, but it is encouraged for the Qlik administrator to confirm what is configured for their environment(s).&lt;/p&gt; . . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Identification of Unused Private Sheets . Open up the Operations Monitor application, inside of Monitoring Apps stream: . . Navigate to the Sheet Usage sheet. . . Sort by Unused Private Sheets descending. . . Now ideally, any unused applications should be removed before this sheet pruning activity. This will allow for bulk removal of those private sheets as the entire app has been identified as unused. This example is intended to only remove private sheets from published applications that are used. Ensure that Remove/Quarantine Unused Apps has been followed first. . From the App Stream column, select Unpublished, and then select Select alternative so that all streams are selected (all published applications). Ensure that if a Quarantine stream exists, that it is also deselected – as there is no need to prune applications that are already marked for potential removal. . . . Now on this example server, the Operations Monitor application has been selected, and it is visible that there is a single unused prviate sheet. In a real-world environment, there would potentially be many, many more, but as this is taken from a rarely used testing environment, this simple example will suffice. . . On the Sheet Usage table, select Private Sheet under the Sheet Type column. . . In this scenario, the App Usage sheet has not been used in over one month but less than two. Ideally, private sheets should only be removed if they have not been used in &gt; 90 days or more, but this needs to be defined by a policy decided internally. Please refer to the Retention Policy section below. . . Suggested Actions . Retention Policy . It is highly suggested to have a corporate policy in place for unused private sheet retention. As the number of these sheets can grow very large and it can become quite difficult to manage manually, an automatic approach is suggested. In order to implement this style of policy, there should be notifications in place so that users are informed that action is needed to retain important assets. . Example Policy . Unused sheets will be purged after &gt; 90 days, run at the start of each business quarter. | An email will go out to all Qlik users two weeks prior to the start of the quarter, letting them know that in order retain their assets, they must ensure that they have been accessed during the next two weeks. The simplest way to do this is is with a generic email to all Qlik users, however if Qlik NPrinting is available to the corporation, NPrinting could be used to customize the emails to include the specific sheets that would be expiring sent to each specific individual. | A reminder email be sent one week following the initial email. | . A Warning . Deleting sheets is a permanent operation. Ensure that every measure/precaution/warning has been taken so that users are well aware of the resulting action. . Process . Within the Operations Monitor on the Sheet Usage sheet, after following process for identifying unused sheets, export the Sheet Usage table to Excel. This output should only contain the private sheets which have been qualified by the aforementioned activity. . | Either manually or programmatically tag the sheets with a desired tag in the QMC. Ensure that this tag is specific to this process, for example UnusedPrivateSheet. If programmatically tagging, follow the Script to Tag Unused Private Sheets section. . | It is suggested to contact the owners of the private sheets so that they can access the sheets that they desire to retain (therefore rendering them no longer “unused”). Refer to the Retention Policy example above. . | After a decided-upon amount of time has passed to allow users to access their content, the tag should then be removed from the sheets. The simplest way to achieve this is to delete the tag from the QMC, as that will remove it from all resources in one shot. . | Ensure that the Operations Monitor has been reloaded, and then repeat steps 1 and 2, re-creating the tag. . | Delete the remaining tagged sheets by following the Script to Delete Tagged Sheets section below. . | . Bulk Private Sheet Removal . The below script snippet requires the Qlik CLI for Windows. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;When possible, one should always remove private sheets manually, leaving that responsibility to the owner of the sheets. That being said, this is not typically possible in large organizations.&lt;/p&gt; . The script below will tag any private sheets with the tag UnusedPrivateSheet. It expects an Excel file (XLSX) as an input, where the name of the column with the Sheet Id is specified. This allows for the Qlik Administrator to export a filtered down list from the Sheet Usage table in the Sheet Usage sheet of the Operations Monitor. . Video Walk-Through . . Script to Tag Unused Private Sheets . # Function to tag private sheet ids from excel and tag them # Assumes the ImportExcel module: `Install-Module -Name ImportExcel` # Assumes tag exists, such as &#39;UnusedPrivateSheet&#39; # GUID validation code referenced from: https://pscustomobject.github.io/powershell/functions/PowerShell-Validate-Guid-copy/ ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;&lt;machine-name&gt;&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # set the number of days back for the app created date # fully qualified path to excel file with sheet ids $inputXlsxPath = &#39;&lt;absolute file path&gt;/&lt;filename&gt;.xlsx&#39; # column number of sheet id column in Excel file $sheetIdColumnNumber = &#39;9&#39; # the desired name of the tag to tag sheets with - it must exist in the QRS $tagName = &#39;UnusedPrivateSheet&#39; # directory for the output file $outFilePath = &#39;C: &#39; # desired filename of the output file $outFileName = &#39;tagged_private_sheets&#39; ################ ##### Main ##### ################ # set the output file path $outFile = ($outFilePath + $outFileName + &#39;.csv&#39;) # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # if the output file already exists, remove it if (Test-Path $outFile) { Remove-Item $outFile } # function to validate GUIDs function Test-IsGuid { [OutputType([bool])] param ( [Parameter(Mandatory = $true)] [string]$ObjectGuid ) [regex]$guidRegex = &#39;(?im)^[{(]?[0-9A-F]{8}[-]?(?:[0-9A-F]{4}[-]?){3}[0-9A-F]{12}[)}]?$&#39; return $ObjectGuid -match $guidRegex } # import sheet ids from excel $data = Import-Excel $inputXlsxPath -DataOnly -StartColumn $sheetIdColumnNumber -EndColumn $($sheetIdColumnNumber + 1) # validate GUIDs and only use those (handles nulls/choosing wrong column) $sheetIds = $data | foreach { $_.psobject.Properties } | where Value -is string | foreach { If(Test-IsGuid -ObjectGuid $_.Value) {$_.Value} } # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts # add headers to output csv Add-Content -Path $outFile -Value $(&#39;SheetObjectName,SheetObjectSheetId,SheetObjectAppId,SheetObjectAppName&#39;) # GET desired tag JSON $tagsJson = Get-QlikTag -filter &quot;name eq &#39;$tagName&#39;&quot; -raw # get the id of the tag $tagId = $tagsJson.id # if the tag exists if($tagsJson) { # for each tag foreach ($sheetId in $sheetIds) { # GET the object, ensuring it is a private sheet $sheetObjJson = Get-QlikObject -filter &quot;published eq false and approved eq false and id eq $sheetId&quot; -full -raw # if the object exists and is a private sheet if ($sheetObjJson) { # set a flag to check if the tag is already assigned to the sheet $tagAlreadyThere = $false # get the current tags assigned to sheet, if any $currentTags = $sheetObjJson.tags # for each tag foreach ($tag in $currentTags) { # if the target tag is already there, set the flag to &quot;true&quot; if ($tagId -eq $tag.id) { $tagAlreadyThere = $true break } else { continue } } # get the sheet name, app id, and app name $sheetObjName = $sheetObjJson.name $sheetObjAppId = $sheetObjJson.app.id $sheetObjAppName = $sheetObjJson.app.name # if the tag isn&#39;t already there, add it if (!$tagAlreadyThere) { $sheetObjJson.tags += $tagsJson # convert to JSON for the PUT $sheetObjJson = $sheetObjJson | ConvertTo-Json # PUT the sheet with the new tag Invoke-QlikPut -path /qrs/app/object/$sheetId -body $sheetObjJson } # write output Add-Content -Path $outFile -Value $($sheetObjName + &#39;,&#39; + $sheetId + &#39;,&#39; + $sheetObjAppId + &#39;,&#39; + $sheetObjAppName) } # the sheet is not a community sheet else { $sheetId + &#39; is not a private sheet. Skipping.&#39; } } } # the tag doesn&#39;t exist else { &quot;Tag: &#39;&quot; + $tagName + &quot;&#39; doesn&#39;t exist. Please create it in the QMC.&quot; } . Once the script has been run above, and a review of the tagging has been confirmed as correct, the script below can be run to permanently delete these base/community sheets. This process cannot be reversed. . . Script to Delete Tagged Sheets . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;It is highly recommended to &lt;em&gt;backup your site and applications&lt;/em&gt; before considering taking the approach of programmatic sheet removal. This process cannot be reversed. The sheet pointers are stored in the repository database, and the sheets reside within the qvfs themselves.&lt;/p&gt; . In order to completely remove sheets from both an application and the repository database, the Qlik Engine JSON API must be used. To work with this API, the sample script leverages Enigma.js. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;If it is attempted to use the QRS API to remove sheets instead of the Engine API, only the “pointers” to those sheets will be removed from the repository database–the sheet information itself stored inside of the qvf will not be removed. This is why the Engine API must be leveraged for programmatic deletion, as it purges both.&lt;/p&gt; . Prerequisites . NodeJS | . This process uses NodeJS to interact with the Qlik Engine JSON API. To confirm that NodeJS is installed and properly configured, run the following commands in cmd.exe: . node --version | npm --version | . Steps . Download the following files from here and place them in a desired folder. remove_tagged_private_sheets.js | package.json | . | Edit the following mandatory variables in remove_tagged_private_sheets.js host | TAG_TO_SEARCH_FOR | . | Open a cmd prompt, and navigate to the folder from step 1. | Enter npm install | To execute the program, enter node remove_tagged_private_sheets.js | Refer to both log.txt and output.csv | Tags . #quarterly . #asset_management . #apps . #sheets . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/remove_unused_private_sheets.html",
		"relUrl": "/docs/asset_management/apps/remove_unused_private_sheets.html"
	},
	"54": {
		"title": "Remove Unused Streams",
		"content": "Remove Unused Streams . Cadence Monthly . Sites developmentproduction .   Initial Recurring . Estimated Time | 2 min | 2 min | . Benefits: . Decrease maintenance | Increase focus | . . Goal . There are several reasons why empty streams may be present in a Qlik environment. Sometimes, applications are moved from one stream to another and the admin doesn’t notice that the former stream is now empty. Other times, streams are created in anticipation of new applications being published to them, yet none ever actually get published. It is always a good practice to periodically check on the validity of the streams and to remove those that are deemed unnecessary. . Table of Contents . QMC - Manually Check Published Apps For Each Stream | Get List of Unused Streams (Qlik CLI for Windows) Script | . | . . QMC - Manually Check Published Apps For Each Stream . In the QMC, select Streams: . . To view all applications that were published to a specific stream, select the stream name and enter into edit mode. . . In Edit mode, select the Apps tab. . . Applications that were published to this stream will now be visible. . . This process should be repeated for each Stream to find unused streams. . If there is a considerable quantity of streams, consider the Qlik CLI for Windows method. . . Get List of Unused Streams (Qlik CLI for Windows) . The below script snippet requires the Qlik CLI for Windows. . Streams should always be removed manually. The below script is only used to identify what streams are empty. . Script . # Script to find empty streams ################ ## Parameters ## ################ # Assumes default credentials are used for the Qlik CLI for Windows Connection # machine name $computerName = &#39;machineName&#39; # leave empty if windows auth is on default VP $virtualProxyPrefix = &#39;/default&#39; # directory for the output file $filePath = &#39;C: tmp2 &#39; # desired filename of the output file $fileName = &#39;empty_streams&#39; # desired format of the output file (can be &#39;json&#39; or &#39;csv&#39;) $outputFormat = &#39;json&#39; ################ ##### Main ##### ################ # create filePath if (Test-Path $filePath) { } else { New-Item -ItemType directory -Path $filePath | Out-Null } # set the output file path $outFile = ($filePath + $fileName + &#39;_&#39; + $(Get-Date -f &quot;yyyy-MM-dd&quot;) + &#39;.&#39; + $outputFormat) # if the output file already exists, remove it if (Test-Path $outFile) { Remove-Item $outFile } # set the computer name for the Qlik connection call $computerNameFull = ($computerName + $virtualProxyPrefix).ToString() # connect to Qlik Connect-Qlik -ComputerName $computerNameFull -UseDefaultCredentials -TrustAllCerts | Out-Null # GET all streams $streamJson = Get-QlikStream -raw # GET all applications, and then get all unique stream ids $appStreamIds = Get-QlikApp -filter &quot;published eq true&quot; | foreach{$_.stream.id} | Sort-Object | Get-Unique # compare both lists to see if any stream ids belong to no apps $emptyStreamIds = ($streamJson | foreach{$_.id}) | ?{$appStreamIds -notcontains $_} # if there are any empty streams, retain the full detail of them $streamEmptyJson = $streamJson | ?{$emptyStreamIds -contains $_.id} # see if there are any empty streams (&amp;{If($emptyStreamIds.count) {$(&quot;Empty Streams Found: &quot; + $emptyStreamIDs.count); $streamEmptyJson} Else {&quot;No Empty Streams Found&quot;}}) # if there are any empty streams, write them to $outfile If ($emptyStreamIds.count) { (&amp;{If($outputFormat.ToLower() -eq &#39;csv&#39;) { $streamEmptyJson | ConvertTo-Csv -NoTypeInformation | Set-Content $outFile } Else { $streamEmptyJson | ConvertTo-Json | Set-Content $outFile } }) } . Tags . #monthly . #asset_management . #streams .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/streams/remove_unused_streams.html",
		"relUrl": "/docs/asset_management/streams/remove_unused_streams.html"
	},
	"55": {
		"title": "Resiliency & HA",
		"content": "Resiliency &amp; HA . Goal . The goal of this page is to gain an understanding of the various resiliency and high availability options available to a Qlik site. . Table of Contents . Resiliency &amp; High Availability | . . Resiliency &amp; High Availability . When speaking about resiliency and high availability within the context of Qlik architecture, there are three tiers to focus on: . User Resiliency Requires 2+ Qlik proxy/engine nodes | Requires third-party network load balancer | . | Reload Resiliency Requires 2+ Qlik scheduler nodes | . | Site-wide High Availability Requires both 1 and 2 from above | Requires decoupled repository database and decoupled file share | . The repository database can be stream replicated or clustered for resiliency | The file share must be resilient - Requires 2+ Qlik nodes with all services enabled, with 1+ nominated as failover candidates | . | A User Resilient site minimally would look like this: . . A Reload Resilient site minimally would look like this: . . Site-wide High Availability would minimally look like this: . . But for the vast majority of Enterprise deployments, Site-wide High Availability would minimally look like this: . . As organizations need to scale out further, they can opt to horizontally scale either the compute dedicated to the front and/or back end nodes as the needs of the organization dictate: . . Tags . #architecture . #resiliency . #ha . #HA .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_architecture_scale_plan/resiliency_ha.html",
		"relUrl": "/docs/system_planning/review_architecture_scale_plan/resiliency_ha.html"
	},
	"56": {
		"title": "Review App Cache Warming",
		"content": "Review App Cache Warming . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 120 Min | 5 min | . Benefits: . Increase performance | Segregation/isolation of resources | . Goal . The goal of this activity is for the Qlik Administrator to review available tooling to determine whether adoption of a cache warming strategy is appropriate for their environment. . Table of Contents . What is Cache Warming | When is Cache Warming appropriate? | Scenario (1), large apps Qualifying App Popularity | Determining App Size What is Large? | QMC (Basic) | App Metadata Analyzer (Advanced) | . | . | Scenario (2), Important Apps | Implementing a Cache Warming Strategy | . . What is Cache Warming . Briefly, cache warming refers to programmatically opening Qlik apps to ensure that the first user to open the app has a better response time. For a more exhaustive explaination refer to the write-up on Cache Warming for more details on what cache warming is. . When is Cache Warming appropriate? . The exact answer will always vary between Qlik sites, but broadly the two use cases where this strategy is commonly needed is: . For frequently accessed large apps | To ensure high performance | Use case (1) is incredibly common for many Qlik sites. It is common to have a few larger Qlik apps which are frequently or semi-frequently used and to have complaints of poor performance. . Use case (2) is less common but may be required due to hard or soft SLAs for the Qlik deployment or generally due to provide a higher level of performance for select apps. . If the Qlik Administrator already knows which app(s) need to be cache warmed due need then the Administrator can continue to the article on Cache Warming to select an appropriate approach. . Scenario (1), large apps . In this section we will outline how to use two pieces of tooling to help aid qualifying what Qlik apps should be targetted for a cache warming strategy. . Qualifying App Popularity . It is common for customers to use the qualifications of top 5 / 10 / 15 of number of sessions as a measure of popularity. Use the method outlined in Analyze App Adoption to determine which apps are most popular. . Determining App Size . With the list of Qlik apps which are most popular, the Qlik Administrator needs to now determine which of those app(s) are large. . For the purposes of this exercise we will work off an example that we have determined that the app named Generic DoD HR Project - 1.1M Sample Hash256() has been determined to be sufficiently popular to investigate it’s app size. This approach should be repeated for all large Qlik apps which have moderate popularity. . What is Large? . Qlik apps can perform well at scale, but a common baseline is a large Qlik app is one which is greater than 1 GB on disk / 5 GB in RAM. . QMC (Basic) . In the QMC, navigate to the Apps section. . . Using the Column selector, toggle the File Size (MB) column. . . In the newly constructed table, use the filter on the Name column to filter on the app by its name and inspect the File Size (MB) column. . . Repeat this process for all popular apps. If the app(s) is large, then a cache warming strategy is appropriate. . App Metadata Analyzer (Advanced) . In the previous section we used the QMC to inspect the application’s size on disk. That method provided a quick and dirty metric for the size of a Qlik app in memory. In this section we will use an additional tool to provide a more accurate determination. . Refer to the App Metadata Analyzer section for more guidance on configuring this app. . With a configured and reloaded app, open the app in the Hub, and open the Dashboard sheet. . . Use the global search section to search for the app(s) by name. . . . With the app(s) selected, inspect the App Memory Footprint (MB) table for the app’s in-memory size. . . Scenario (2), Important Apps . For this scenario it is expected that the Qlik Administrator has received feedback from their user population about the relative importance of Qlik apps. This may be due to the audience for this app (i.e. executives) or due to demos being performed on the platform for a important audience. . Implementing a Cache Warming Strategy . If app(s) need to be cache warmed, then the Qlik administrator should decide on an appropriate tool for the task. Refer to Cache Warming for more guidance on this. . Tags . #quarterly . #asset_management . #apps . #cache_warming .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/review_app_cache_warming.html",
		"relUrl": "/docs/asset_management/apps/review_app_cache_warming.html"
	},
	"57": {
		"title": "Review Architecture/Scale Plan",
		"content": "Review Architecture/Scale Plan . Cadence Yearly . Sites production .   Initial Recurring . Estimated Time | 2-3 hrs | 1 hr | . Benefits: . Provide documentation | Increase architectural knowledge | Plan for future deployments | . . Goal . The goal is to have documented Qlik architecture diagrams of n and n+1 deployments*, as well as an understanding of high-level architectural concepts within Qlik. . * n refers to the current deployment, while n+1 refers to an anticipated state of the next deployment. . This is integral for: . understanding the deployment and how it is laid out . | planning for growth . | articulating change . | understanding resiliency &amp; availability . | having documentation available to others . | . . Table of Contents . Supporting Documentation | Building an Architecture Diagram Core Requirements | On-Premises Diagram Example | AWS Diagram Example | Azure Diagram Example | . | Planning for N+1 Architectures High-level Scaling Concepts | Review Capacity Plan | . | . . Supporting Documentation . Please take the time to review the below if unfamiliar before continuing on with this page. . Architecture 101 (Components, Terminology) . | Load Balancing Concepts . | Resiliency &amp; HA . | Example Production Architectures . | . . Building an Architecture Diagram . Core Requirements . An editor. This could be Visio, PowerPoint, or web editors like Gliffy and Draw.io. . | A set of base icons or symbols . If the deployment is on-premises . a server . | a database . | a file share . | a network load balancer . | . | If the deployment is in the cloud: . AWS icons can be found here . | Azure icons can be found here . | GCP icons can be found here . | . | . | Knowledge of what Qlik services are active on what nodes . | Knowledge of what each Qlik node is being used for . | Knowledge of where the Qlik file share is and the Qlik repository database is . | . Nice to Haves . Server names and aliases . | Any network load balancers/interfaces in front of Qlik . | Any firewall settings pertinent to Qlik . | . On-Premises Diagram Example . Growth Environment . . Enterprise Environment . . AWS Diagram Example . Enterprise Environment . . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;Please note that these cloud diagrams are intended for Qlik admins and occasionaly used to translate needs to supporting LOBs like IT. The examples below do not conform to the individual cloud vendor architectural diagram standards, as these aren’t designed to be consumed by cloud engineers/network admins, etc. If one would like to include VPCs, AZs, SGs, Network ACLs, all the better – however it goes beyond the basics of this exercise.&lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;SMB file share - either FSx (requires domain) or EBS storage on EC2 instance.&lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;One could also leverage the &lt;em&gt;Growth Environment&lt;/em&gt; in AWS, with both the repository database and fileshare on an AWS EC2 instance.&lt;/p&gt; . Azure Diagram Example . Enterprise Environment . . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;Please note that these cloud diagrams are intended for Qlik admins and occasionally used to translate needs to supporting LOBs like IT. The examples below do not conform to the individual cloud vendor architectural diagram standards, as these aren’t designed to be consumed by cloud engineers/network admins, etc. If one would like to include Virtual Networks, Subnets, Resource Groups, etc, all the better – however it goes beyond the basics of this exercise.&lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;Azure Files for SMB Storage with Cmdkey or Windows Credential Manager.&lt;/p&gt; . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;One could also leverage the &lt;em&gt;Growth Environment&lt;/em&gt; in Azure, with both the repository database and fileshare on an Azure VM.&lt;/p&gt; . . Planning for N+1 Architectures . In order to plan for an upcoming architectural event, it is imperative to have an understanding of the varying methods of scaling a site, as well an awareness of any architectural impacts the current Capacity Plan might have. . High-level Scaling Concepts . Broadly speaking, there are two primary scaling methodologies – however, do note that these are not mutually exclusive: . Horizontal Scaling Adding additional nodes/services, providing a wide, resilient topology. | . | Vertical Scaling Expanding current server footprints, i.e. adding additional cores/RAM. | . | Horizontal scaling is typically common if a Qlik environment has small to medium sized applications with many users. Meaning, applications can be loaded quickly onto many different engines with little delay, and calculations are fast – meaning that a shared cache isn’t necessarily as integral for these applications. This methodology is also common in virtual environments on-premises where VM sizes may be restricted. For instance, if an organization caps VM sizes at 96 or 128 GB of RAM, more than likely that Qlik environment will end up with a wider footprint, and will adopt practices to allow their applications to fit it. . Vertical scaling is typically common where the user base is not extensive, and the applications are quite large. Less nodes with larger capacity allows for larger applications with more users taking advantage of the same cache. These applications are usually cache warmed so that they are readily available for users without delay. . Both of these methodologies are frequently combined when an organization has a mix of both very large apps and smaller apps with a wide user pool. It is usually common for organizations to have “small - medium app engines” and “large app engines” – for example, maybe four of the former and two of the latter. Leveraging load balancing rules (as described in Review Pinning/Load Balancing), large applications are “pinned” to the larger nodes, and vice versa. . Review Capacity Plan . In order to plan for the next architectural event, one must first review the current Capacity Plan. . Common questions that would have impact: . Is there a significant license growth event that would mandate additional proxy/engine nodes? . | In general across end-user engine nodes, are the CPU/RAM metrics in a healthy state consistently? If not, this might warrant the need for vertical growth or app optimization. . | Are there intra-day reloads running on end-user nodes that are affecting performance, therefore the end-user experience? That could warrant offloading them to dedicated schedulers (this is highly encouraged and preferred). . | Are there applications that are being considered for “application pinning” to specific engine nodes via load balancing rules? Could application optimization bring these applications down in size to avoid that, or are they simply monolithic by nature and need to be pinned? Are there enough engine nodes currently to support the segregation of assets while providing resiliency (2+ nodes for each), or do more engine nodes need to be added? Is vertical growth required to support these large applications on less nodes, given the fact that there will be more user caching on less nodes potentially? . | Is horizontal growth preferred, or is vertical growth preferred, or is there a business event driving one or the other? Is a mix of both possible? This will involve discussions with IT to see what is possible. . | Is something outside the Qlik deployment driving an architectural event, e.g. there is money to be spent on infrastructure now, though a license event might not occur for another 6 months? This will involve speaking with the business to see what types of applications/use cases are in the pipeline to see what infrastructure should support future needs. . | Is ODAG in play or going to be in play? Should these application reloads happen on a dedicated scheduler? . | Is Qlik NPrinting or Qlik InsightBot on the horizon or in play? Should these run against dedicated engines? . | These are all questions that should be considered while planning for the next-state architecture. . Tags . #yearly . #system_planning . #architecture . #scale .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_architecture_scale_plan.html",
		"relUrl": "/docs/system_planning/review_architecture_scale_plan.html"
	},
	"58": {
		"title": "Review Data Exports",
		"content": "Review Data Exports . Cadence Monthly . Sites production .   Initial Recurring . Estimated Time | 30 min | 15 min | . Benefits: . Understand user behavior | Increase analysis within Qlik | . . Goal . The goal for this activity is to review the Operations Monitor in Qlik Sense Enterprise to review what user(s) are exporting data to Excel. Exports to Excel are expensive for the Qlik Engine since it needs to: . Construct a duplicative hypercube of the requested data | Calculate the aggregates (if needed, e.g. pivot tables) | Explode the hypercube from memory and dump it to disk | Notify the browser that there is a file ready for download | Deliver that stream across the network to the end user | . For the majority of use cases, bulk exports to Excel signal that dashboards need to be better optimized to meet the work-flow needs of the user base. The action from this activity is for the administrator to consult with the app’s owner / developer and potentially the end user(s) to discuss the data needs that they have and how the Qlik app can better support them. Alternatively an upgrade to Qlik Sense Enterprise June 2019 is warranted to enable the Copy value to clipboard functionality which allows a user to copy just a cell value from Qlik. . Table of Contents . Operations Monitor Confirm Operations Monitor is Operational | Review Operations Monitor | . | . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Review Operations Monitor . Open up the Operations Monitor application and navigate to the Export Overview sheet: . . Inside the app review the Users Exporting table for a list of users who have exported data to Excel (1) and review the Export Details table for the applications where the exports originated from (2). . . Tags . #monthly . #audit . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/audit/review_data_exports.html",
		"relUrl": "/docs/audit/review_data_exports.html"
	},
	"59": {
		"title": "Review Disk Space",
		"content": "Review Disk Space . Cadence Monthly . Sites all .   Initial Recurring . Estimated Time | 20 min | 20 min | . Benefits: . Increase stability | . . Goal . The goal for this monthly activity is to review the available disk space on the storage devices which are used by Qlik Sense Enterprise. For some organizations, IT groups have monitoring configured for shared storage. In these instances, this activity may not be needed by administrators of those Qlik Sense deployments. This activity will instruct the administrator on how to enumerate the storage dependencies for their Qlik Sense Enterprise cluster and thus allow the administrator to manually check for disk space usage or confirm with their IT groups that those devices are already monitored. . Table of Contents . Enumerate Storage used by a Qlik Sense Cluster Storage used for Data Connections | Storage used for Qlik Sense storage | Local storage for each Qlik Sense node | . | . . Enumerate Storage used by a Qlik Sense Cluster . Storage used for Data Connections . In the QMC, select Data Connections. . . In the upper right hand side of the screen, select the Column selector, and then select the Connection String and Type columns. To make the resulting table a bit more manageable, optionally deselect the Tags column. . . Now select the filter icon for the Type column and then filter on folder. . . At this stage sort by Connection String so that the groupings are visible for paths that are used for data sources. . . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;If this is a multi-node Qlik Sense site then the presence of local paths (&lt;code class=&quot;language-html highlighter-rouge&quot;&gt;C: &lt;/code&gt;, &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;D: &lt;/code&gt;) is concerning. Refer to &lt;a href=&quot;.. asset_management data_connections analyze_data_connections.html&quot;&gt;Analyzing Data Connections&lt;/a&gt; for guidance on converting these to UNC share paths.&lt;/p&gt; . Storage used for Qlik Sense storage . With the distinct locations that are used for data connections, go to the Service Cluster section in the QMC. . . Add this location to the running list from the previous section on Data Connections. . Local storage for each Qlik Sense node . Go to the Nodes section in the QMC. . . Add to the list the server names for each member of the Qlik Sense Enterprise cluster. . After completing each step, the administrator will have an enumerated list of storage dependencies for the Qlik Sense cluster. . Data source storage | Storage for the shared resources across the nodes | Local storage for the servers in the Qlik Sense cluster | . At this juncture, the administrator should check the percentage of available free space on each device. For shared storage which is administered by other teams (e.g. SAN or NAS devices) confirm with the relevant team(s) as to percentage of available free space. . If monitoring is not configured for any of the enumerated storage dependencies, then the administrator should record the percentage in a location for reference on the following month. This will allow the administrator to build out projections for the storage needs over the next 6-24 months. This projection then can be used to justify budget for additional storage for shared storage devices or to anticipate where additional storage should be allocated to VMs (if virtualization is used). . Tags . #monthly . #system_planning . #disk .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_disk_space.html",
		"relUrl": "/docs/system_planning/review_disk_space.html"
	},
	"60": {
		"title": "Review License Allocation",
		"content": "Review License Allocations . Cadence Weekly . Sites all .   Initial Recurring . Estimated Time | 20 min | 10 min | . Benefits: . Ensure licenses are available | Plan for license growth | . Goal . The goal of this activity is to evaluate license growth and needs using the built in License Monitor application. . Table of Contents . License Monitor Confirm License Monitor is Operational | . | Check License Growth | . . License Monitor . This page leverages the License Monitor. Please refer to the License Monitor page for an overview and relevant documentation links. . Confirm License Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the License Monitor application. Confirm that the application’s data is up-to-date. . . If the License Monitor is not up-to-date, please refer to the License Monitor Documentation for configuration details and troubleshooting steps. . . Check License Growth . First check the QMC to see how many total licenses are available for Professional and Analyzer users. Navigate to the License Management section of the QMC. . . Ensure License Usage Summary is selected on the right, and then check the total licenses on the left for both Professional and Analyzer. . . Next, navigate to the Monitoring Apps section of the QMC, which will route to the Hub. . . Select the License Monitor application. . . Select the Overview sheet. . . Click the Duplicate button on the toolbar. . . Edit the User Timeline chart by selecting it and deleting the Access Type measure. . . . With Access Type removed, open the Measures pane and select Add Trend under the User Accessing Apps measure. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This feature was introduced in the February 2020 release, and is documented &lt;a href=&quot;https://help.qlik.com/en-US/sense/Subsystems/Hub/Content/Sense_Hub/Measures/trend-lines.htm&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. If the site is not on this release or later, alternative methods can be explored &lt;a href=&quot;https://community.qlik.com/t5/Qlik-Sense-Documents-Videos/Calculating-trend-lines-values-and-formulas-on-charts-and-tables/ta-p/1479463&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; using functions like &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;linest_m()&lt;/code&gt; and &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;linest_b()&lt;/code&gt; functions.&lt;/p&gt; . . Select Linear as the Trend Type. . . Move the Allocation Changed in 7 days table to the left and add a Filter Pane in the empty space to the left of the alterered User Timeline chart. . . Select Dimension and choose Access Type. . . . Click Done. . . Select Analyzer in the filter pane and view/analyze the growth rate in analyzer app usage. To do this, grab two points on the chart that intersect with Y axis grid lines. In the sample below these two values are ‘20’ and ‘30’ respectively. Since the two points are 4 months apart, one can deduce that the linear growth rate is approximately 2.5 new analyzers per month. . . Repeat for Professional. In this sample, the growth line is flat. . . In this exercise, it is apparent that 100 analyzer licenses are available. Since the most recent month shows ‘32’ in use, we can expect that licenses will max out in (100-32)/2.5 or ~27 months. These are of course estimates without any other variables in play. . The work done to customize the sheet is auto-saved. To make the sheet clear and readily accessible for next time, rename the sheet to User License Trend by clicking in the white space above the app objects as shown by the arrow in the image below, then alter the label to User License Trend. . . Tags . #weekly . #licensing . #license . #users . #license_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/licensing/review_license_allocations.html",
		"relUrl": "/docs/licensing/review_license_allocations.html"
	},
	"61": {
		"title": "Review/Optimize QVDs",
		"content": "Review/Optimize QVDs . Cadence Quarterly . Sites developmentproduction .   Initial Recurring . Estimated Time | 2 hrs | 15 min | . Benefits: . Reduce Maintenance | Increase Data Consistency | . . Goal . The goal here is to identify and remedy inefficiencies and overlaps in QVD files, as well as detecting anomalies that could point to reload logic issues or unplanned spikes in growth/capacity. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This exercise does not refer to optimizing QVDs in the traditional sense of optimizing their read-time, but rather optimizing the QVD inventory.&lt;/p&gt; . Prerequisite . Qlik Sense QVD Monitor . This pages leverages the Qlik Sense QVD Monitor application. For documentation, please refer to QVD Monitor. . Exercise . Assumptions . Qlik Sense QVD Monitor application is configured | Application is published to the Monitoring apps stream | Daily reload task | *Preferably greater than 14 days worth of data, so that trends are visible | . Process . Open the Qlik Sense QVD Monitor application, and then select the QVD Details sheet. . . . Priority 1 . Note the overall metrics at the top of the sheet, and ensure that all of the QVD folders are reflected in the QVD Growth Rates table. . . Priority 2 . Look for anomalies in the growth rates (pos or neg) of the QVD files. These will be highlighted in red, and the pivot table may need to be expanded in order to see them. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;The default thresholds can be modified in the load script.&lt;/p&gt; . . When the goal is to examine growth by day and any trending of a QVD, display the QVD Metadata Details table at the bottom of the sheet (the first of two objects in the container). . . It is also suggested to view the row count trends of the same QVD in the second object in the container, named Row Count Trends. . . Decisions on whether or not a growth trend is “normal” or “anomalous” are up to the organization. However, in general it is considered a best practice that negative growth is typically something that should be justified, and spikes in growth or irregular growth should also be validated. For those situations, it is best to open the application that builds the QVD in question (QVD generator typically) and/or speak with the developer responsible. It could also be tied to a recent logic change, or source change, and, in that case, should also be validated. . Priority 3 . Navigate to the Search Columns sheet in the application. . . In the Column Occurrences table on the left side of the sheet, a complete list of QVD columns is visible, with the number of occurrences that each column name has across all QVDs. Note that this table only contains the most recent QVD load of all QVDs. . . Select a column in this table to filter the main table down, and view the details about the QVDs that contain that column name. This is critical to ensure that the same column is not being used with either different source data, or logic that produces materially different values across QVDs for the same column. When this happens, it can cause confusion amongst developers/designers/users, and it can cause inconsistencies across apps. Refer to the example below where WorkBookName was chosen, and the resulting QVDs that contained that column had differing numbers of unique values for that same column. This could be an inconsistency that should be investigated further. . . Priority 4 . “QVD Overlap” is a term for when many column names from two or more QVDs overlap heavily, say 50% or more. Sometimes this is done by design, for example, having a summary and detail version of the QVD can be great for performance, and having segmented QVDs for large datasets is in fact considered a best practices. However, when it is unintended that QVDs greatly overlap each other or are duplicates of each other, it could imply extra reload work, storage bloat, extra maintenance, and could cause confusion among developers, designers, and users. . Navigate to the QVD Overlap sheet. . . In the Source → Target Overlap table on the right, find and select QVDs that have a large percentage of overlap with other QVDs. . . Then individually select the Target QVDs to see the column overlaps between the QVDs. . . Analyze the overlap to determine whether or not it is something to react to. For example, if the two QVDs have a 95% overlap, and they are the same granularity of data, this may be duplicative work and might be confusing to your Qlik Sense developers. It may also result in inconsistencies down the road, if the QVDs get differing logic in how they are loaded. Though, this can occur if new fields are added to QVDs that are written out monthly. Naturally, as new fields become added, they will begin to differ–which is natural. . Tags . #quarterly . #asset_management . #qvds . #qvd_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/qvds/review_optimize_qvds.html",
		"relUrl": "/docs/asset_management/qvds/review_optimize_qvds.html"
	},
	"62": {
		"title": "Review Pinning/Load Balancing",
		"content": "Review Pinning/Load Balancing . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 20 Min | 5 min | . Benefits: . Increase performance | Segregation/isolation of resources | . Goal . The goal of this activity is to review the distribution of apps in an enviroment and implement load balancing or “pinning” if needed. Scenarios where customized load balancing inside of Qlik Sense Enterprise may be warranted include: . Segregation of Published apps from unpublished apps (aka a dedicated ad-hoc development node or nodes) | Segregation of larger applications to a dedicated node or nodes | Segregation of a specific line of business’s apps to a dedicated node or nodes (e.g. isolating the apps in streams for the Sales group to a specific node) | By reading through the exercise on this page, the Qlik administrator is expected to have an understanding of how to leverage this capability and implement as the organization requires. . The output of the Capacity Plan should be a good indicator if this capability is necessary. . This guide will cover two specific scenarios (2 and 3) from above and link to coverage of scenario 1 so that the administrator has a template of a few options should they need to implement customized load balancing for their Qlik site. . Table of Contents . What is Load Balancing in Qlik Sense Enterprise? | Scenario 1 - Isolation of Production from Development | Scenario 2 - Segregation of Larger Qlik Apps Configuration Walk-Through | Validation | . | Scenario 3 - Segregation of apps by line of business (streams) Configuration Walk-Through | Validation | . | . . What is Load Balancing in Qlik Sense Enterprise? . Inside of Qlik Sense Enterprise there is a feature which allows for the administrator to customize the distribution of apps in their site across Qlik Engines. This feature uses Load Balancing Rules to define the conditions under which Qlik apps should be available on a Qlik Engine. The net effect of this feature is that the administrator has the capability to isolate or segregate Qlik apps based on the needs of their site. An example of this can be illustrated like so: . . In this example the administrator has pinned apps 1 &amp; 2 to the Consume 1 node and apps 3-5 on the Consume 2 node. Apps 1-5 are likewise available to both back-end scheduler nodes. . While the scenarios which will be covered in this guide will focus on isolation of apps from the vantage of end users, the same principles can be applied to reload tasks. When a task is started, any node which has a Scheduler and Engine which has an app load balanced to it is eligable to execute this reload. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;Qlik Sense Enterprise has a default and unmodifiable rule named &lt;code class=&quot;language-html highlighter-rouge&quot;&gt;ResourcesOnCentralNode&lt;/code&gt; which means that the Central node will need to have all Qlik apps available on it. This rule cannot be modified due to architectural assumptions in the design of Qlik Sense Enterprise. For example, that apps which need to be migrated will be migrated by the Central node’s Engine. Practically this means that when attempting to segregate / isolate apps using load balancing rules, the Central node’s Engine should not be configured as a Load Balancing Engine for any Virtual Proxy. Otherwise applications may be opened by the Central node’s Engine.&lt;/p&gt; . Scenario 1 - Isolation of Production from Development . In this scenario the administrator wishes to create an ad-hoc development node which only hosts unpublished Qlik apps separate from a production node which hosts published Qlik apps. Refer to this document on Qlik Community for guidance on this scenario. . Scenario 2 - Segregation of Larger Qlik Apps . In this scenario, the Qlik administrator is intending to isolate specific, larger Qlik apps to a dedicated node in the cluster. This scenario can be ideal in situations where a site hosts a few, select large Qlik apps with the remaining apps in the site’s portfolio being of modest size. This isolation allows the site to not attempt to evenly distribute the load for these select apps which will cause duplicative RAM use across the nodes in the cluster. Duplication for smaller apps may be ideal for resiliency purposes but the site may not have the compute capacity to have resiliency for these larger Qlik apps or the administrator would like to ensure a more predictable user experience by re-using the cache on a single Engine (or 2+ for resiliency). . The App Metadata Analyzer can be helpful in determing whether there are abnormally large apps in your Qlik Sense site. . Configuration Walk-Through . Create the custom property NodeType for apps and nodes with the value option of Dedicated. | . . Apply the Dedicated value to the node which host the large Qlik apps. | . . Apply the Dedicated value to the large Qlik app(s) which will be isolated. Repeat this process for all apps which are to be isolated. | . . Hint: The Column Selector can enable NodeType custom property in the apps section of the QMC to be able to filter on all the apps with the Dedicated custom property value assigned. . . In the Load balancing rules section of the QMC, disable the default rule ResourcesOnNonCentralNodes. This default rule load balances all apps which are not in the Monitoring Apps stream to all RIM nodes. If there are other customized load balancing rules which are enabled, consider disabling those if they are not integral to your Qlik site. . | Create a new Load balancing rule with the following values: Name: Dedicated Apps to Dedicated Node | Description: Apps with the NodeType custom property value of Dedicated will be isolated to the node which has the Dedicated custom property value. All non-“Dedicated” apps will available on remaining RIM nodes. | Resource Filter: App_* | Actions: Load balancing | Conditions: ((node.@NodeType=resource.@NodeType) or (node.@NodeType.Empty() and resource.@NodeType.Empty())) | Context: Both in hub and QMC | . | A Preview will display the effect of the load balancing rule. | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;It may be helpful to select the Transpose button to view the resulting preview in a more consumable format.&lt;/p&gt; . . Validation . Ensure that the Virtual Proxy that will be used by the end users includes both the “Dedicated” node(s) as well as the non-dedicated node(s). | . . As a user who would be expected to see one of the apps which have been pinned to the Dedicated Node(s), open the Hub. | . . Open the app: Create a new sheet | Add a KPI Visualization Object with the Measure of =ComputerName() which is used to display the Engine which is opening the Qlik app | . | . . Scenario 3 - Segregation of apps by line of business (streams) . In the previous scenario, the Qlik administrator isolated specific Qlik apps to a dedicated node in a cluster. For this scenario, the Qlik administrator will isolate all apps in specific streams to specific nodes. This use case makes sense when a Qlik site is servicing multiple tenants or departments and the adminstrator wants to isolate the consumption of Qlik apps by those departments / tenants. This scenario will use a custom property at the stream layer rather than at the app layer. . Configuration Walk-Through . Create the custom property StreamLevelNode for streams and nodes with the value options of Operations &amp; Sales. | . . Apply the Operations value to the node which will host that group’s apps. Do the same for the Sales value. | . . Apply the Operations value to the Operations stream and Sales to the Sales stream. | . . In the Load balancing rules section of the QMC, disable the default rule ResourcesOnNonCentralNodes. This default rule load balances all apps which are not in the Monitoring Apps stream to all RIM nodes. If there are other customized load balancing rules which are enabled, consider disabling those if they are not integral to your Qlik site. . | Create a new Load balancing rule with the following values: Name: Dedicated Apps to Dedicated Node | Description: Apps who are members of streams with a given StreamLevelNode custom property value will be load balanced to nodes with that same custom property value. | Resource Filter: App_* | Actions: Load balancing | Conditions: ((node.@StreamLevelNode=resource.stream.@StreamLevelNode) or (node.@StreamLevelNode.Empty() and resource.@StreamLevelNode.Empty())) | Context: Both in hub and QMC | . | A Preview will display the effect of the load balancing rule. | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;It may be helpful to select the Transpose button to view the resulting preview in a more consumable format.&lt;/p&gt; . . Validation . Ensure that the Virtual Proxy that will be used by the end users includes all RIM nodes. | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;In this example, apps are only load balanced across two RIM nodes. In another, more robust environment, the administrator will likely want to include at least one additional Engine in order to ensure availability of apps which are not members of the streams which have been isolated.&lt;/p&gt; . . As a user who would be expected to see one of the streams who apps are pinned to an isolated node, open the Hub. | . . Open the app: Create a new sheet | Add a KPI Visualization Object with the Measure of =ComputerName() which is used to display the Engine which is opening the Qlik app | . | . . Tags . #quarterly . #asset_management . #apps . #load_balancing . #app_metadata_analyzer .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/apps/review_pinning_load_balancing.html",
		"relUrl": "/docs/asset_management/apps/review_pinning_load_balancing.html"
	},
	"63": {
		"title": "Review/Update Capacity Plan",
		"content": "Review/Update Capacity Plan . Cadence Quarterly . Sites production .   Initial Recurring . Estimated Time | 1/2 day | 2 hrs | . Benefits: . Plan for growth | Anticipate user/architectural events | . . Goal . This page is intended to act as an example of what a high-level capacity plan could look like. It is assumed that the organization would build one themselves with some of the below considerations in mind, or would work with Qlik’s Services organization to have one defined/executed. . It’s important for stakeholders, budgetholders, and Qlik deployment owners to have advance notice when additional resources will be needed. This exercise helps an administrator prepare for those requests and demonstrate the need. . Specific areas: . Document current state and expected state of several asset groups, which helps for planning. | Document and justify the actions that are needed for capacity/architecture changes. | . . Capacity Planning Process . There are four primary pillars that this process covers–review each process below for details: . Licenses . | Users . | System . | Applications . | . . Capacity Planning Example . The below is a high-level mockup of what a capacity plan’s output could look like, including the four points from the Capacity Planning Process. For details on how to locate/calculate these metrics, please refer to the associated process item above. . ACME Corp . Licenses .   Licenses Licenses Allocated Licenses Allocated Unused Licenses Remaining . Professional | 50 | 44 | 2 | 6 | . Analyzer | 200 | 180 | 15 | 20 | . Actions . Analyze the allocated licenses for possible re-assignment. | Review unused licenses. | Notify appropriate stakeholders that additional licenses will be needed in the near future. | Users . Peak Concurrency Total Users Active Users 1+ Sessions Active Users 5+ Sessions . 43 | 224 | 207 | 176 | . * Activity based off last 3 months, assuming quarterly planning. . Actions . Review system specs to see how it is performing with the above currently, and how it could scale. | System . Engine CPU Engine RAM Batch Window Avg Intra-day Reloads per Day . Good | Good | Good | 354 | .   Max Concurrent Users Per Engine . Engine 1 | 43 | . Engine 2 | 40 | .   Intra-day Reloads per Engine End-User Facing . Engine 1 | 386 | Yes | . Engine 2 | 214 | Yes | . Actions . Begin offloading intra-day reloads to an isolated scheduler. | Application . Candidates for “App Pinning” Candidates for Data Model Optimization ODAG Apps Qlik NPrinting Apps Qlik InsightBot Apps . 2 | 3 | 1 | 0 | 0 | . Actions . Review three applications for optimization and two applications for app pinning. . | Review Architecture/Scale Plan to see if app pinning is possible with the current architectural footprint, or if it would require an architectural event, e.g. horizontally scaling (adding another proxy/engine node). . | Review where ODAG reloads are being processed, and if they need to be offloaded to a dedicated scheduler if not already, or if more cores are required for additional concurrent reloads. . | If NPrinting or InsightBot are in use, validate that there are dedicated applications for each of these components that are silo’ed off from end users. I.e., there is a duplicated, stripped down version of a production app for use with either NPrinting or InsightBot. . | Tags . #quarterly . #system_planning . #capacity_plan .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_update_capacity_plan.html",
		"relUrl": "/docs/system_planning/review_update_capacity_plan.html"
	},
	"64": {
		"title": "Security Rule Analyzer",
		"content": "Security Rule Analyzer . developmentproduction . Estimated Configuration Time | 30 min | . Table of Contents . About | Screenshot | Where to get it | . About . The Security Rule Analyzer is intended to be a companion application to the Analyze Security Rules section. . The application is an application supported by the Americas Enterprise Architecture team from Qlik. It is a very straight forward application that makes to calls to the QRS (repository database) that fetches metadata around custom properties and all security rule information. The application itself takes advantage of the existing monitor_apps_REST_app data connection, so there is no installer and it is plug and play, spare a couple of variable settings and ensuring that the user executing the reload has RootAdmin rights and access to the data connection. Complete setup instructions can be found in the script. . Screenshot . . Where to get it . The application can be found on GitHub here. . Tags . #security . #rule . #security_rule . #analyzer . &amp;nbsp .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/security_rule_analyzer.html",
		"relUrl": "/docs/tooling/security_rule_analyzer.html"
	},
	"65": {
		"title": "Security Rules",
		"content": "Security Rules .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/security_rules.html",
		"relUrl": "/docs/asset_management/security_rules.html"
	},
	"66": {
		"title": "Streams",
		"content": "Streams .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/streams.html",
		"relUrl": "/docs/asset_management/streams.html"
	},
	"67": {
		"title": "System",
		"content": "Capacity Plan: System . Goal . The goal of this exercise is to identify any potential system concerns across a production site. The Operations Monitor application exposes this information simply, so that it can be easily referenced. . There are a number of metrics that should be focused on, including the following: . Engine CPU | Engine RAM | Users per Engine | Intra-day Reloads | Batch Window | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;Note that more important than any of the above is the overall end-user experience. If the end-users are complaining about performance issues, even though the above metrics look to be good, it could likely be due to the application itself. Refer to: &lt;a href=&quot;/docs/system_planning/review_update_capacity_plan/applications.html&quot;&gt;Applications&lt;/a&gt;.&lt;/p&gt; . Table of Contents . Operations Monitor Confirm Operations Monitor is Operational | . | Gather Metrics CPU | RAM | Max Concurrent Users per Engine | Intra-day Reloads | Batch Window | . | Example Takeaway | . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . Gather Metrics . Select the Performance sheet of the Operations Monitor. . . Select the Month field and select the last three months, assuming this exercise is being executed quarterly. . . CPU . Now, focus on the Qlik Sense Engine CPU chart for a moment. Ensure that the measure is set to Max CPU. Note that this chart defaults to viewing at the Month, however it can drill down to the day, hour, and ten-minute timeline. It is advised to look for extended durations of high CPU utilization, and to see if those events are recurring. In the below chart, we can see that these servers are not heavily utilized, as the maximum CPU does not go above 32%. . . RAM . Next, focuse on the Qlik Sense Engine RAM (GB) chart. This chart is of the same construct as the prior, except focusing on RAM utilization. It is important to note that this chart is showing not only the “base RAM footprint” of the applications, but also the “result set cache RAM”, as well as RAM for calculations, etc. Take the time to review the chart below and look for extended periods of time of very high RAM utilization, say around 90%, where the server is continuously fighting to clear cache to make room for new result sets. It should be relatively clear if the server is over-taxed if it is flat-lining consistently. The servers in the chart below look healthy, however. . . As there is no simple way here to show what applications are consuming what percentage of that RAM total, a way to draw a line in the sand could be the following best practice: . *The total base RAM footprint of all applications available to be lifted onto the engine should be &lt;= 40% of the total RAM of the server. | . * Of course, this is a best practice from keeping a server from going off the rails, but it is not guaranteed. If there were thousands of users hitting that box at the same time, they would more than likely consume more than the available amount of RAM left over–but in general, it is a good practice if users have been properly distributed across engines. . To gather this metric, we need to leverage the App Metadata Analyzer. Confirm that it is setup, and then navigate to the App Availability sheet. . . In the Engine Node: Available Apps &amp; Base RAM Footprint table, one can view the total base RAM footprint for all applications that are able to be lifted on each individual engine node. . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;This practice should also be repeated with selecting only the highly-used applications, as it is highly unlikely that &lt;em&gt;all&lt;/em&gt; applications will be open in RAM at the same time in production, unless there are only a few applications that are deployed. It is suggested to select only highly used applications, which can be found in the &lt;strong&gt;Operations Monitor&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Session Overview&lt;/strong&gt; sheet -&amp;gt; &lt;strong&gt;Top 50 Apps&lt;/strong&gt; chart.&lt;/p&gt; . . With no specific applications selected: . . Here, it is visible that all apps are available on all engine nodes–meaning there are no custom load balancing rules in place. The total application RAM footprint is 155 GB, and in this case, the servers have a total of 512 GB of RAM. This puts the total base RAM footprint at 30% of the total server RAM–which is below that 40% line from the best practice above–so all is well for the time being. . Max Concurrent Users per Engine . It is also valuable to view the distribution of users across the engines, as this metric is important for creating a custom gauge to help establish a breaking point. For example, if the engine starts performing poorly with 50 concurrent users on it (assuming uniform applications available across all engines), that number can then be used as a compelling event to expand horizontally to another engine node if breached. An admin can then review this metric quarterly to warrant if there needs to be an architectural event to keep this number below the set threshold. . * This is of course tackling the issue of performance by adding more hardware, while one will also want to consider simultaneously optimizing their applications. There is no silver bullet. . To create this chart, open up the Operations Monitor and create a new sheet. Name it Max Concurrent Users by Hostname. . . Drag and drop a Pivot Table, then add the Hostname field as a dimension. . . Next, add the Max Concurrent Users metric as the measure. . . Name the chart Max Concurrent Users by Hostname. Following, review the total concurrent users per engine node. . . Intra-day Reloads . The primary goal of this sub-section is to report on the number of intra-day reloads so that they can be examined. The result should be to minimize if not completely decouple intra-day reloads from end-user facing engine nodes. If the environment is smaller and reloads must operate on end-user nodes, that should at a minimum be pushed to the overnight batch window where viable. If there are intra-day reloads for hourly reloading apps or otherwise, ideally, they should be offloaded to a separate scheduler. . While remaining in the Operations Monitor, navigate to the Task Overview sheet. . . Select the last three months (assuming quarterly review), and then select any hours that would be considered as “business hours”. . In this example, it is visible that there are 354 reloads per day within business hours–the majority of which are from several applications that reload on a frequent basis. . . Now, if all of these reloads are happening on dedicated scheduler nodes and completing successfully without overloading the server(s), then all is well. To view where the reloads are happening, a new chart can be created. . Duplicate the sheet, and copy the Reload Count bar chart. . . Make room for the new chart, and paste it. . . Delete the Reload Status dimension from the new bar chart. . . Add the Hostname field. . . Drag the Hostname field up, to be above the Task Name field. . . Review the new chart to see where reloads are occurring. . . Select each node, and confirm whether they are scheduler nodes or end-user facing nodes. | Record the intra-day reloads for each. | . * If there is a “sandbox” node in the production environment, this node can be ignored from this review, as that node will have hub-based reloads. . Batch Window . In general, this section represents the capacity of the site’s batch window (which is predominantly nightly). The steps to optimize the batch window can be found here: Optimize Batch Window. . After reading the optimization process, one can apply the following rules: . Good There is plenty of room for more reloads in the batch window. Apps are available with new data prior to any morning rush, and reloads are consistent in speed from day to day. | . | OK There is some congestion of the batch window, and there is not a lot of room to add many more tasks. Some variation of reload speed may be occurring, but not much. | . | Bad Fully congested batch window–no room for any additions. Tasks are at risk of or are running over into business hours. | . | . Example Takeaway . Referring to the examples above (obviously this is a rarely used testing environment), tables that could be used for capacity planning could look like the following: . Engine CPU Engine RAM Batch Window Avg Intra-day Reloads per Day . Good | Good | Good | 354 | .   Max Concurrent Users Per Engine . Engine 1 | 1 | . Engine 2 | 2 | . Engine 3 | 1 | .   Intra-day Reloads per Engine End-User Facing . Engine 1 | 386 | No | . Engine 2 | 214 | Yes | . Engine 3 | 0 | Yes | . Tags . #capacity_plan . #system . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_update_capacity_plan/system.html",
		"relUrl": "/docs/system_planning/review_update_capacity_plan/system.html"
	},
	"68": {
		"title": "System Planning",
		"content": "System Planning .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning",
		"relUrl": "/docs/system_planning"
	},
	"69": {
		"title": "System Spot Check",
		"content": "System Spot Check .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_spot_check",
		"relUrl": "/docs/system_spot_check"
	},
	"70": {
		"title": "Review Tasks",
		"content": "Spot-Check: Tasks . Cadence Daily . Sites production .   Initial Recurring . Estimated Time | 3 Min | 3 min | . Benefits: . Improve app availability | Increase awareness | . . Goal . The goal for this spot-check is to be aware of any task failures which have occurred in the past 24 hours. Unexpected task failures should be restarted insofar as this is sufficient resources are available in the environment to perform intra-day reloads. For example, bulk queries against a Production database during the business day may be against policy / best practices for a given organization. While this administrative task is not intended to be a deep dive, investigation into the causes of task failures for (a) critical tasks / task chains and/or (b) repeated failures is encouraged. . Table of Contents . QMC - Tasks | Task Failure Notifications | . . QMC - Tasks . In the QMC, select Tasks: . . For smaller deployments, a simple sorting of the Last execution column can be done to focus on recently executed tasks: . . For deployments with a great number of reload tasks, further filtering can be done using the filter option in the Last execution column to provide greater specificity: . . To do a cursory exploration of the task failure, select the i icon to bring up an informational modal for the reload task: . . Reload tasks have two primary components: (a) : Initiation of the reload task from the Qlik Repository Service to the Qlik Scheduler Service which determines an available Qlik Engine to execute the reload task. | (b) : Execution of the application’s load script by the Qlik Engine | . | . From the above screen-shot, the presence of the Download script log (step 3) indicates that the flow has successfully reached step (b). Therefore, the failure was due to the logic specified in the application’s load script. If step 3 is not available, this typically signals that the process failed on step (a). Further investigation of the trace in the informational modal will be needed. If no obvious clues are present, a deeper investigation of the Qlik Sense logs would be required. . Once the administrator is satisfied at a cursory understanding of what reload tasks have failed recently as well as a proximate root cause, they can proceed to the action. At this point, a general understanding of the Qlik deployment will be required. For many environments, restarting the task is acceptable. But as outlined in the Goal section, some environments may be resource constrained on the Qlik server side or the data source side such that an intra-day reload is not recommended. The administrator is encouraged to use their best judgment on the appropriate step. . Task Failure Notifications . Rather than checking for task failures in the QMC or finding out from developers/end-users, one can explore automated notifications. . There are many ways to achieve this: . Qlik Alerting (formerly known as Ping Alerting): the suggested solution, as it is built for this purpose . | Qlik NPrinting : allows for pixel perfect and highly customized emails . | SMTP Appender (more information here): Plain text emails; limited customizability. . | . More info to come in this section with examples. . Tags . #daily . #spot_check . #tasks .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_spot_check/tasks.html",
		"relUrl": "/docs/system_spot_check/tasks.html"
	},
	"71": {
		"title": "Tasks",
		"content": "Tasks .",
		"url": "https://adminplaybook.qlik-poc.com/docs/asset_management/tasks.html",
		"relUrl": "/docs/asset_management/tasks.html"
	},
	"72": {
		"title": "Review Expensive Objects/Apps (Telemetry)",
		"content": "Review Expensive Objects (Telemetry) . Cadence Daily . Sites production .   Initial Recurring . Estimated Time | 35 min | 8 min | . Benefits: . Optimize objects | Increase performance | Control thresholds | . . Goal . The goal for this spot-check is use the Qlik Sense Telemetry Dashboard to look for expensive objects. This process is ideal to be generally aware of the performance of a Qlik Sense Enterprise deployment. With a narrow set of items to focus on, action can be taken to improve performance. . Table of Contents . Prerequisite | Telemetry Dashboard - Details | . . Prerequisite . Telemetry Dashboard | . The Qlik Sense Telemetry Dashboard is a project created and maintained by the Enterprise Architecture team in Qlik’s Americas Presales organization. It leverages additional logging which can be enabled in Qlik Sense Enterprise February 2018 and newer releases of Qlik Sense with some data enrichment with API calls to provide visualization-level performance data for Qlik apps. The goal of the tool is to present this performance data in actionable ways for administrators / developers to have precision when focusing on performance improvements for Qlik apps. . . Telemetry Dashboard - Details . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;What is considered an unacceptable threshold will vary by environment. Generally visualizations which take over 60 seconds to render or apps which take over 2 minutes to open are highly undesirable from an end-user experience perspective. These are good baselines for qualifying problematic behavior in an environment. As optimizations are applied, tackling marginally undesirable response times can be undertaken.&lt;/p&gt; . Open the Qlik Sense Telemetry Dashboard application and navigate to the Details sheet. Inside this sheet, drill to the most recent day (since this task is a daily task), select the relevant Request Filters, and drill to the Process Time values which broach the threshold that you are interested in. Example: . . For this example, we are interested in Visualization objects, App Open Events, and Reloads which have taken more than 30 seconds. . The primary Engine calls which are relevant for most users of the Telemetry Dashboard are: OpenApp : The request to the Engine to open an application. This is the time it takes for the Engine to request the application binary from disk, load it over the network, and un-compress it to RAM. | Reloads : How long a reload execution takes. This type of request is less frequently relevant to most organizations. At least for the scope of this task. | Visualizations : The request to the Engine to render a visualization, e.g. a table chart. | . | A more exhaustive write-up of the Telemetry Dashboard is found here | . With a narrowed set of interactions, action plans can be constructed. For example: . App Open : If the threshold is unacceptable given the important of the application, then a cache warming process may be appropriate. Reference our article on Cache Warming here for more specific guidance on techniques. | Visualization : Use the data from the Telemetry Dashboard like App, Sheet and Object Type + ObjectID to reach out to the application’s owner and/or maintainer in order to focus optimization efforts. Since the Qlik Sense Telemetry Dashboard does not imply a specific fix, the developer will need to review the application to see what improvements can be made. Potential avenues for fixes: Table / Pivot Table : Use calculation conditions to force drill downs. | Other visualizations : Review expressions used. Simplify where possible and/or off-load the calculations to the application’s load script to pre-calculate. The Diagnostic Toolkit project can be used for reference. | . | . Tags . #daily . #spot_check . #telemetry . #telemetry_dashboard .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_spot_check/telemetry.html",
		"relUrl": "/docs/system_spot_check/telemetry.html"
	},
	"73": {
		"title": "Telemetry Dashboard",
		"content": "Telemetry Dashboard . developmentproduction . Estimated Configuration Time | 30 min | . Table of Contents . About What is Telemetry in Qlik Sense Enterprise? | Overview of the project | Considerations on setting thresholds | Using the app for analysis | . | Documentation: | . About . The Qlik Sense Telemetry Dashboard is a project created and maintained by the Enterprise Architecture team in Qlik’s Americas Presales organization. It leverages additional logging which can be enabled in Qlik Sense Enterprise February 2018 and newer releases of Qlik Sense with some data enrichment with API calls to provide visualization-level performance data for Qlik apps. The goal of the tool is to present this performance data in actionable ways for administrators / developers to have precision when focusing on performance improvements for Qlik apps. . What is Telemetry in Qlik Sense Enterprise? . Telemetry in Qlik Sense Enterprise is essentially: . a threshold based logging. for | Qlik Engine requests | (1) makes Telemetry different than traditional application logs. ERRORs or WARNs in Telemetry are based on the thresholds the administrator sets. . (2) refers to the fact that the Qlik Engine treats requests differently than what a user might expect. Engine requests can refer to obvious things like opening an app (OpenApp method) or reloading an app (DoReload or DoReloadEx methods). Those are straight-forward. Less intuitive is that when inside of a Qlik app, each visualization on a sheet is a separate request to the Engine. In this example we are high-lighting the 8 different requests (GetLayout method) which are made when opening our example sheet: . . When a request breaches the configured threshold, the individual request will be logged. In the example above, if the filter pane in the upper left hand corner breaches the threshold, the log file will record the request for just that object (e.g. a GetLayout call for object NpbzKm). . The key pieces of information that are logged when a request breaches the threshold that are used in the associated Qlik are include: . Field Explanation . Timestamp | Time of the breached threshold | . Level | Level of the breach (ERROR vs. WARN) | . ActiveUserId | UserId of the user who was making the request | . ActiveUserDirectory | UserDirectory of the user who was making the request | . Method | Engine API method of the request(e.g. GetLayout, OpenApp, DoReload) | . DocId | AppID where the request occurred | . ObjectId | ObjectId of the object requested | . ObjectType | The type of object requested, if relevant | . ProcessTime | How long did this request take to be completed? | . NetRAM | How much RAM did the request take | . This is the extent of the product level changes which were first implemented in Qlik Sense Enterprise February 2018. . In order to increase the consumability and provide context for these logs, the Enterprise Architecture Team has created a project to surface this data in context and in a Qlik app. . Overview of the project . The Qlik Sense Telemetry Dashboard project provides an installer which installs a couple of things which will be used in presenting the data in a consumable fashion: . It installs NodeJS scripts which build out meta-data about apps | It configures data connections used by the Qlik app | It imports the Qlik App | It configures chained tasks which (a) rebuild the meta-data and (b) reload the Qlik app | (1) is needed to build out the hierarchical relationship between visualization objects, the sheets in which the visualizations live, and the apps which hold those sheets. . . This hierarchy allows for the consumer of the Telemetry Dashboard Qlik app to place the request in context. Moreover, with this context, specific feedback can be made to the developer(s) of this app to focus their optimization efforts on the offending visualization(s) rather than providing much more generic feedback that ‘the app is slow’. . Since the project needs these NodeJS scripts to build the hierarchy of app &lt;&gt; sheets &lt;&gt; visualizations, the installer configures an External Program Task which runs the NodeJS scripts as a Qlik task. This External Program Task is chained so that its successful execution reloads the Qlik app which depends on output from the External Program Task. . Considerations on setting thresholds . As covered above, this feature in Qlik Sense Enterprise requires the administrator to set the thresholds which register as ERRORs or WARNs. Making the configuration change is covered on the project’s wiki. We will cover what values are appropriate to be set. A common set of configs are: . ErrorPeakMemory=2147483648 WarningPeakMemory=1073741824 ErrorProcessTimeMs=60000 WarningProcessTimeMs=30000 . This means that any request which uses over 2GB (2147483648/1024/1024/1024) will be logged as an ERROR in the log. Requests over 1GB will be logged as a WARN. Likewise, any request which takes over 60s (60000/1000) will be logged an ERROR and those taking over 30s will be logged as a WARN. . These settings are generally appropriate for most environments. . As a rule, the Error and Warning values for PeakMemory and ProcessTimeMs should have enough of a gap to cover a steady increase of the underlying values. For example, using ProcessTimeMs, setting the Error value to 60s and Warning value to 59 seconds would not provide enough delta between Error and Warning to determine whether a particular Error event was due to a steady increase of the underlying data (as is expected for many Qlik apps) or due to a one-off event. Providing a healthy gap between the Warning and Error values will allow the Qlik administrator and the BI Developers a directional sense of changes over time. . For Production environments, the Warning thresholds, especially the ProcessTimeMs value, should be set at a level where the response time is slow enough to at least register. The Error thresholds should be set at a level where relatively quick attention by the administrator and/or developers is needed. . Using the app for analysis . Once the Telemetry logging is configured, the server has run for a few days to build out a historical record of the requests that breached the configured thresholds, and the Telemetry Dashboard has been installed + reloaded, the administrator and/or developers should review the app for anomalies. . &lt;more stuff to be added later&gt; . Documentation: . Qlik Sense Telemetry Dashboard Project | Youtube Video - Initial Configuration | Youtube Video - Installation | Youtube Video - App Config | Youtube Video - Analysis | . Tags . #tooling . #telemetry .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling/telemetry_dashboard.html",
		"relUrl": "/docs/tooling/telemetry_dashboard.html"
	},
	"74": {
		"title": "Tooling Appendix",
		"content": "Tooling Appendix . This section is intended to provide a high-level overview of the tools that the Playbook references. The Playbook does not house exhaustive documentation/instructions for each tool, but will provide an outline of what the tool does and direct to the relevant docs/downloads. . . Name Description Category Distribution Method Format Source Supported By . License Monitor | Qlik app to monitor license usage | Asset Management | Native - Installed | QVF | N/A, Installed | Qlik | . Operations Monitor | Qlik app to monitor performance, reloads, sessions, and more | Asset Management | Native - Installed | QVF | N/A, Installed | Qlik | . App Metadata Analyzer | Qlik app to inspect data model metadata | Asset Management | Native - Hidden | QVF | C: ProgramData Qlik  Sense Repository DefaultApps | Qlik | . Reload Monitor | Qlik app to monitor reloads | Asset Management | Native - Hidden | QVF | C: ProgramData Qlik  Sense Repository DefaultApps | Qlik | . Data Connection Analyzer | Qlik app to map apps &lt;&gt; data connections | Asset Management | 3rd Party | QVF | Github | Americas Enterprise Architecture Team, Qlik | . QVD Monitor | Qlik app to monitor QVDs | Asset Management | 3rd Party | QVF | Github | Americas Enterprise Architecture Team, Qlik | . Security Rules Analyzer | Qlik app to flag non-performant security rules | Asset Management | 3rd Party | QVF | Github | Americas Enterprise Architecture Team, Qlik | . Telemetry Dashboard | Qlik app to monitor granular performance metrics in Qlik apps | Performance | 3rd Party | QVF + Application | Github | Americas Enterprise Architecture Team, Qlik | . Extension Usage Dashboard | Qlik app to map apps &lt;&gt; extensions | Asset Management | 3rd Party | QVF + Application | Github | Americas Enterprise Architecture Team, Qlik | . Qlik CLI for Windows | PowerShell library to automation | Automation | 3rd Party | PowerShell | Github | Adam Haydon, Qlik | . Cache Warming | Programmatic tools to cache Qlik apps | Performance | 3rd Party | Application | Various (see article) | Various | .",
		"url": "https://adminplaybook.qlik-poc.com/docs/tooling_appendix",
		"relUrl": "/docs/tooling_appendix"
	},
	"75": {
		"title": "Users",
		"content": "Capacity Plan: Users . Goal . The goal of this exercise is to identify user activity over a period of time since this exercise was last ran to identify variances in usage and anticipate future growth. . There are a number of metrics that should be focused on, including the following: . Peak Concurrency | Active Users 1+ Sessions | Active Users 5+ Sessions | . Table of Contents . Operations Monitor Confirm Operations Monitor is Operational | . | User Activity Gather Metrics | . | Example Takeaway | . . Operations Monitor . This page leverages the Operations Monitor. Please refer to the Operations Monitor page for an overview and relevant documentation links. . Confirm Operations Monitor is Operational . Navigate to the Monitoring apps and select the Details button (info icon) on the Operations Monitor application. Confirm that the application’s data is up-to-date. . . If the Operations Monitor is not up-to-date, please refer to the Operations Monitor Documentation for configuration details and troubleshooting steps. . . User Activity . Gather Metrics . Select the Session Overview sheet from the Operations Monitor. . . Now, assuming this capacity plan exercise is in fact being executed quarterly, select the last three months of session data. This can easily be achieved by selecting the target months in the User and App Count Trend chart in the bottom right. The Latest Activity Measure field that always has one selected does not apply to this area, and only applies to sheet usage–so it can be ignored. . . While remaining on this sheet, take note of the Max Concurrent Users KPI. This is the Peak Concurrency KPI mentioned above, and is critical to help plan for growth from an architecture perspective. . . Following, select the Session Details sheet. . . Next, be sure to review the App Session Summary object as well as the User Session Summary objects as they display valuable information, such as how many individual sessions users have had, and against what applications. These are very useful metrics both for overall user usage and adoption, and will impact the Applications section of the capacity plan. . . That being said, two additional metrics that are not available by default on the sheet should be added: . The total number of distinct users that have had at least 1 session over the last x days | The total number of distinct users that have had 5 or more sessions over the last x days | . These two metrics will help to identify and bucket how many active and semi-active users exist in the environment, and then will help with a license optimization exercise, which can be found here: Optimize License Allocations. . To add these two metrics, start by duplicating the sheet. . . Following, find space for two KPI objects. For example, shrink down the User Session Summary table, and insert two KPI objects above it. . . Next, select the first KPI and add the measure: . Count({&lt;[User Name]={&quot;=Sum([Session Count])&gt;0&quot;}&gt;}DISTINCT [User Name]) . . . Name this KPI: Active Users: 1+ Sessions . Next, repeat the process above for adding the following expression to the second KPI: . Count({&lt;[User Name]={&quot;=Sum([Session Count])&gt;=5&quot;}&gt;}DISTINCT [User Name]) . Name this KPI: Active Users: 5+ Sessions . . Example Takeaway . An example output from this site could looks like the following: . Peak Concurrency Total Users Active Users 1+ Sessions Active Users 5+ Sessions . 2 | 10 | 7 | 4 | . &lt;p&gt;&lt;i class=&quot;fas fa-exclamation-circle fa-sm&quot;&gt;&lt;/i&gt; Note&lt;/p&gt; . &lt;p&gt;The &lt;em&gt;Licenses Allocated Unused&lt;/em&gt; metric found &lt;a href=&quot;/docs/system_planning/review_update_capacity_plan/licenses.html&quot;&gt;here&lt;/a&gt; is calculated on 30 days, not 90, as the above chart shows. Take this into consideration in the capacity plan.&lt;/p&gt; . Tags . #capacity_plan . #users . #operations_monitor .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/system_planning/review_update_capacity_plan/users.html",
		"relUrl": "/docs/system_planning/review_update_capacity_plan/users.html"
	},
	"76": {
		"title": "Verify/Execute Backups",
		"content": "Verify/Execute Backups . Cadence Monthly . Sites all .   Initial Recurring . Estimated Time | 2 hrs | 1 hr | . Benefits: . Demonstrate Preparedness | Increase fault tolerance | . . Goal . One has backups, right? The goal for this activity is for the administrator to verify that appropriate backups of their Qlik Sense Enterprise deployment have been made. This allows the administrator to increase the organization’s fault tolerance, demonstrate preparedness, and harden out processes around backups before they are needed. . Table of Contents . What do I backup? Persistent Content | PostgreSQL | Certificates | Ancillary content | . | How do I backup? Persistent Content | PostgreSQL | Certificates | Ancillary content | Full backups | . | How do I verify that backups are being done? | Documentation | . What do I backup? . With Qlik Sense Enterprise on Windows there are three core components (and an additional one) which need to be backed up in order to maintain a minimally functioning site: . The persistent content used by the site | The site wide metadata stored in PostgreSQL | The certificates used by the Qlik site for internal communication | Ancillary content | Persistent Content . The persistent content used by the site includes things like apps, extensions, and images used in application thumbnails. These are the core elements of a Qlik site. In a disaster, an administrator can use these flattened assets to rebuild a Qlik site from scratch. These pieces of content are stored in the Service Cluster for the site. To confirm this location in the QMC go to the Service Cluster section. . . Note the Root folder. . . PostgreSQL . Beneath every Qlik Sense Enterprise on Windows site is a series of PostgreSQL databases which store site-wide metadata. This metadata contains ownership of assets, hierarchies, authentication settings, security rules, licenses, and so on. Think the information with is displayed in the QMC. The combination of this metadata and the persistent content allow for a more seamless restoration process. To take the example of Qlik apps, the structure of the physical file on disk is flattened. In a published Qlik app, there may be sheets which are part of the base app, sheets published to the community for consumption across the user base, and private sheets created by individuals for their personal analyses. The physical file makes no distinction between these states of sheets, so if an administrator were to import the file from disk they would get a flattened list of all the sheets no matter their status. . Certificates . When Qlik Sense Enterprise was designed, security against eavesdropping was built in by using certificates which Qlik generates to encrypt all traffic between services. This communication is encrypted no matter if the communication is self-contained to a single server or distributed across multiple nodes in a cluster. These certificates likewise are used to encrypt sensitive data which is stored in PostgreSQL (e.g. passwords to data connections). Due to this, the original certificates in a Qlik site are key seamless recovery. Without the availabity of these certificates the site can be recovered but at a consequence of needing to re-enter passwords, re-distribute certificates to other members of the Qlik Sense cluster, as well as replacement of certificates used in integration points like Web Ticketing using the Qlik Proxy API, automation tasks using the Qlik Repository API, or visualization integrations using the Qlik Engine API. . Ancillary content . The previous items are universal to every Qlik Sense Enterprise site. This fourth item is customized per deployment. When we say ancillary content, we refer to all the non Qlik stuff which is important to a Qlik site. These can include but as not limited to: . Third party certificates attached to the Qlik Sense Proxy Service to enable trusted HTTPS communication | Third party drivers installed to the Windows Operating System to connect to databases without bundled connectivity options | Server Side Extension modules (e.g. Python, R) which run on the Qlik server | Automation code which is running on the Qlik server (for example like a back-up process) | . How do I backup? . Persistent Content . Since these files are just files on a file system, it is ideal to use existing mechanisms for backing up files. This can be snapshots on a VM level, backup processes on storage devices (e.g. NAS, SAN) if those devices are used to host the Qlik share. . The important component of this backup process is that it needs to be made in synchronization with the backup of the PostgreSQL meta-data. This is ideal for Qlik Sense services to be stopped during this backup processes. This ensures that the state on disk is exactly the state in the PostgreSQL database. Think ensuring that no user has created a new sheet. In a pinch an asynchronous backup is acceptable but there may be data loss. . If an administrator needs to manually backup these files we will cover these in a later section where we review a complete backup solution. . PostgreSQL . In order to backup PostgreSQL, use the pg_dump PostgreSQL process. This can be illustrated in this example PowerShell script: . Script . # Define the backup directory $backupDir = &#39;C: QSR&#39; # Define the PostgreSQL password $pwd = &#39;MySuperSecretPassword&#39; # Pass the password to an environmental variable for PostgreSQL $env:PGPASSWORD = $pwd # Determine the installation path $installPath = (Get-ChildItem HKLM: SOFTWARE Microsoft Windows CurrentVersion Uninstall | % { Get-ItemProperty $_.PsPath } | Select DisplayName,InstallLocation | Where-Object {$_.DisplayName -eq &#39;Qlik Sense Repository&#39;}).InstallLocation # Create backup directory if needed if (Test-Path $backupDir) { } else { New-Item -ItemType directory -Path $backupDir | Out-Null } # GOTO installation directory&#39;s PostgreSQL subdirectory Set-Location $installPath Repository PostgreSQL * bin # Execute the backup . pg_dump.exe -h localhost -p 4432 -U postgres -b -F t -f &quot;$($backupDir) $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-QSR.tar&quot; QSR . pg_dump.exe -h localhost -p 4432 -U postgres -b -F t -f &quot;$($backupDir) $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-Licenses.tar&quot; Licenses . pg_dump.exe -h localhost -p 4432 -U postgres -b -F t -f &quot;$($backupDir) $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-SenseServices.tar&quot; SenseServices . pg_dump.exe -h localhost -p 4432 -U postgres -b -F t -f &quot;$($backupDir) $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-QLogs.tar&quot; QLogs . pg_dump.exe -h localhost -p 4432 -U postgres -b -F t -f &quot;$($backupDir) $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-QSMQ.tar&quot; QSMQ # Remove the environmental variable for PostgreSQL password Remove-Item Env: PGPASSWORD . As a note for this script, it is backing up all the databases used by Qlik Sense Enterprise as of February 2020 including the optional Centralized Logging database. Please reference documentation of the version of Qlik Sense Enterprise that is running to ensure full coverage for the databases used. The ideal page for this list is Qlik’s page on installing and configuring PostgreSQL which outlines the databases which are needed to be present. . Certificates . There are three certificates on a Qlik site: . a Root Certificate | a Server Certificate | a Client certificate | (1) is used to generate (2) and (3). Each component uses either (2) or (3) depending on whether it is the client or server in the exchange. (2) likewise is bound to the API ports which can be integrated into authentication modules (i.e. using the QPS API) or automation processes (i.e. using the QRS API). It is important to keep a backup of these certificates in case of recovery. Strictly speaking there isn’t a need for incremental backups of these certificates but it is wise to integrate a backup process nonetheless. An example of how to automate this process is here: . Script . # Define the backup directory $backupDir = &#39;C: QSR&#39; # Define the certificate password $pwd = &#39;MySuperSecretPassword&#39; # Create backup directory if needed if (Test-Path $backupDir) { } else { New-Item -ItemType directory -Path $backupDir | Out-Null } # Get Root Certificate Thumbprint $store = Get-Item &quot;cert: LocalMachine Root&quot;; $store.Open(&quot;ReadOnly&quot;); $certs = $store.Certificates.Find(&quot;FindByExtension&quot;, &quot;1.3.6.1.5.5.7.13.3&quot;, $false); $rootThumb = $certs.Thumbprint # Get Server Certificate Thumbprint $store = Get-Item &quot;cert: LocalMachine My&quot;; $store.Open(&quot;ReadOnly&quot;); $certs = $store.Certificates.Find(&quot;FindByExtension&quot;, &quot;1.3.6.1.5.5.7.13.3&quot;, $false); $serverThumb = $certs.Thumbprint # Get Client Certificate Thumbprint $store = Get-Item &quot;cert: CurrentUser My&quot;; $store.Open(&quot;ReadOnly&quot;); $certs = $store.Certificates.Find(&quot;FindByExtension&quot;, &quot;1.3.6.1.5.5.7.13.3&quot;, $false); $clientThumb = $certs.Thumbprint # Export the certificates to the backupDir $null = certutil -f -p $pwd -exportpfx -privatekey Root $rootThumb &quot;$backupDir $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-root.pfx&quot; $null = certutil -f -p $pwd -exportpfx -privatekey MY $serverThumb &quot;$backupDir $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-server.pfx&quot; NoRoot $null = certutil -f -p $pwd -exportpfx -privatekey -user MY $clientThumb &quot;$backupDir $(Get-Date -Format &#39;yyyy-MM-dd&#39;)-client.pfx&quot; NoRoot . A non-automated process is documented on Qlik’s help site. . Ancillary content . For this section, the administrator will need to survey the other non-Qlik stuff and ensure appropriate backups of those items has occurred. This can range from installation files (in the case of drivers) and configurations (for DSNs) to third party certificates. Initially this process will require some effort, but practically administrators should get into the habit of placing the needed non-Qlik things into a location which can be referenced in documentation for the entirity of the Qlik deployment along with the specific needed configurations. This planning and documentation can be critical in a disaster scenario or well appreciated by the next person who takes over responsibilities for administering the Qlik deployment. . Full backups . With every (modern) Qlik Sense Enterprise installation, a utility is bundled with the install. This utility (QlikSenseUtil.exe) is placed in the install folder for Qlik Sense Enterprise in the Repository’s Util directory. With a default installation path, the path to utility would be C: Program Files Qlik Sense Repository Util QlikSenseUtil QlikSenseUtil.exe. One of the many pieces of functionality that this utility provides is the ability to do a full backup of a Qlik Sense Enterprise site via command line. There is a robust Qlik Support article located here which goes over this utility in more depth, but a sample command which can be used to do a full site backup is: . &quot;C: Program Files Qlik Sense Repository Util QlikSenseUtil QlikSenseUtil.exe&quot; -backup -databaseHostname=&quot;localhost&quot; -databasePassword=&quot;MySuperSecretPassword&quot; -path=&quot;D: Backups&quot; -rootPath=&quot; MyServer Share&quot; . The utility will snapshot all the files on Qlik’s share, the database, and the internal Qlik certificates. This tool is ideal for many organizations where the flexibility of custom backup scripts is not needed. . How do I verify that backups are being done? . After ensuring that there is a backup process in place, the administrator should check for the existence of backups in the designated backup location. This backup location should be off the Qlik Sense Enterprise server. It is therefore ideal to structure the backups into a format where a quick check can be done to confirm that backups are occurring. For example: . D: QSBackup D: QSBackup YYYY-MM-DD D: QSBackup YYYY-MM-DD QlikShare D: QSBackup YYYY-MM-DD Certificates D: QSBackup YYYY-MM-DD Database . Using a structure like this allows for the administrator to quickly survey: . whether the expected root folder exists (YYYY-MM-DD) | whether the files in the QlikShare, Certificates, and Database folders have non-zero file size | . Documentation . Help.qlik.com | . Tags . #monthly . #backup_and_archiving . #backup .   .",
		"url": "https://adminplaybook.qlik-poc.com/docs/backup_and_archiving/verify_backup_execution.html",
		"relUrl": "/docs/backup_and_archiving/verify_backup_execution.html"
	}
}
